// generated by cdk8s
import { ApiObject, ApiObjectMetadata, GroupVersionKind } from 'cdk8s';
import { Construct } from 'constructs';


/**
 * AutoscalingPolicy is the Schema for the AutoscalingPolicys API. Describes an autoscaling policy for Dataproc cluster autoscaler.
 *
 * @schema AutoscalingPolicy
 */
export class AutoscalingPolicy extends ApiObject {
  /**
   * Returns the apiVersion and kind for "AutoscalingPolicy"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'AutoscalingPolicy',
  }

  /**
   * Renders a Kubernetes manifest for "AutoscalingPolicy".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: AutoscalingPolicyProps): any {
    return {
      ...AutoscalingPolicy.GVK,
      ...toJson_AutoscalingPolicyProps(props),
    };
  }

  /**
   * Defines a "AutoscalingPolicy" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: AutoscalingPolicyProps) {
    super(scope, id, {
      ...AutoscalingPolicy.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...AutoscalingPolicy.GVK,
      ...toJson_AutoscalingPolicyProps(resolved),
    };
  }
}

/**
 * AutoscalingPolicy is the Schema for the AutoscalingPolicys API. Describes an autoscaling policy for Dataproc cluster autoscaler.
 *
 * @schema AutoscalingPolicy
 */
export interface AutoscalingPolicyProps {
  /**
   * @schema AutoscalingPolicy#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * AutoscalingPolicySpec defines the desired state of AutoscalingPolicy
   *
   * @schema AutoscalingPolicy#spec
   */
  readonly spec: AutoscalingPolicySpec;

}

/**
 * Converts an object of type 'AutoscalingPolicyProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicyProps(obj: AutoscalingPolicyProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_AutoscalingPolicySpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * AutoscalingPolicySpec defines the desired state of AutoscalingPolicy
 *
 * @schema AutoscalingPolicySpec
 */
export interface AutoscalingPolicySpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
   *
   * @schema AutoscalingPolicySpec#deletionPolicy
   */
  readonly deletionPolicy?: AutoscalingPolicySpecDeletionPolicy;

  /**
   * @schema AutoscalingPolicySpec#forProvider
   */
  readonly forProvider: AutoscalingPolicySpecForProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
   *
   * @schema AutoscalingPolicySpec#initProvider
   */
  readonly initProvider?: AutoscalingPolicySpecInitProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. ManagementPolicies specify the array of actions Crossplane is allowed to take on the managed and external resources. This field is planned to replace the DeletionPolicy field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. If both are custom, the DeletionPolicy field will be ignored. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223 and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
   *
   * @schema AutoscalingPolicySpec#managementPolicies
   */
  readonly managementPolicies?: AutoscalingPolicySpecManagementPolicies[];

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema AutoscalingPolicySpec#providerConfigRef
   */
  readonly providerConfigRef?: AutoscalingPolicySpecProviderConfigRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema AutoscalingPolicySpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: AutoscalingPolicySpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema AutoscalingPolicySpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: AutoscalingPolicySpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'AutoscalingPolicySpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpec(obj: AutoscalingPolicySpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_AutoscalingPolicySpecForProvider(obj.forProvider),
    'initProvider': toJson_AutoscalingPolicySpecInitProvider(obj.initProvider),
    'managementPolicies': obj.managementPolicies?.map(y => y),
    'providerConfigRef': toJson_AutoscalingPolicySpecProviderConfigRef(obj.providerConfigRef),
    'publishConnectionDetailsTo': toJson_AutoscalingPolicySpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_AutoscalingPolicySpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
 *
 * @schema AutoscalingPolicySpecDeletionPolicy
 */
export enum AutoscalingPolicySpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema AutoscalingPolicySpecForProvider
 */
export interface AutoscalingPolicySpecForProvider {
  /**
   * Basic algorithm for autoscaling. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProvider#basicAlgorithm
   */
  readonly basicAlgorithm?: AutoscalingPolicySpecForProviderBasicAlgorithm[];

  /**
   * The  location where the autoscaling policy should reside. The default value is global.
   *
   * @schema AutoscalingPolicySpecForProvider#location
   */
  readonly location?: string;

  /**
   * The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
   *
   * @schema AutoscalingPolicySpecForProvider#project
   */
  readonly project?: string;

  /**
   * Describes how the autoscaler will operate for secondary workers. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProvider#secondaryWorkerConfig
   */
  readonly secondaryWorkerConfig?: AutoscalingPolicySpecForProviderSecondaryWorkerConfig[];

  /**
   * Describes how the autoscaler will operate for primary workers. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProvider#workerConfig
   */
  readonly workerConfig?: AutoscalingPolicySpecForProviderWorkerConfig[];

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProvider(obj: AutoscalingPolicySpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'basicAlgorithm': obj.basicAlgorithm?.map(y => toJson_AutoscalingPolicySpecForProviderBasicAlgorithm(y)),
    'location': obj.location,
    'project': obj.project,
    'secondaryWorkerConfig': obj.secondaryWorkerConfig?.map(y => toJson_AutoscalingPolicySpecForProviderSecondaryWorkerConfig(y)),
    'workerConfig': obj.workerConfig?.map(y => toJson_AutoscalingPolicySpecForProviderWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
 *
 * @schema AutoscalingPolicySpecInitProvider
 */
export interface AutoscalingPolicySpecInitProvider {
  /**
   * Basic algorithm for autoscaling. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecInitProvider#basicAlgorithm
   */
  readonly basicAlgorithm?: AutoscalingPolicySpecInitProviderBasicAlgorithm[];

  /**
   * The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
   *
   * @schema AutoscalingPolicySpecInitProvider#project
   */
  readonly project?: string;

  /**
   * Describes how the autoscaler will operate for secondary workers. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecInitProvider#secondaryWorkerConfig
   */
  readonly secondaryWorkerConfig?: AutoscalingPolicySpecInitProviderSecondaryWorkerConfig[];

  /**
   * Describes how the autoscaler will operate for primary workers. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecInitProvider#workerConfig
   */
  readonly workerConfig?: AutoscalingPolicySpecInitProviderWorkerConfig[];

}

/**
 * Converts an object of type 'AutoscalingPolicySpecInitProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecInitProvider(obj: AutoscalingPolicySpecInitProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'basicAlgorithm': obj.basicAlgorithm?.map(y => toJson_AutoscalingPolicySpecInitProviderBasicAlgorithm(y)),
    'project': obj.project,
    'secondaryWorkerConfig': obj.secondaryWorkerConfig?.map(y => toJson_AutoscalingPolicySpecInitProviderSecondaryWorkerConfig(y)),
    'workerConfig': obj.workerConfig?.map(y => toJson_AutoscalingPolicySpecInitProviderWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * A ManagementAction represents an action that the Crossplane controllers can take on an external resource.
 *
 * @schema AutoscalingPolicySpecManagementPolicies
 */
export enum AutoscalingPolicySpecManagementPolicies {
  /** Observe */
  OBSERVE = "Observe",
  /** Create */
  CREATE = "Create",
  /** Update */
  UPDATE = "Update",
  /** Delete */
  DELETE = "Delete",
  /** LateInitialize */
  LATE_INITIALIZE = "LateInitialize",
  /** * */
  VALUE_ = "*",
}

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema AutoscalingPolicySpecProviderConfigRef
 */
export interface AutoscalingPolicySpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema AutoscalingPolicySpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema AutoscalingPolicySpecProviderConfigRef#policy
   */
  readonly policy?: AutoscalingPolicySpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecProviderConfigRef(obj: AutoscalingPolicySpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_AutoscalingPolicySpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsTo
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: AutoscalingPolicySpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsTo(obj: AutoscalingPolicySpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_AutoscalingPolicySpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema AutoscalingPolicySpecWriteConnectionSecretToRef
 */
export interface AutoscalingPolicySpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema AutoscalingPolicySpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema AutoscalingPolicySpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecWriteConnectionSecretToRef(obj: AutoscalingPolicySpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderBasicAlgorithm
 */
export interface AutoscalingPolicySpecForProviderBasicAlgorithm {
  /**
   * Duration between scaling events. A scaling period starts after the update operation from the previous event has completed. Bounds: [2m, 1d]. Default: 2m.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithm#cooldownPeriod
   */
  readonly cooldownPeriod?: string;

  /**
   * YARN autoscaling configuration. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithm#yarnConfig
   */
  readonly yarnConfig?: AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig[];

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderBasicAlgorithm' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderBasicAlgorithm(obj: AutoscalingPolicySpecForProviderBasicAlgorithm | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'cooldownPeriod': obj.cooldownPeriod,
    'yarnConfig': obj.yarnConfig?.map(y => toJson_AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig
 */
export interface AutoscalingPolicySpecForProviderSecondaryWorkerConfig {
  /**
   * Maximum number of instances for this group. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set. Bounds: [minInstances, ). Defaults to 0.
   *
   * @default 0.
   * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig#maxInstances
   */
  readonly maxInstances?: number;

  /**
   * Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
   *
   * @default 0.
   * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig#minInstances
   */
  readonly minInstances?: number;

  /**
   * Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker. The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if maxInstances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created. If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
   *
   * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig#weight
   */
  readonly weight?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderSecondaryWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderSecondaryWorkerConfig(obj: AutoscalingPolicySpecForProviderSecondaryWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxInstances': obj.maxInstances,
    'minInstances': obj.minInstances,
    'weight': obj.weight,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderWorkerConfig
 */
export interface AutoscalingPolicySpecForProviderWorkerConfig {
  /**
   * Maximum number of instances for this group.
   *
   * @schema AutoscalingPolicySpecForProviderWorkerConfig#maxInstances
   */
  readonly maxInstances?: number;

  /**
   * Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
   *
   * @default 2.
   * @schema AutoscalingPolicySpecForProviderWorkerConfig#minInstances
   */
  readonly minInstances?: number;

  /**
   * Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker. The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if maxInstances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created. If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
   *
   * @schema AutoscalingPolicySpecForProviderWorkerConfig#weight
   */
  readonly weight?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderWorkerConfig(obj: AutoscalingPolicySpecForProviderWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxInstances': obj.maxInstances,
    'minInstances': obj.minInstances,
    'weight': obj.weight,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecInitProviderBasicAlgorithm
 */
export interface AutoscalingPolicySpecInitProviderBasicAlgorithm {
  /**
   * Duration between scaling events. A scaling period starts after the update operation from the previous event has completed. Bounds: [2m, 1d]. Default: 2m.
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithm#cooldownPeriod
   */
  readonly cooldownPeriod?: string;

  /**
   * YARN autoscaling configuration. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithm#yarnConfig
   */
  readonly yarnConfig?: AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig[];

}

/**
 * Converts an object of type 'AutoscalingPolicySpecInitProviderBasicAlgorithm' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecInitProviderBasicAlgorithm(obj: AutoscalingPolicySpecInitProviderBasicAlgorithm | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'cooldownPeriod': obj.cooldownPeriod,
    'yarnConfig': obj.yarnConfig?.map(y => toJson_AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecInitProviderSecondaryWorkerConfig
 */
export interface AutoscalingPolicySpecInitProviderSecondaryWorkerConfig {
  /**
   * Maximum number of instances for this group. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set. Bounds: [minInstances, ). Defaults to 0.
   *
   * @default 0.
   * @schema AutoscalingPolicySpecInitProviderSecondaryWorkerConfig#maxInstances
   */
  readonly maxInstances?: number;

  /**
   * Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
   *
   * @default 0.
   * @schema AutoscalingPolicySpecInitProviderSecondaryWorkerConfig#minInstances
   */
  readonly minInstances?: number;

  /**
   * Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker. The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if maxInstances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created. If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
   *
   * @schema AutoscalingPolicySpecInitProviderSecondaryWorkerConfig#weight
   */
  readonly weight?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecInitProviderSecondaryWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecInitProviderSecondaryWorkerConfig(obj: AutoscalingPolicySpecInitProviderSecondaryWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxInstances': obj.maxInstances,
    'minInstances': obj.minInstances,
    'weight': obj.weight,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecInitProviderWorkerConfig
 */
export interface AutoscalingPolicySpecInitProviderWorkerConfig {
  /**
   * Maximum number of instances for this group.
   *
   * @schema AutoscalingPolicySpecInitProviderWorkerConfig#maxInstances
   */
  readonly maxInstances?: number;

  /**
   * Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
   *
   * @default 2.
   * @schema AutoscalingPolicySpecInitProviderWorkerConfig#minInstances
   */
  readonly minInstances?: number;

  /**
   * Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker. The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if maxInstances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created. If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
   *
   * @schema AutoscalingPolicySpecInitProviderWorkerConfig#weight
   */
  readonly weight?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecInitProviderWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecInitProviderWorkerConfig(obj: AutoscalingPolicySpecInitProviderWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxInstances': obj.maxInstances,
    'minInstances': obj.minInstances,
    'weight': obj.weight,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema AutoscalingPolicySpecProviderConfigRefPolicy
 */
export interface AutoscalingPolicySpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema AutoscalingPolicySpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: AutoscalingPolicySpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema AutoscalingPolicySpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: AutoscalingPolicySpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecProviderConfigRefPolicy(obj: AutoscalingPolicySpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRef
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRef(obj: AutoscalingPolicySpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsToMetadata(obj: AutoscalingPolicySpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig
 */
export interface AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig {
  /**
   * Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations. Bounds: [0s, 1d].
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#gracefulDecommissionTimeout
   */
  readonly gracefulDecommissionTimeout?: string;

  /**
   * Fraction of average pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. Bounds: [0.0, 1.0].
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleDownFactor
   */
  readonly scaleDownFactor?: number;

  /**
   * Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change. Bounds: [0.0, 1.0]. Default: 0.0.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleDownMinWorkerFraction
   */
  readonly scaleDownMinWorkerFraction?: number;

  /**
   * Fraction of average pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). Bounds: [0.0, 1.0].
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleUpFactor
   */
  readonly scaleUpFactor?: number;

  /**
   * Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change. Bounds: [0.0, 1.0]. Default: 0.0.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleUpMinWorkerFraction
   */
  readonly scaleUpMinWorkerFraction?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig(obj: AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gracefulDecommissionTimeout': obj.gracefulDecommissionTimeout,
    'scaleDownFactor': obj.scaleDownFactor,
    'scaleDownMinWorkerFraction': obj.scaleDownMinWorkerFraction,
    'scaleUpFactor': obj.scaleUpFactor,
    'scaleUpMinWorkerFraction': obj.scaleUpMinWorkerFraction,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig
 */
export interface AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig {
  /**
   * Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations. Bounds: [0s, 1d].
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig#gracefulDecommissionTimeout
   */
  readonly gracefulDecommissionTimeout?: string;

  /**
   * Fraction of average pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. Bounds: [0.0, 1.0].
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig#scaleDownFactor
   */
  readonly scaleDownFactor?: number;

  /**
   * Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change. Bounds: [0.0, 1.0]. Default: 0.0.
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig#scaleDownMinWorkerFraction
   */
  readonly scaleDownMinWorkerFraction?: number;

  /**
   * Fraction of average pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). Bounds: [0.0, 1.0].
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig#scaleUpFactor
   */
  readonly scaleUpFactor?: number;

  /**
   * Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change. Bounds: [0.0, 1.0]. Default: 0.0.
   *
   * @schema AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig#scaleUpMinWorkerFraction
   */
  readonly scaleUpMinWorkerFraction?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig(obj: AutoscalingPolicySpecInitProviderBasicAlgorithmYarnConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gracefulDecommissionTimeout': obj.gracefulDecommissionTimeout,
    'scaleDownFactor': obj.scaleDownFactor,
    'scaleDownMinWorkerFraction': obj.scaleDownMinWorkerFraction,
    'scaleUpFactor': obj.scaleUpFactor,
    'scaleUpMinWorkerFraction': obj.scaleUpMinWorkerFraction,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema AutoscalingPolicySpecProviderConfigRefPolicyResolution
 */
export enum AutoscalingPolicySpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema AutoscalingPolicySpecProviderConfigRefPolicyResolve
 */
export enum AutoscalingPolicySpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy(obj: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}


/**
 * Cluster is the Schema for the Clusters API. Manages a Cloud Dataproc cluster resource.
 *
 * @schema Cluster
 */
export class Cluster extends ApiObject {
  /**
   * Returns the apiVersion and kind for "Cluster"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'Cluster',
  }

  /**
   * Renders a Kubernetes manifest for "Cluster".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: ClusterProps): any {
    return {
      ...Cluster.GVK,
      ...toJson_ClusterProps(props),
    };
  }

  /**
   * Defines a "Cluster" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: ClusterProps) {
    super(scope, id, {
      ...Cluster.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...Cluster.GVK,
      ...toJson_ClusterProps(resolved),
    };
  }
}

/**
 * Cluster is the Schema for the Clusters API. Manages a Cloud Dataproc cluster resource.
 *
 * @schema Cluster
 */
export interface ClusterProps {
  /**
   * @schema Cluster#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * ClusterSpec defines the desired state of Cluster
   *
   * @schema Cluster#spec
   */
  readonly spec: ClusterSpec;

}

/**
 * Converts an object of type 'ClusterProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterProps(obj: ClusterProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_ClusterSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ClusterSpec defines the desired state of Cluster
 *
 * @schema ClusterSpec
 */
export interface ClusterSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
   *
   * @schema ClusterSpec#deletionPolicy
   */
  readonly deletionPolicy?: ClusterSpecDeletionPolicy;

  /**
   * @schema ClusterSpec#forProvider
   */
  readonly forProvider: ClusterSpecForProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
   *
   * @schema ClusterSpec#initProvider
   */
  readonly initProvider?: ClusterSpecInitProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. ManagementPolicies specify the array of actions Crossplane is allowed to take on the managed and external resources. This field is planned to replace the DeletionPolicy field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. If both are custom, the DeletionPolicy field will be ignored. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223 and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
   *
   * @schema ClusterSpec#managementPolicies
   */
  readonly managementPolicies?: ClusterSpecManagementPolicies[];

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema ClusterSpec#providerConfigRef
   */
  readonly providerConfigRef?: ClusterSpecProviderConfigRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema ClusterSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: ClusterSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema ClusterSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: ClusterSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'ClusterSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpec(obj: ClusterSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_ClusterSpecForProvider(obj.forProvider),
    'initProvider': toJson_ClusterSpecInitProvider(obj.initProvider),
    'managementPolicies': obj.managementPolicies?.map(y => y),
    'providerConfigRef': toJson_ClusterSpecProviderConfigRef(obj.providerConfigRef),
    'publishConnectionDetailsTo': toJson_ClusterSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_ClusterSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
 *
 * @schema ClusterSpecDeletionPolicy
 */
export enum ClusterSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema ClusterSpecForProvider
 */
export interface ClusterSpecForProvider {
  /**
   * Allows you to configure various aspects of the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProvider#clusterConfig
   */
  readonly clusterConfig?: ClusterSpecForProviderClusterConfig[];

  /**
   * Does not affect auto scaling decomissioning from an autoscaling policy. Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress. Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs). Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of Duration). Only supported on Dataproc image versions 1.2 and higher. For more context see the docs
   *
   * @schema ClusterSpecForProvider#gracefulDecommissionTimeout
   */
  readonly gracefulDecommissionTimeout?: string;

  /**
   * The list of labels (key/value pairs) to be applied to instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name which is the name of the cluster.
   *
   * @schema ClusterSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The name of the cluster, unique within the project and zone.
   *
   * @schema ClusterSpecForProvider#name
   */
  readonly name?: string;

  /**
   * The ID of the project in which the cluster will exist. If it is not provided, the provider project is used.
   *
   * @schema ClusterSpecForProvider#project
   */
  readonly project?: string;

  /**
   * The region in which the cluster and associated nodes will be created in. Defaults to global.
   *
   * @default global.
   * @schema ClusterSpecForProvider#region
   */
  readonly region?: string;

  /**
   * Allows you to configure a virtual Dataproc on GKE cluster. Structure defined below.
   *
   * @schema ClusterSpecForProvider#virtualClusterConfig
   */
  readonly virtualClusterConfig?: ClusterSpecForProviderVirtualClusterConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProvider(obj: ClusterSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterConfig': obj.clusterConfig?.map(y => toJson_ClusterSpecForProviderClusterConfig(y)),
    'gracefulDecommissionTimeout': obj.gracefulDecommissionTimeout,
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'name': obj.name,
    'project': obj.project,
    'region': obj.region,
    'virtualClusterConfig': obj.virtualClusterConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
 *
 * @schema ClusterSpecInitProvider
 */
export interface ClusterSpecInitProvider {
  /**
   * Allows you to configure various aspects of the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProvider#clusterConfig
   */
  readonly clusterConfig?: ClusterSpecInitProviderClusterConfig[];

  /**
   * Does not affect auto scaling decomissioning from an autoscaling policy. Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress. Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs). Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of Duration). Only supported on Dataproc image versions 1.2 and higher. For more context see the docs
   *
   * @schema ClusterSpecInitProvider#gracefulDecommissionTimeout
   */
  readonly gracefulDecommissionTimeout?: string;

  /**
   * The list of labels (key/value pairs) to be applied to instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name which is the name of the cluster.
   *
   * @schema ClusterSpecInitProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The name of the cluster, unique within the project and zone.
   *
   * @schema ClusterSpecInitProvider#name
   */
  readonly name?: string;

  /**
   * The ID of the project in which the cluster will exist. If it is not provided, the provider project is used.
   *
   * @schema ClusterSpecInitProvider#project
   */
  readonly project?: string;

  /**
   * The region in which the cluster and associated nodes will be created in. Defaults to global.
   *
   * @default global.
   * @schema ClusterSpecInitProvider#region
   */
  readonly region?: string;

  /**
   * Allows you to configure a virtual Dataproc on GKE cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProvider#virtualClusterConfig
   */
  readonly virtualClusterConfig?: ClusterSpecInitProviderVirtualClusterConfig[];

}

/**
 * Converts an object of type 'ClusterSpecInitProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProvider(obj: ClusterSpecInitProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterConfig': obj.clusterConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfig(y)),
    'gracefulDecommissionTimeout': obj.gracefulDecommissionTimeout,
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'name': obj.name,
    'project': obj.project,
    'region': obj.region,
    'virtualClusterConfig': obj.virtualClusterConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * A ManagementAction represents an action that the Crossplane controllers can take on an external resource.
 *
 * @schema ClusterSpecManagementPolicies
 */
export enum ClusterSpecManagementPolicies {
  /** Observe */
  OBSERVE = "Observe",
  /** Create */
  CREATE = "Create",
  /** Update */
  UPDATE = "Update",
  /** Delete */
  DELETE = "Delete",
  /** LateInitialize */
  LATE_INITIALIZE = "LateInitialize",
  /** * */
  VALUE_ = "*",
}

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema ClusterSpecProviderConfigRef
 */
export interface ClusterSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecProviderConfigRef#policy
   */
  readonly policy?: ClusterSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecProviderConfigRef(obj: ClusterSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema ClusterSpecPublishConnectionDetailsTo
 */
export interface ClusterSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema ClusterSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: ClusterSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema ClusterSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: ClusterSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema ClusterSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsTo(obj: ClusterSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_ClusterSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_ClusterSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema ClusterSpecWriteConnectionSecretToRef
 */
export interface ClusterSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema ClusterSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema ClusterSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'ClusterSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecWriteConnectionSecretToRef(obj: ClusterSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfig
 */
export interface ClusterSpecForProviderClusterConfig {
  /**
   * The autoscaling policy config associated with the cluster. Note that once set, if autoscaling_config is the only field set in cluster_config, it can only be removed by setting policy_uri = "", rather than removing the whole block. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#autoscalingConfig
   */
  readonly autoscalingConfig?: ClusterSpecForProviderClusterConfigAutoscalingConfig[];

  /**
   * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#dataprocMetricConfig
   */
  readonly dataprocMetricConfig?: ClusterSpecForProviderClusterConfigDataprocMetricConfig[];

  /**
   * The Customer managed encryption keys settings for the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#encryptionConfig
   */
  readonly encryptionConfig?: ClusterSpecForProviderClusterConfigEncryptionConfig[];

  /**
   * The config settings for port access on the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#endpointConfig
   */
  readonly endpointConfig?: ClusterSpecForProviderClusterConfigEndpointConfig[];

  /**
   * Common config settings for resources of Google Compute Engine cluster instances, applicable to all instances in the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#gceClusterConfig
   */
  readonly gceClusterConfig?: ClusterSpecForProviderClusterConfigGceClusterConfig[];

  /**
   * Commands to execute on each node after config is completed. You can specify multiple versions of these. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#initializationAction
   */
  readonly initializationAction?: ClusterSpecForProviderClusterConfigInitializationAction[];

  /**
   * The settings for auto deletion cluster schedule. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#lifecycleConfig
   */
  readonly lifecycleConfig?: ClusterSpecForProviderClusterConfigLifecycleConfig[];

  /**
   * The Google Compute Engine config settings for the master instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#masterConfig
   */
  readonly masterConfig?: ClusterSpecForProviderClusterConfigMasterConfig[];

  /**
   * The config setting for metastore service with the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#metastoreConfig
   */
  readonly metastoreConfig?: ClusterSpecForProviderClusterConfigMetastoreConfig[];

  /**
   * The Google Compute Engine config settings for the additional instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#preemptibleWorkerConfig
   */
  readonly preemptibleWorkerConfig?: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig[];

  /**
   * Security related configuration. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#securityConfig
   */
  readonly securityConfig?: ClusterSpecForProviderClusterConfigSecurityConfig[];

  /**
   * The config settings for software inside the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#softwareConfig
   */
  readonly softwareConfig?: ClusterSpecForProviderClusterConfigSoftwareConfig[];

  /**
   * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
   *
   * @schema ClusterSpecForProviderClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

  /**
   * The Cloud Storage temp bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
   *
   * @schema ClusterSpecForProviderClusterConfig#tempBucket
   */
  readonly tempBucket?: string;

  /**
   * The Google Compute Engine config settings for the worker instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#workerConfig
   */
  readonly workerConfig?: ClusterSpecForProviderClusterConfigWorkerConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfig(obj: ClusterSpecForProviderClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscalingConfig': obj.autoscalingConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigAutoscalingConfig(y)),
    'dataprocMetricConfig': obj.dataprocMetricConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfig(y)),
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigEncryptionConfig(y)),
    'endpointConfig': obj.endpointConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigEndpointConfig(y)),
    'gceClusterConfig': obj.gceClusterConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfig(y)),
    'initializationAction': obj.initializationAction?.map(y => toJson_ClusterSpecForProviderClusterConfigInitializationAction(y)),
    'lifecycleConfig': obj.lifecycleConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigLifecycleConfig(y)),
    'masterConfig': obj.masterConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigMasterConfig(y)),
    'metastoreConfig': obj.metastoreConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigMetastoreConfig(y)),
    'preemptibleWorkerConfig': obj.preemptibleWorkerConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig(y)),
    'securityConfig': obj.securityConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigSecurityConfig(y)),
    'softwareConfig': obj.softwareConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigSoftwareConfig(y)),
    'stagingBucket': obj.stagingBucket,
    'tempBucket': obj.tempBucket,
    'workerConfig': obj.workerConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfig {
  /**
   * Configuration of auxiliary services used by this cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfig#auxiliaryServicesConfig
   */
  readonly auxiliaryServicesConfig?: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig[];

  /**
   * The configuration for running the Dataproc cluster on Kubernetes. Structure defined below.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfig#kubernetesClusterConfig
   */
  readonly kubernetesClusterConfig?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig[];

  /**
   * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfig(obj: ClusterSpecForProviderVirtualClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'auxiliaryServicesConfig': obj.auxiliaryServicesConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig(y)),
    'kubernetesClusterConfig': obj.kubernetesClusterConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig(y)),
    'stagingBucket': obj.stagingBucket,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfig
 */
export interface ClusterSpecInitProviderClusterConfig {
  /**
   * The autoscaling policy config associated with the cluster. Note that once set, if autoscaling_config is the only field set in cluster_config, it can only be removed by setting policy_uri = "", rather than removing the whole block. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#autoscalingConfig
   */
  readonly autoscalingConfig?: ClusterSpecInitProviderClusterConfigAutoscalingConfig[];

  /**
   * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#dataprocMetricConfig
   */
  readonly dataprocMetricConfig?: ClusterSpecInitProviderClusterConfigDataprocMetricConfig[];

  /**
   * The Customer managed encryption keys settings for the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#encryptionConfig
   */
  readonly encryptionConfig?: ClusterSpecInitProviderClusterConfigEncryptionConfig[];

  /**
   * The config settings for port access on the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#endpointConfig
   */
  readonly endpointConfig?: ClusterSpecInitProviderClusterConfigEndpointConfig[];

  /**
   * Common config settings for resources of Google Compute Engine cluster instances, applicable to all instances in the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#gceClusterConfig
   */
  readonly gceClusterConfig?: ClusterSpecInitProviderClusterConfigGceClusterConfig[];

  /**
   * Commands to execute on each node after config is completed. You can specify multiple versions of these. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#initializationAction
   */
  readonly initializationAction?: ClusterSpecInitProviderClusterConfigInitializationAction[];

  /**
   * The settings for auto deletion cluster schedule. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#lifecycleConfig
   */
  readonly lifecycleConfig?: ClusterSpecInitProviderClusterConfigLifecycleConfig[];

  /**
   * The Google Compute Engine config settings for the master instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#masterConfig
   */
  readonly masterConfig?: ClusterSpecInitProviderClusterConfigMasterConfig[];

  /**
   * The config setting for metastore service with the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#metastoreConfig
   */
  readonly metastoreConfig?: ClusterSpecInitProviderClusterConfigMetastoreConfig[];

  /**
   * The Google Compute Engine config settings for the additional instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#preemptibleWorkerConfig
   */
  readonly preemptibleWorkerConfig?: ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig[];

  /**
   * Security related configuration. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#securityConfig
   */
  readonly securityConfig?: ClusterSpecInitProviderClusterConfigSecurityConfig[];

  /**
   * The config settings for software inside the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#softwareConfig
   */
  readonly softwareConfig?: ClusterSpecInitProviderClusterConfigSoftwareConfig[];

  /**
   * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
   *
   * @schema ClusterSpecInitProviderClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

  /**
   * The Cloud Storage temp bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
   *
   * @schema ClusterSpecInitProviderClusterConfig#tempBucket
   */
  readonly tempBucket?: string;

  /**
   * The Google Compute Engine config settings for the worker instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderClusterConfig#workerConfig
   */
  readonly workerConfig?: ClusterSpecInitProviderClusterConfigWorkerConfig[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfig(obj: ClusterSpecInitProviderClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscalingConfig': obj.autoscalingConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigAutoscalingConfig(y)),
    'dataprocMetricConfig': obj.dataprocMetricConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigDataprocMetricConfig(y)),
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigEncryptionConfig(y)),
    'endpointConfig': obj.endpointConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigEndpointConfig(y)),
    'gceClusterConfig': obj.gceClusterConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigGceClusterConfig(y)),
    'initializationAction': obj.initializationAction?.map(y => toJson_ClusterSpecInitProviderClusterConfigInitializationAction(y)),
    'lifecycleConfig': obj.lifecycleConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigLifecycleConfig(y)),
    'masterConfig': obj.masterConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigMasterConfig(y)),
    'metastoreConfig': obj.metastoreConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigMetastoreConfig(y)),
    'preemptibleWorkerConfig': obj.preemptibleWorkerConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig(y)),
    'securityConfig': obj.securityConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigSecurityConfig(y)),
    'softwareConfig': obj.softwareConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigSoftwareConfig(y)),
    'stagingBucket': obj.stagingBucket,
    'tempBucket': obj.tempBucket,
    'workerConfig': obj.workerConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfig {
  /**
   * Configuration of auxiliary services used by this cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfig#auxiliaryServicesConfig
   */
  readonly auxiliaryServicesConfig?: ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig[];

  /**
   * The configuration for running the Dataproc cluster on Kubernetes. Structure defined below.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfig#kubernetesClusterConfig
   */
  readonly kubernetesClusterConfig?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig[];

  /**
   * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfig(obj: ClusterSpecInitProviderVirtualClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'auxiliaryServicesConfig': obj.auxiliaryServicesConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig(y)),
    'kubernetesClusterConfig': obj.kubernetesClusterConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig(y)),
    'stagingBucket': obj.stagingBucket,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecProviderConfigRefPolicy
 */
export interface ClusterSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecProviderConfigRefPolicy(obj: ClusterSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRef
 */
export interface ClusterSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: ClusterSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsToConfigRef(obj: ClusterSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema ClusterSpecPublishConnectionDetailsToMetadata
 */
export interface ClusterSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema ClusterSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema ClusterSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema ClusterSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsToMetadata(obj: ClusterSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigAutoscalingConfig
 */
export interface ClusterSpecForProviderClusterConfigAutoscalingConfig {
  /**
   * The autoscaling policy used by the cluster.
   *
   * @schema ClusterSpecForProviderClusterConfigAutoscalingConfig#policyUri
   */
  readonly policyUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigAutoscalingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigAutoscalingConfig(obj: ClusterSpecForProviderClusterConfigAutoscalingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'policyUri': obj.policyUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfig
 */
export interface ClusterSpecForProviderClusterConfigDataprocMetricConfig {
  /**
   * Metrics sources to enable.
   *
   * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfig#metrics
   */
  readonly metrics?: ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigDataprocMetricConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfig(obj: ClusterSpecForProviderClusterConfigDataprocMetricConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metrics': obj.metrics?.map(y => toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigEncryptionConfig
 */
export interface ClusterSpecForProviderClusterConfigEncryptionConfig {
  /**
   * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
   *
   * @schema ClusterSpecForProviderClusterConfigEncryptionConfig#kmsKeyName
   */
  readonly kmsKeyName?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigEncryptionConfig(obj: ClusterSpecForProviderClusterConfigEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kmsKeyName': obj.kmsKeyName,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigEndpointConfig
 */
export interface ClusterSpecForProviderClusterConfigEndpointConfig {
  /**
   * The flag to enable http access to specific ports on the cluster from external sources (aka Component Gateway). Defaults to false.
   *
   * @default false.
   * @schema ClusterSpecForProviderClusterConfigEndpointConfig#enableHttpPortAccess
   */
  readonly enableHttpPortAccess?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigEndpointConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigEndpointConfig(obj: ClusterSpecForProviderClusterConfigEndpointConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableHttpPortAccess': obj.enableHttpPortAccess,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfig
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfig {
  /**
   * By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. If set to true, all instances in the cluster will only have internal IP addresses. Note: Private Google Access (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster will be launched in.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#internalIpOnly
   */
  readonly internalIpOnly?: boolean;

  /**
   * A map of the Compute Engine metadata entries to add to all instances (see Project and instance metadata).
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#metadata
   */
  readonly metadata?: { [key: string]: string };

  /**
   * The name or self_link of the Google Compute Engine network to the cluster will be part of. Conflicts with subnetwork. If neither is specified, this defaults to the "default" network.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#network
   */
  readonly network?: string;

  /**
   * Node Group Affinity for sole-tenant clusters.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#nodeGroupAffinity
   */
  readonly nodeGroupAffinity?: ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity[];

  /**
   * Reservation Affinity for consuming zonal reservation.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#reservationAffinity
   */
  readonly reservationAffinity?: ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity[];

  /**
   * The service account to be used by the Node VMs. If not specified, the "default" service account is used.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccount
   */
  readonly serviceAccount?: string;

  /**
   * Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccountRef
   */
  readonly serviceAccountRef?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef;

  /**
   * The set of Google API scopes to be made available on all of the node VMs under the service_account specified. Both OAuth2 URLs and gcloud short names are supported. To allow full access to all Cloud APIs, use the cloud-platform scope. See a complete list of scopes here.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccountScopes
   */
  readonly serviceAccountScopes?: string[];

  /**
   * Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccountSelector
   */
  readonly serviceAccountSelector?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector;

  /**
   * Shielded Instance Config for clusters using Compute Engine Shielded VMs.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#shieldedInstanceConfig
   */
  readonly shieldedInstanceConfig?: ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig[];

  /**
   * The name or self_link of the Google Compute Engine subnetwork the cluster will be part of. Conflicts with network.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * The list of instance tags applied to instances in the cluster. Tags are used to identify valid sources or targets for network firewalls.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#tags
   */
  readonly tags?: string[];

  /**
   * The GCP zone where your data is stored and used (i.e. where the master and the worker nodes will be created in). If region is set to 'global' (default) then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement to determine this automatically for you. Note: This setting additionally determines and restricts which computing resources are available for use with other configs such as cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfig(obj: ClusterSpecForProviderClusterConfigGceClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'internalIpOnly': obj.internalIpOnly,
    'metadata': ((obj.metadata) === undefined) ? undefined : (Object.entries(obj.metadata).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'network': obj.network,
    'nodeGroupAffinity': obj.nodeGroupAffinity?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity(y)),
    'reservationAffinity': obj.reservationAffinity?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity(y)),
    'serviceAccount': obj.serviceAccount,
    'serviceAccountRef': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef(obj.serviceAccountRef),
    'serviceAccountScopes': obj.serviceAccountScopes?.map(y => y),
    'serviceAccountSelector': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector(obj.serviceAccountSelector),
    'shieldedInstanceConfig': obj.shieldedInstanceConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig(y)),
    'subnetwork': obj.subnetwork,
    'tags': obj.tags?.map(y => y),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigInitializationAction
 */
export interface ClusterSpecForProviderClusterConfigInitializationAction {
  /**
   * The script to be executed during initialization of the cluster. The script must be a GCS file with a gs:// prefix.
   *
   * @schema ClusterSpecForProviderClusterConfigInitializationAction#script
   */
  readonly script?: string;

  /**
   * The maximum duration (in seconds) which script is allowed to take to execute its action. GCP will default to a predetermined computed value if not set (currently 300).
   *
   * @schema ClusterSpecForProviderClusterConfigInitializationAction#timeoutSec
   */
  readonly timeoutSec?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigInitializationAction' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigInitializationAction(obj: ClusterSpecForProviderClusterConfigInitializationAction | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'script': obj.script,
    'timeoutSec': obj.timeoutSec,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigLifecycleConfig
 */
export interface ClusterSpecForProviderClusterConfigLifecycleConfig {
  /**
   * The time when cluster will be auto-deleted. A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
   *
   * @schema ClusterSpecForProviderClusterConfigLifecycleConfig#autoDeleteTime
   */
  readonly autoDeleteTime?: string;

  /**
   * The duration to keep the cluster alive while idling (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
   *
   * @schema ClusterSpecForProviderClusterConfigLifecycleConfig#idleDeleteTtl
   */
  readonly idleDeleteTtl?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigLifecycleConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigLifecycleConfig(obj: ClusterSpecForProviderClusterConfigLifecycleConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoDeleteTime': obj.autoDeleteTime,
    'idleDeleteTtl': obj.idleDeleteTtl,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMasterConfig
 */
export interface ClusterSpecForProviderClusterConfigMasterConfig {
  /**
   * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#accelerators
   */
  readonly accelerators?: ClusterSpecForProviderClusterConfigMasterConfigAccelerators[];

  /**
   * Disk Config
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecForProviderClusterConfigMasterConfigDiskConfig[];

  /**
   * The URI for the image to use for this worker.  See the guide for more information.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#imageUri
   */
  readonly imageUri?: string;

  /**
   * The name of a Google Compute Engine machine type to create for the master. If not specified, GCP will default to a predetermined computed value (currently n1-standard-4).
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#machineType
   */
  readonly machineType?: string;

  /**
   * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone. See the guide for details about which CPU families are available (and defaulted) for each zone.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Specifies the number of master nodes to create. If not specified, GCP will default to a predetermined computed value (currently 1).
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#numInstances
   */
  readonly numInstances?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMasterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMasterConfig(obj: ClusterSpecForProviderClusterConfigMasterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_ClusterSpecForProviderClusterConfigMasterConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigMasterConfigDiskConfig(y)),
    'imageUri': obj.imageUri,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMetastoreConfig
 */
export interface ClusterSpecForProviderClusterConfigMetastoreConfig {
  /**
   * Resource name of an existing Dataproc Metastore service.
   *
   * @schema ClusterSpecForProviderClusterConfigMetastoreConfig#dataprocMetastoreService
   */
  readonly dataprocMetastoreService?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMetastoreConfig(obj: ClusterSpecForProviderClusterConfigMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocMetastoreService': obj.dataprocMetastoreService,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig
 */
export interface ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig {
  /**
   * Disk Config
   *
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig[];

  /**
   * Specifies the number of preemptible nodes to create. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE Accepted values are:
   *
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig(obj: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig(y)),
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigSecurityConfig
 */
export interface ClusterSpecForProviderClusterConfigSecurityConfig {
  /**
   * Kerberos Configuration
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfig#kerberosConfig
   */
  readonly kerberosConfig?: ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigSecurityConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigSecurityConfig(obj: ClusterSpecForProviderClusterConfigSecurityConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigSoftwareConfig
 */
export interface ClusterSpecForProviderClusterConfigSoftwareConfig {
  /**
   * The Cloud Dataproc image version to use for the cluster - this controls the sets of software versions installed onto the nodes when you create clusters. If not specified, defaults to the latest version. For a list of valid versions see Cloud Dataproc versions
   *
   * @schema ClusterSpecForProviderClusterConfigSoftwareConfig#imageVersion
   */
  readonly imageVersion?: string;

  /**
   * The set of optional components to activate on the cluster. See Available Optional Components.
   *
   * @schema ClusterSpecForProviderClusterConfigSoftwareConfig#optionalComponents
   */
  readonly optionalComponents?: string[];

  /**
   * A list of override and additional properties (key/value pairs) used to modify various aspects of the common configuration files used when creating a cluster. For a list of valid properties please see Cluster properties
   *
   * @schema ClusterSpecForProviderClusterConfigSoftwareConfig#overrideProperties
   */
  readonly overrideProperties?: { [key: string]: string };

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigSoftwareConfig(obj: ClusterSpecForProviderClusterConfigSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'imageVersion': obj.imageVersion,
    'optionalComponents': obj.optionalComponents?.map(y => y),
    'overrideProperties': ((obj.overrideProperties) === undefined) ? undefined : (Object.entries(obj.overrideProperties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigWorkerConfig
 */
export interface ClusterSpecForProviderClusterConfigWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#accelerators
   */
  readonly accelerators?: ClusterSpecForProviderClusterConfigWorkerConfigAccelerators[];

  /**
   * Disk Config
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig[];

  /**
   * The URI for the image to use for this worker.  See the guide for more information.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#imageUri
   */
  readonly imageUri?: string;

  /**
   * The name of a Google Compute Engine machine type to create for the worker nodes. If not specified, GCP will default to a predetermined computed value (currently n1-standard-4).
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone. See the guide for details about which CPU families are available (and defaulted) for each zone.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Specifies the number of worker nodes to create. If not specified, GCP will default to a predetermined computed value (currently 2). There is currently a beta feature which allows you to run a Single Node Cluster. In order to take advantage of this you need to set "dataproc:dataproc.allow.zero.workers" = "true" in cluster_config.software_config.properties
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#numInstances
   */
  readonly numInstances?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigWorkerConfig(obj: ClusterSpecForProviderClusterConfigWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_ClusterSpecForProviderClusterConfigWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig(y)),
    'imageUri': obj.imageUri,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig {
  /**
   * The config setting for metastore service with the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig#metastoreConfig
   */
  readonly metastoreConfig?: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig[];

  /**
   * The Spark History Server configuration for the workload.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig#sparkHistoryServerConfig
   */
  readonly sparkHistoryServerConfig?: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig(obj: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metastoreConfig': obj.metastoreConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig(y)),
    'sparkHistoryServerConfig': obj.sparkHistoryServerConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig {
  /**
   * The configuration for running the Dataproc cluster on GKE.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig#gkeClusterConfig
   */
  readonly gkeClusterConfig?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig[];

  /**
   * A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig#kubernetesNamespace
   */
  readonly kubernetesNamespace?: string;

  /**
   * The software configuration for this Dataproc cluster running on Kubernetes.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig#kubernetesSoftwareConfig
   */
  readonly kubernetesSoftwareConfig?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gkeClusterConfig': obj.gkeClusterConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig(y)),
    'kubernetesNamespace': obj.kubernetesNamespace,
    'kubernetesSoftwareConfig': obj.kubernetesSoftwareConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigAutoscalingConfig
 */
export interface ClusterSpecInitProviderClusterConfigAutoscalingConfig {
  /**
   * The autoscaling policy used by the cluster.
   *
   * @schema ClusterSpecInitProviderClusterConfigAutoscalingConfig#policyUri
   */
  readonly policyUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigAutoscalingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigAutoscalingConfig(obj: ClusterSpecInitProviderClusterConfigAutoscalingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'policyUri': obj.policyUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigDataprocMetricConfig
 */
export interface ClusterSpecInitProviderClusterConfigDataprocMetricConfig {
  /**
   * Metrics sources to enable.
   *
   * @schema ClusterSpecInitProviderClusterConfigDataprocMetricConfig#metrics
   */
  readonly metrics?: ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigDataprocMetricConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigDataprocMetricConfig(obj: ClusterSpecInitProviderClusterConfigDataprocMetricConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metrics': obj.metrics?.map(y => toJson_ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigEncryptionConfig
 */
export interface ClusterSpecInitProviderClusterConfigEncryptionConfig {
  /**
   * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
   *
   * @schema ClusterSpecInitProviderClusterConfigEncryptionConfig#kmsKeyName
   */
  readonly kmsKeyName?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigEncryptionConfig(obj: ClusterSpecInitProviderClusterConfigEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kmsKeyName': obj.kmsKeyName,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigEndpointConfig
 */
export interface ClusterSpecInitProviderClusterConfigEndpointConfig {
  /**
   * The flag to enable http access to specific ports on the cluster from external sources (aka Component Gateway). Defaults to false.
   *
   * @default false.
   * @schema ClusterSpecInitProviderClusterConfigEndpointConfig#enableHttpPortAccess
   */
  readonly enableHttpPortAccess?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigEndpointConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigEndpointConfig(obj: ClusterSpecInitProviderClusterConfigEndpointConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableHttpPortAccess': obj.enableHttpPortAccess,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig
 */
export interface ClusterSpecInitProviderClusterConfigGceClusterConfig {
  /**
   * By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. If set to true, all instances in the cluster will only have internal IP addresses. Note: Private Google Access (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster will be launched in.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#internalIpOnly
   */
  readonly internalIpOnly?: boolean;

  /**
   * A map of the Compute Engine metadata entries to add to all instances (see Project and instance metadata).
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#metadata
   */
  readonly metadata?: { [key: string]: string };

  /**
   * The name or self_link of the Google Compute Engine network to the cluster will be part of. Conflicts with subnetwork. If neither is specified, this defaults to the "default" network.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#network
   */
  readonly network?: string;

  /**
   * Node Group Affinity for sole-tenant clusters.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#nodeGroupAffinity
   */
  readonly nodeGroupAffinity?: ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity[];

  /**
   * Reservation Affinity for consuming zonal reservation.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#reservationAffinity
   */
  readonly reservationAffinity?: ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity[];

  /**
   * The set of Google API scopes to be made available on all of the node VMs under the service_account specified. Both OAuth2 URLs and gcloud short names are supported. To allow full access to all Cloud APIs, use the cloud-platform scope. See a complete list of scopes here.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#serviceAccountScopes
   */
  readonly serviceAccountScopes?: string[];

  /**
   * Shielded Instance Config for clusters using Compute Engine Shielded VMs.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#shieldedInstanceConfig
   */
  readonly shieldedInstanceConfig?: ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig[];

  /**
   * The name or self_link of the Google Compute Engine subnetwork the cluster will be part of. Conflicts with network.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * The list of instance tags applied to instances in the cluster. Tags are used to identify valid sources or targets for network firewalls.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#tags
   */
  readonly tags?: string[];

  /**
   * The GCP zone where your data is stored and used (i.e. where the master and the worker nodes will be created in). If region is set to 'global' (default) then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement to determine this automatically for you. Note: This setting additionally determines and restricts which computing resources are available for use with other configs such as cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfig#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigGceClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigGceClusterConfig(obj: ClusterSpecInitProviderClusterConfigGceClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'internalIpOnly': obj.internalIpOnly,
    'metadata': ((obj.metadata) === undefined) ? undefined : (Object.entries(obj.metadata).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'network': obj.network,
    'nodeGroupAffinity': obj.nodeGroupAffinity?.map(y => toJson_ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity(y)),
    'reservationAffinity': obj.reservationAffinity?.map(y => toJson_ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity(y)),
    'serviceAccountScopes': obj.serviceAccountScopes?.map(y => y),
    'shieldedInstanceConfig': obj.shieldedInstanceConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig(y)),
    'subnetwork': obj.subnetwork,
    'tags': obj.tags?.map(y => y),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigInitializationAction
 */
export interface ClusterSpecInitProviderClusterConfigInitializationAction {
  /**
   * The script to be executed during initialization of the cluster. The script must be a GCS file with a gs:// prefix.
   *
   * @schema ClusterSpecInitProviderClusterConfigInitializationAction#script
   */
  readonly script?: string;

  /**
   * The maximum duration (in seconds) which script is allowed to take to execute its action. GCP will default to a predetermined computed value if not set (currently 300).
   *
   * @schema ClusterSpecInitProviderClusterConfigInitializationAction#timeoutSec
   */
  readonly timeoutSec?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigInitializationAction' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigInitializationAction(obj: ClusterSpecInitProviderClusterConfigInitializationAction | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'script': obj.script,
    'timeoutSec': obj.timeoutSec,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigLifecycleConfig
 */
export interface ClusterSpecInitProviderClusterConfigLifecycleConfig {
  /**
   * The time when cluster will be auto-deleted. A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
   *
   * @schema ClusterSpecInitProviderClusterConfigLifecycleConfig#autoDeleteTime
   */
  readonly autoDeleteTime?: string;

  /**
   * The duration to keep the cluster alive while idling (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
   *
   * @schema ClusterSpecInitProviderClusterConfigLifecycleConfig#idleDeleteTtl
   */
  readonly idleDeleteTtl?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigLifecycleConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigLifecycleConfig(obj: ClusterSpecInitProviderClusterConfigLifecycleConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoDeleteTime': obj.autoDeleteTime,
    'idleDeleteTtl': obj.idleDeleteTtl,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigMasterConfig
 */
export interface ClusterSpecInitProviderClusterConfigMasterConfig {
  /**
   * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfig#accelerators
   */
  readonly accelerators?: ClusterSpecInitProviderClusterConfigMasterConfigAccelerators[];

  /**
   * Disk Config
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig[];

  /**
   * The URI for the image to use for this worker.  See the guide for more information.
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfig#imageUri
   */
  readonly imageUri?: string;

  /**
   * The name of a Google Compute Engine machine type to create for the master. If not specified, GCP will default to a predetermined computed value (currently n1-standard-4).
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfig#machineType
   */
  readonly machineType?: string;

  /**
   * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone. See the guide for details about which CPU families are available (and defaulted) for each zone.
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Specifies the number of master nodes to create. If not specified, GCP will default to a predetermined computed value (currently 1).
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfig#numInstances
   */
  readonly numInstances?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigMasterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigMasterConfig(obj: ClusterSpecInitProviderClusterConfigMasterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_ClusterSpecInitProviderClusterConfigMasterConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig(y)),
    'imageUri': obj.imageUri,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigMetastoreConfig
 */
export interface ClusterSpecInitProviderClusterConfigMetastoreConfig {
  /**
   * Resource name of an existing Dataproc Metastore service.
   *
   * @schema ClusterSpecInitProviderClusterConfigMetastoreConfig#dataprocMetastoreService
   */
  readonly dataprocMetastoreService?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigMetastoreConfig(obj: ClusterSpecInitProviderClusterConfigMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocMetastoreService': obj.dataprocMetastoreService,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig
 */
export interface ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig {
  /**
   * Disk Config
   *
   * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig[];

  /**
   * Specifies the number of preemptible nodes to create. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE Accepted values are:
   *
   * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig(obj: ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig(y)),
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigSecurityConfig
 */
export interface ClusterSpecInitProviderClusterConfigSecurityConfig {
  /**
   * Kerberos Configuration
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfig#kerberosConfig
   */
  readonly kerberosConfig?: ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigSecurityConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigSecurityConfig(obj: ClusterSpecInitProviderClusterConfigSecurityConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigSoftwareConfig
 */
export interface ClusterSpecInitProviderClusterConfigSoftwareConfig {
  /**
   * The Cloud Dataproc image version to use for the cluster - this controls the sets of software versions installed onto the nodes when you create clusters. If not specified, defaults to the latest version. For a list of valid versions see Cloud Dataproc versions
   *
   * @schema ClusterSpecInitProviderClusterConfigSoftwareConfig#imageVersion
   */
  readonly imageVersion?: string;

  /**
   * The set of optional components to activate on the cluster. See Available Optional Components.
   *
   * @schema ClusterSpecInitProviderClusterConfigSoftwareConfig#optionalComponents
   */
  readonly optionalComponents?: string[];

  /**
   * A list of override and additional properties (key/value pairs) used to modify various aspects of the common configuration files used when creating a cluster. For a list of valid properties please see Cluster properties
   *
   * @schema ClusterSpecInitProviderClusterConfigSoftwareConfig#overrideProperties
   */
  readonly overrideProperties?: { [key: string]: string };

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigSoftwareConfig(obj: ClusterSpecInitProviderClusterConfigSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'imageVersion': obj.imageVersion,
    'optionalComponents': obj.optionalComponents?.map(y => y),
    'overrideProperties': ((obj.overrideProperties) === undefined) ? undefined : (Object.entries(obj.overrideProperties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigWorkerConfig
 */
export interface ClusterSpecInitProviderClusterConfigWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfig#accelerators
   */
  readonly accelerators?: ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators[];

  /**
   * Disk Config
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig[];

  /**
   * The URI for the image to use for this worker.  See the guide for more information.
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfig#imageUri
   */
  readonly imageUri?: string;

  /**
   * The name of a Google Compute Engine machine type to create for the worker nodes. If not specified, GCP will default to a predetermined computed value (currently n1-standard-4).
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone. See the guide for details about which CPU families are available (and defaulted) for each zone.
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Specifies the number of worker nodes to create. If not specified, GCP will default to a predetermined computed value (currently 2). There is currently a beta feature which allows you to run a Single Node Cluster. In order to take advantage of this you need to set "dataproc:dataproc.allow.zero.workers" = "true" in cluster_config.software_config.properties
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfig#numInstances
   */
  readonly numInstances?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigWorkerConfig(obj: ClusterSpecInitProviderClusterConfigWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig(y)),
    'imageUri': obj.imageUri,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig {
  /**
   * The config setting for metastore service with the cluster. Structure defined below.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig#metastoreConfig
   */
  readonly metastoreConfig?: ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig[];

  /**
   * The Spark History Server configuration for the workload.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig#sparkHistoryServerConfig
   */
  readonly sparkHistoryServerConfig?: ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig(obj: ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metastoreConfig': obj.metastoreConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig(y)),
    'sparkHistoryServerConfig': obj.sparkHistoryServerConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig {
  /**
   * The configuration for running the Dataproc cluster on GKE.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig#gkeClusterConfig
   */
  readonly gkeClusterConfig?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig[];

  /**
   * A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig#kubernetesNamespace
   */
  readonly kubernetesNamespace?: string;

  /**
   * The software configuration for this Dataproc cluster running on Kubernetes.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig#kubernetesSoftwareConfig
   */
  readonly kubernetesSoftwareConfig?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gkeClusterConfig': obj.gkeClusterConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig(y)),
    'kubernetesNamespace': obj.kubernetesNamespace,
    'kubernetesSoftwareConfig': obj.kubernetesSoftwareConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecProviderConfigRefPolicyResolution
 */
export enum ClusterSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecProviderConfigRefPolicyResolve
 */
export enum ClusterSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface ClusterSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsToConfigRefPolicy(obj: ClusterSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics
 */
export interface ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics {
  /**
   * One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
   *
   * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics#metricOverrides
   */
  readonly metricOverrides?: string[];

  /**
   * A source for the collection of Dataproc OSS metrics (see available OSS metrics).
   *
   * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics#metricSource
   */
  readonly metricSource?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics(obj: ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metricOverrides': obj.metricOverrides?.map(y => y),
    'metricSource': obj.metricSource,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity {
  /**
   * The URI of a sole-tenant node group resource that the cluster will be created on.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity#nodeGroupUri
   */
  readonly nodeGroupUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity(obj: ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodeGroupUri': obj.nodeGroupUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity {
  /**
   * Corresponds to the type of reservation consumption.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity#consumeReservationType
   */
  readonly consumeReservationType?: string;

  /**
   * Corresponds to the label key of reservation resource.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity#key
   */
  readonly key?: string;

  /**
   * Corresponds to the label values of reservation resource.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity(obj: ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumeReservationType': obj.consumeReservationType,
    'key': obj.key,
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef#policy
   */
  readonly policy?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector#policy
   */
  readonly policy?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig {
  /**
   * Defines whether instances have integrity monitoring enabled.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableIntegrityMonitoring
   */
  readonly enableIntegrityMonitoring?: boolean;

  /**
   * Defines whether instances have Secure Boot enabled.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableSecureBoot
   */
  readonly enableSecureBoot?: boolean;

  /**
   * Defines whether instances have the vTPM enabled.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableVtpm
   */
  readonly enableVtpm?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig(obj: ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableIntegrityMonitoring': obj.enableIntegrityMonitoring,
    'enableSecureBoot': obj.enableSecureBoot,
    'enableVtpm': obj.enableVtpm,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMasterConfigAccelerators
 */
export interface ClusterSpecForProviderClusterConfigMasterConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMasterConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMasterConfigAccelerators(obj: ClusterSpecForProviderClusterConfigMasterConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig
 */
export interface ClusterSpecForProviderClusterConfigMasterConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMasterConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMasterConfigDiskConfig(obj: ClusterSpecForProviderClusterConfigMasterConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig
 */
export interface ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig(obj: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig
 */
export interface ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig {
  /**
   * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustAdminServer
   */
  readonly crossRealmTrustAdminServer?: string;

  /**
   * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustKdc
   */
  readonly crossRealmTrustKdc?: string;

  /**
   * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustRealm
   */
  readonly crossRealmTrustRealm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustSharedPasswordUri
   */
  readonly crossRealmTrustSharedPasswordUri?: string;

  /**
   * Flag to indicate whether to Kerberize the cluster.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#enableKerberos
   */
  readonly enableKerberos?: boolean;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#kdcDbKeyUri
   */
  readonly kdcDbKeyUri?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#keyPasswordUri
   */
  readonly keyPasswordUri?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificated, the password is generated by Dataproc.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#keystorePasswordUri
   */
  readonly keystorePasswordUri?: string;

  /**
   * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#keystoreUri
   */
  readonly keystoreUri?: string;

  /**
   * The URI of the KMS key used to encrypt various sensitive files.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#kmsKeyUri
   */
  readonly kmsKeyUri?: string;

  /**
   * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#realm
   */
  readonly realm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the root principal password.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#rootPrincipalPasswordUri
   */
  readonly rootPrincipalPasswordUri?: string;

  /**
   * The lifetime of the ticket granting ticket, in hours.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#tgtLifetimeHours
   */
  readonly tgtLifetimeHours?: number;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#truststorePasswordUri
   */
  readonly truststorePasswordUri?: string;

  /**
   * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#truststoreUri
   */
  readonly truststoreUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig(obj: ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'crossRealmTrustAdminServer': obj.crossRealmTrustAdminServer,
    'crossRealmTrustKdc': obj.crossRealmTrustKdc,
    'crossRealmTrustRealm': obj.crossRealmTrustRealm,
    'crossRealmTrustSharedPasswordUri': obj.crossRealmTrustSharedPasswordUri,
    'enableKerberos': obj.enableKerberos,
    'kdcDbKeyUri': obj.kdcDbKeyUri,
    'keyPasswordUri': obj.keyPasswordUri,
    'keystorePasswordUri': obj.keystorePasswordUri,
    'keystoreUri': obj.keystoreUri,
    'kmsKeyUri': obj.kmsKeyUri,
    'realm': obj.realm,
    'rootPrincipalPasswordUri': obj.rootPrincipalPasswordUri,
    'tgtLifetimeHours': obj.tgtLifetimeHours,
    'truststorePasswordUri': obj.truststorePasswordUri,
    'truststoreUri': obj.truststoreUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigWorkerConfigAccelerators
 */
export interface ClusterSpecForProviderClusterConfigWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigWorkerConfigAccelerators(obj: ClusterSpecForProviderClusterConfigWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig
 */
export interface ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig(obj: ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
  /**
   * Resource name of an existing Dataproc Metastore service.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig#dataprocMetastoreService
   */
  readonly dataprocMetastoreService?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig(obj: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocMetastoreService': obj.dataprocMetastoreService,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
  /**
   * Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig#dataprocCluster
   */
  readonly dataprocCluster?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig(obj: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocCluster': obj.dataprocCluster,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
  /**
   * A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional)
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig#gkeClusterTarget
   */
  readonly gkeClusterTarget?: string;

  /**
   * GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig#nodePoolTarget
   */
  readonly nodePoolTarget?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gkeClusterTarget': obj.gkeClusterTarget,
    'nodePoolTarget': obj.nodePoolTarget?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
  /**
   * The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig#componentVersion
   */
  readonly componentVersion?: { [key: string]: string };

  /**
   * The properties to set on daemon config files. Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'componentVersion': ((obj.componentVersion) === undefined) ? undefined : (Object.entries(obj.componentVersion).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics
 */
export interface ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics {
  /**
   * One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
   *
   * @schema ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics#metricOverrides
   */
  readonly metricOverrides?: string[];

  /**
   * A source for the collection of Dataproc OSS metrics (see available OSS metrics).
   *
   * @schema ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics#metricSource
   */
  readonly metricSource?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics(obj: ClusterSpecInitProviderClusterConfigDataprocMetricConfigMetrics | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metricOverrides': obj.metricOverrides?.map(y => y),
    'metricSource': obj.metricSource,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity
 */
export interface ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity {
  /**
   * The URI of a sole-tenant node group resource that the cluster will be created on.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity#nodeGroupUri
   */
  readonly nodeGroupUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity(obj: ClusterSpecInitProviderClusterConfigGceClusterConfigNodeGroupAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodeGroupUri': obj.nodeGroupUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity
 */
export interface ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity {
  /**
   * Corresponds to the type of reservation consumption.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity#consumeReservationType
   */
  readonly consumeReservationType?: string;

  /**
   * Corresponds to the label key of reservation resource.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity#key
   */
  readonly key?: string;

  /**
   * Corresponds to the label values of reservation resource.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity(obj: ClusterSpecInitProviderClusterConfigGceClusterConfigReservationAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumeReservationType': obj.consumeReservationType,
    'key': obj.key,
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig
 */
export interface ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig {
  /**
   * Defines whether instances have integrity monitoring enabled.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableIntegrityMonitoring
   */
  readonly enableIntegrityMonitoring?: boolean;

  /**
   * Defines whether instances have Secure Boot enabled.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableSecureBoot
   */
  readonly enableSecureBoot?: boolean;

  /**
   * Defines whether instances have the vTPM enabled.
   *
   * @schema ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableVtpm
   */
  readonly enableVtpm?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig(obj: ClusterSpecInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableIntegrityMonitoring': obj.enableIntegrityMonitoring,
    'enableSecureBoot': obj.enableSecureBoot,
    'enableVtpm': obj.enableVtpm,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigMasterConfigAccelerators
 */
export interface ClusterSpecInitProviderClusterConfigMasterConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigMasterConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigMasterConfigAccelerators(obj: ClusterSpecInitProviderClusterConfigMasterConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig
 */
export interface ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig(obj: ClusterSpecInitProviderClusterConfigMasterConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig
 */
export interface ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig(obj: ClusterSpecInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig
 */
export interface ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig {
  /**
   * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustAdminServer
   */
  readonly crossRealmTrustAdminServer?: string;

  /**
   * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustKdc
   */
  readonly crossRealmTrustKdc?: string;

  /**
   * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustRealm
   */
  readonly crossRealmTrustRealm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustSharedPasswordUri
   */
  readonly crossRealmTrustSharedPasswordUri?: string;

  /**
   * Flag to indicate whether to Kerberize the cluster.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#enableKerberos
   */
  readonly enableKerberos?: boolean;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#kdcDbKeyUri
   */
  readonly kdcDbKeyUri?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#keyPasswordUri
   */
  readonly keyPasswordUri?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificated, the password is generated by Dataproc.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#keystorePasswordUri
   */
  readonly keystorePasswordUri?: string;

  /**
   * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#keystoreUri
   */
  readonly keystoreUri?: string;

  /**
   * The URI of the KMS key used to encrypt various sensitive files.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#kmsKeyUri
   */
  readonly kmsKeyUri?: string;

  /**
   * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#realm
   */
  readonly realm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the root principal password.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#rootPrincipalPasswordUri
   */
  readonly rootPrincipalPasswordUri?: string;

  /**
   * The lifetime of the ticket granting ticket, in hours.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#tgtLifetimeHours
   */
  readonly tgtLifetimeHours?: number;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#truststorePasswordUri
   */
  readonly truststorePasswordUri?: string;

  /**
   * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig#truststoreUri
   */
  readonly truststoreUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig(obj: ClusterSpecInitProviderClusterConfigSecurityConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'crossRealmTrustAdminServer': obj.crossRealmTrustAdminServer,
    'crossRealmTrustKdc': obj.crossRealmTrustKdc,
    'crossRealmTrustRealm': obj.crossRealmTrustRealm,
    'crossRealmTrustSharedPasswordUri': obj.crossRealmTrustSharedPasswordUri,
    'enableKerberos': obj.enableKerberos,
    'kdcDbKeyUri': obj.kdcDbKeyUri,
    'keyPasswordUri': obj.keyPasswordUri,
    'keystorePasswordUri': obj.keystorePasswordUri,
    'keystoreUri': obj.keystoreUri,
    'kmsKeyUri': obj.kmsKeyUri,
    'realm': obj.realm,
    'rootPrincipalPasswordUri': obj.rootPrincipalPasswordUri,
    'tgtLifetimeHours': obj.tgtLifetimeHours,
    'truststorePasswordUri': obj.truststorePasswordUri,
    'truststoreUri': obj.truststoreUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators
 */
export interface ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators(obj: ClusterSpecInitProviderClusterConfigWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig
 */
export interface ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig(obj: ClusterSpecInitProviderClusterConfigWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
  /**
   * Resource name of an existing Dataproc Metastore service.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig#dataprocMetastoreService
   */
  readonly dataprocMetastoreService?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig(obj: ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocMetastoreService': obj.dataprocMetastoreService,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
  /**
   * Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig#dataprocCluster
   */
  readonly dataprocCluster?: string;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig(obj: ClusterSpecInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocCluster': obj.dataprocCluster,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
  /**
   * A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional)
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig#gkeClusterTarget
   */
  readonly gkeClusterTarget?: string;

  /**
   * GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig#nodePoolTarget
   */
  readonly nodePoolTarget?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gkeClusterTarget': obj.gkeClusterTarget,
    'nodePoolTarget': obj.nodePoolTarget?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
  /**
   * The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig#componentVersion
   */
  readonly componentVersion?: { [key: string]: string };

  /**
   * The properties to set on daemon config files. Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'componentVersion': ((obj.componentVersion) === undefined) ? undefined : (Object.entries(obj.componentVersion).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy#resolution
   */
  readonly resolution?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy#resolve
   */
  readonly resolve?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
  /**
   * The target GKE node pool.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#nodePool
   */
  readonly nodePool?: string;

  /**
   * (Input only) The configuration for the GKE node pool. If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#nodePoolConfig
   */
  readonly nodePoolConfig?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig[];

  /**
   * The roles associated with the GKE node pool. One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#roles
   */
  readonly roles?: string[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodePool': obj.nodePool,
    'nodePoolConfig': obj.nodePoolConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig(y)),
    'roles': obj.roles?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
  /**
   * The target GKE node pool.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#nodePool
   */
  readonly nodePool?: string;

  /**
   * (Input only) The configuration for the GKE node pool. If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#nodePoolConfig
   */
  readonly nodePoolConfig?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig[];

  /**
   * The roles associated with the GKE node pool. One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#roles
   */
  readonly roles?: string[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodePool': obj.nodePool,
    'nodePoolConfig': obj.nodePoolConfig?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig(y)),
    'roles': obj.roles?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
  /**
   * The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#autoscaling
   */
  readonly autoscaling?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling[];

  /**
   * The node pool configuration.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#config
   */
  readonly config?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig[];

  /**
   * The list of Compute Engine zones where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#locations
   */
  readonly locations?: string[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscaling': obj.autoscaling?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling(y)),
    'config': obj.config?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig(y)),
    'locations': obj.locations?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
  /**
   * The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#autoscaling
   */
  readonly autoscaling?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling[];

  /**
   * The node pool configuration.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#config
   */
  readonly config?: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig[];

  /**
   * The list of Compute Engine zones where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#locations
   */
  readonly locations?: string[];

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscaling': obj.autoscaling?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling(y)),
    'config': obj.config?.map(y => toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig(y)),
    'locations': obj.locations?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
  /**
   * The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling#maxNodeCount
   */
  readonly maxNodeCount?: number;

  /**
   * The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling#minNodeCount
   */
  readonly minNodeCount?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxNodeCount': obj.maxNodeCount,
    'minNodeCount': obj.minNodeCount,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
  /**
   * The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#localSsdCount
   */
  readonly localSsdCount?: number;

  /**
   * The name of a Compute Engine machine type.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Whether the nodes are created as preemptible VM instances. Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#preemptible
   */
  readonly preemptible?: boolean;

  /**
   * Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#spot
   */
  readonly spot?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'localSsdCount': obj.localSsdCount,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'preemptible': obj.preemptible,
    'spot': obj.spot,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
  /**
   * The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling#maxNodeCount
   */
  readonly maxNodeCount?: number;

  /**
   * The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling#minNodeCount
   */
  readonly minNodeCount?: number;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxNodeCount': obj.maxNodeCount,
    'minNodeCount': obj.minNodeCount,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig
 */
export interface ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
  /**
   * The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#localSsdCount
   */
  readonly localSsdCount?: number;

  /**
   * The name of a Compute Engine machine type.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Whether the nodes are created as preemptible VM instances. Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#preemptible
   */
  readonly preemptible?: boolean;

  /**
   * Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
   *
   * @schema ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#spot
   */
  readonly spot?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig(obj: ClusterSpecInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'localSsdCount': obj.localSsdCount,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'preemptible': obj.preemptible,
    'spot': obj.spot,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */


/**
 * Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.
 *
 * @schema Job
 */
export class Job extends ApiObject {
  /**
   * Returns the apiVersion and kind for "Job"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'Job',
  }

  /**
   * Renders a Kubernetes manifest for "Job".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: JobProps): any {
    return {
      ...Job.GVK,
      ...toJson_JobProps(props),
    };
  }

  /**
   * Defines a "Job" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: JobProps) {
    super(scope, id, {
      ...Job.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...Job.GVK,
      ...toJson_JobProps(resolved),
    };
  }
}

/**
 * Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.
 *
 * @schema Job
 */
export interface JobProps {
  /**
   * @schema Job#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * JobSpec defines the desired state of Job
   *
   * @schema Job#spec
   */
  readonly spec: JobSpec;

}

/**
 * Converts an object of type 'JobProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobProps(obj: JobProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_JobSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * JobSpec defines the desired state of Job
 *
 * @schema JobSpec
 */
export interface JobSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
   *
   * @schema JobSpec#deletionPolicy
   */
  readonly deletionPolicy?: JobSpecDeletionPolicy;

  /**
   * @schema JobSpec#forProvider
   */
  readonly forProvider: JobSpecForProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
   *
   * @schema JobSpec#initProvider
   */
  readonly initProvider?: JobSpecInitProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. ManagementPolicies specify the array of actions Crossplane is allowed to take on the managed and external resources. This field is planned to replace the DeletionPolicy field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. If both are custom, the DeletionPolicy field will be ignored. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223 and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
   *
   * @schema JobSpec#managementPolicies
   */
  readonly managementPolicies?: JobSpecManagementPolicies[];

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema JobSpec#providerConfigRef
   */
  readonly providerConfigRef?: JobSpecProviderConfigRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema JobSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: JobSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema JobSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: JobSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'JobSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpec(obj: JobSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_JobSpecForProvider(obj.forProvider),
    'initProvider': toJson_JobSpecInitProvider(obj.initProvider),
    'managementPolicies': obj.managementPolicies?.map(y => y),
    'providerConfigRef': toJson_JobSpecProviderConfigRef(obj.providerConfigRef),
    'publishConnectionDetailsTo': toJson_JobSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_JobSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
 *
 * @schema JobSpecDeletionPolicy
 */
export enum JobSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema JobSpecForProvider
 */
export interface JobSpecForProvider {
  /**
   * By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure that the job is first cancelled before issuing the delete.
   *
   * @schema JobSpecForProvider#forceDelete
   */
  readonly forceDelete?: boolean;

  /**
   * @schema JobSpecForProvider#hadoopConfig
   */
  readonly hadoopConfig?: JobSpecForProviderHadoopConfig[];

  /**
   * @schema JobSpecForProvider#hiveConfig
   */
  readonly hiveConfig?: JobSpecForProviderHiveConfig[];

  /**
   * The list of labels (key/value pairs) to add to the job.
   *
   * @schema JobSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * @schema JobSpecForProvider#pigConfig
   */
  readonly pigConfig?: JobSpecForProviderPigConfig[];

  /**
   * @schema JobSpecForProvider#placement
   */
  readonly placement?: JobSpecForProviderPlacement[];

  /**
   * @schema JobSpecForProvider#prestoConfig
   */
  readonly prestoConfig?: JobSpecForProviderPrestoConfig[];

  /**
   * The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider project is used.
   *
   * @schema JobSpecForProvider#project
   */
  readonly project?: string;

  /**
   * @schema JobSpecForProvider#pysparkConfig
   */
  readonly pysparkConfig?: JobSpecForProviderPysparkConfig[];

  /**
   * @schema JobSpecForProvider#reference
   */
  readonly reference?: JobSpecForProviderReference[];

  /**
   * The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If not specified, defaults to global.
   *
   * @schema JobSpecForProvider#region
   */
  readonly region?: string;

  /**
   * Reference to a Cluster in dataproc to populate region.
   *
   * @schema JobSpecForProvider#regionRef
   */
  readonly regionRef?: JobSpecForProviderRegionRef;

  /**
   * Selector for a Cluster in dataproc to populate region.
   *
   * @schema JobSpecForProvider#regionSelector
   */
  readonly regionSelector?: JobSpecForProviderRegionSelector;

  /**
   * @schema JobSpecForProvider#scheduling
   */
  readonly scheduling?: JobSpecForProviderScheduling[];

  /**
   * @schema JobSpecForProvider#sparkConfig
   */
  readonly sparkConfig?: JobSpecForProviderSparkConfig[];

  /**
   * @schema JobSpecForProvider#sparksqlConfig
   */
  readonly sparksqlConfig?: JobSpecForProviderSparksqlConfig[];

}

/**
 * Converts an object of type 'JobSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProvider(obj: JobSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'forceDelete': obj.forceDelete,
    'hadoopConfig': obj.hadoopConfig?.map(y => toJson_JobSpecForProviderHadoopConfig(y)),
    'hiveConfig': obj.hiveConfig?.map(y => toJson_JobSpecForProviderHiveConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pigConfig': obj.pigConfig?.map(y => toJson_JobSpecForProviderPigConfig(y)),
    'placement': obj.placement?.map(y => toJson_JobSpecForProviderPlacement(y)),
    'prestoConfig': obj.prestoConfig?.map(y => toJson_JobSpecForProviderPrestoConfig(y)),
    'project': obj.project,
    'pysparkConfig': obj.pysparkConfig?.map(y => toJson_JobSpecForProviderPysparkConfig(y)),
    'reference': obj.reference?.map(y => toJson_JobSpecForProviderReference(y)),
    'region': obj.region,
    'regionRef': toJson_JobSpecForProviderRegionRef(obj.regionRef),
    'regionSelector': toJson_JobSpecForProviderRegionSelector(obj.regionSelector),
    'scheduling': obj.scheduling?.map(y => toJson_JobSpecForProviderScheduling(y)),
    'sparkConfig': obj.sparkConfig?.map(y => toJson_JobSpecForProviderSparkConfig(y)),
    'sparksqlConfig': obj.sparksqlConfig?.map(y => toJson_JobSpecForProviderSparksqlConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
 *
 * @schema JobSpecInitProvider
 */
export interface JobSpecInitProvider {
  /**
   * By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure that the job is first cancelled before issuing the delete.
   *
   * @schema JobSpecInitProvider#forceDelete
   */
  readonly forceDelete?: boolean;

  /**
   * @schema JobSpecInitProvider#hadoopConfig
   */
  readonly hadoopConfig?: JobSpecInitProviderHadoopConfig[];

  /**
   * @schema JobSpecInitProvider#hiveConfig
   */
  readonly hiveConfig?: JobSpecInitProviderHiveConfig[];

  /**
   * The list of labels (key/value pairs) to add to the job.
   *
   * @schema JobSpecInitProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * @schema JobSpecInitProvider#pigConfig
   */
  readonly pigConfig?: JobSpecInitProviderPigConfig[];

  /**
   * @schema JobSpecInitProvider#placement
   */
  readonly placement?: any[];

  /**
   * @schema JobSpecInitProvider#prestoConfig
   */
  readonly prestoConfig?: JobSpecInitProviderPrestoConfig[];

  /**
   * The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider project is used.
   *
   * @schema JobSpecInitProvider#project
   */
  readonly project?: string;

  /**
   * @schema JobSpecInitProvider#pysparkConfig
   */
  readonly pysparkConfig?: JobSpecInitProviderPysparkConfig[];

  /**
   * @schema JobSpecInitProvider#reference
   */
  readonly reference?: JobSpecInitProviderReference[];

  /**
   * @schema JobSpecInitProvider#scheduling
   */
  readonly scheduling?: JobSpecInitProviderScheduling[];

  /**
   * @schema JobSpecInitProvider#sparkConfig
   */
  readonly sparkConfig?: JobSpecInitProviderSparkConfig[];

  /**
   * @schema JobSpecInitProvider#sparksqlConfig
   */
  readonly sparksqlConfig?: JobSpecInitProviderSparksqlConfig[];

}

/**
 * Converts an object of type 'JobSpecInitProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProvider(obj: JobSpecInitProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'forceDelete': obj.forceDelete,
    'hadoopConfig': obj.hadoopConfig?.map(y => toJson_JobSpecInitProviderHadoopConfig(y)),
    'hiveConfig': obj.hiveConfig?.map(y => toJson_JobSpecInitProviderHiveConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pigConfig': obj.pigConfig?.map(y => toJson_JobSpecInitProviderPigConfig(y)),
    'placement': obj.placement?.map(y => y),
    'prestoConfig': obj.prestoConfig?.map(y => toJson_JobSpecInitProviderPrestoConfig(y)),
    'project': obj.project,
    'pysparkConfig': obj.pysparkConfig?.map(y => toJson_JobSpecInitProviderPysparkConfig(y)),
    'reference': obj.reference?.map(y => toJson_JobSpecInitProviderReference(y)),
    'scheduling': obj.scheduling?.map(y => toJson_JobSpecInitProviderScheduling(y)),
    'sparkConfig': obj.sparkConfig?.map(y => toJson_JobSpecInitProviderSparkConfig(y)),
    'sparksqlConfig': obj.sparksqlConfig?.map(y => toJson_JobSpecInitProviderSparksqlConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * A ManagementAction represents an action that the Crossplane controllers can take on an external resource.
 *
 * @schema JobSpecManagementPolicies
 */
export enum JobSpecManagementPolicies {
  /** Observe */
  OBSERVE = "Observe",
  /** Create */
  CREATE = "Create",
  /** Update */
  UPDATE = "Update",
  /** Delete */
  DELETE = "Delete",
  /** LateInitialize */
  LATE_INITIALIZE = "LateInitialize",
  /** * */
  VALUE_ = "*",
}

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema JobSpecProviderConfigRef
 */
export interface JobSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecProviderConfigRef#policy
   */
  readonly policy?: JobSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'JobSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecProviderConfigRef(obj: JobSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema JobSpecPublishConnectionDetailsTo
 */
export interface JobSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema JobSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: JobSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema JobSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: JobSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema JobSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsTo(obj: JobSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_JobSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_JobSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema JobSpecWriteConnectionSecretToRef
 */
export interface JobSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema JobSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema JobSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'JobSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecWriteConnectionSecretToRef(obj: JobSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderHadoopConfig
 */
export interface JobSpecForProviderHadoopConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecForProviderHadoopConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema JobSpecForProviderHadoopConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecForProviderHadoopConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
   *
   * @schema JobSpecForProviderHadoopConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderHadoopConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderHadoopConfigLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
   *
   * @schema JobSpecForProviderHadoopConfig#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
   *
   * @schema JobSpecForProviderHadoopConfig#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
   *
   * @schema JobSpecForProviderHadoopConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderHadoopConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderHadoopConfig(obj: JobSpecForProviderHadoopConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderHadoopConfigLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderHiveConfig
 */
export interface JobSpecForProviderHiveConfig {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecForProviderHiveConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
   *
   * @schema JobSpecForProviderHiveConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
   *
   * @schema JobSpecForProviderHiveConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
   *
   * @schema JobSpecForProviderHiveConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderHiveConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
   *
   * @schema JobSpecForProviderHiveConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderHiveConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderHiveConfig(obj: JobSpecForProviderHiveConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPigConfig
 */
export interface JobSpecForProviderPigConfig {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecForProviderPigConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
   *
   * @schema JobSpecForProviderPigConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderPigConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderPigConfigLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
   *
   * @schema JobSpecForProviderPigConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
   *
   * @schema JobSpecForProviderPigConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderPigConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
   *
   * @schema JobSpecForProviderPigConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPigConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPigConfig(obj: JobSpecForProviderPigConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderPigConfigLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPlacement
 */
export interface JobSpecForProviderPlacement {
  /**
   * The name of the cluster where the job will be submitted.
   *
   * @schema JobSpecForProviderPlacement#clusterName
   */
  readonly clusterName?: string;

  /**
   * Reference to a Cluster in dataproc to populate clusterName.
   *
   * @schema JobSpecForProviderPlacement#clusterNameRef
   */
  readonly clusterNameRef?: JobSpecForProviderPlacementClusterNameRef;

  /**
   * Selector for a Cluster in dataproc to populate clusterName.
   *
   * @schema JobSpecForProviderPlacement#clusterNameSelector
   */
  readonly clusterNameSelector?: JobSpecForProviderPlacementClusterNameSelector;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacement' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacement(obj: JobSpecForProviderPlacement | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterName': obj.clusterName,
    'clusterNameRef': toJson_JobSpecForProviderPlacementClusterNameRef(obj.clusterNameRef),
    'clusterNameSelector': toJson_JobSpecForProviderPlacementClusterNameSelector(obj.clusterNameSelector),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPrestoConfig
 */
export interface JobSpecForProviderPrestoConfig {
  /**
   * Presto client tags to attach to this query.
   *
   * @schema JobSpecForProviderPrestoConfig#clientTags
   */
  readonly clientTags?: string[];

  /**
   * Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecForProviderPrestoConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * @schema JobSpecForProviderPrestoConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderPrestoConfigLoggingConfig[];

  /**
   * The format in which query output will be displayed. See the Presto documentation for supported output formats.
   *
   * @schema JobSpecForProviderPrestoConfig#outputFormat
   */
  readonly outputFormat?: string;

  /**
   * A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
   *
   * @schema JobSpecForProviderPrestoConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
   *
   * @schema JobSpecForProviderPrestoConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderPrestoConfig#queryList
   */
  readonly queryList?: string[];

}

/**
 * Converts an object of type 'JobSpecForProviderPrestoConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPrestoConfig(obj: JobSpecForProviderPrestoConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clientTags': obj.clientTags?.map(y => y),
    'continueOnFailure': obj.continueOnFailure,
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderPrestoConfigLoggingConfig(y)),
    'outputFormat': obj.outputFormat,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPysparkConfig
 */
export interface JobSpecForProviderPysparkConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecForProviderPysparkConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver.
   *
   * @schema JobSpecForProviderPysparkConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecForProviderPysparkConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
   *
   * @schema JobSpecForProviderPysparkConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderPysparkConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderPysparkConfigLoggingConfig[];

  /**
   * The HCFS URI of the main Python file to use as the driver. Must be a .py file.
   *
   * @schema JobSpecForProviderPysparkConfig#mainPythonFileUri
   */
  readonly mainPythonFileUri?: string;

  /**
   * A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @schema JobSpecForProviderPysparkConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
   *
   * @schema JobSpecForProviderPysparkConfig#pythonFileUris
   */
  readonly pythonFileUris?: string[];

}

/**
 * Converts an object of type 'JobSpecForProviderPysparkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPysparkConfig(obj: JobSpecForProviderPysparkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderPysparkConfigLoggingConfig(y)),
    'mainPythonFileUri': obj.mainPythonFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pythonFileUris': obj.pythonFileUris?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderReference
 */
export interface JobSpecForProviderReference {
  /**
   * @schema JobSpecForProviderReference#jobId
   */
  readonly jobId?: string;

}

/**
 * Converts an object of type 'JobSpecForProviderReference' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderReference(obj: JobSpecForProviderReference | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jobId': obj.jobId,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a Cluster in dataproc to populate region.
 *
 * @schema JobSpecForProviderRegionRef
 */
export interface JobSpecForProviderRegionRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecForProviderRegionRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecForProviderRegionRef#policy
   */
  readonly policy?: JobSpecForProviderRegionRefPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionRef(obj: JobSpecForProviderRegionRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecForProviderRegionRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a Cluster in dataproc to populate region.
 *
 * @schema JobSpecForProviderRegionSelector
 */
export interface JobSpecForProviderRegionSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema JobSpecForProviderRegionSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema JobSpecForProviderRegionSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema JobSpecForProviderRegionSelector#policy
   */
  readonly policy?: JobSpecForProviderRegionSelectorPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionSelector(obj: JobSpecForProviderRegionSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_JobSpecForProviderRegionSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderScheduling
 */
export interface JobSpecForProviderScheduling {
  /**
   * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
   *
   * @schema JobSpecForProviderScheduling#maxFailuresPerHour
   */
  readonly maxFailuresPerHour?: number;

  /**
   * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
   *
   * @schema JobSpecForProviderScheduling#maxFailuresTotal
   */
  readonly maxFailuresTotal?: number;

}

/**
 * Converts an object of type 'JobSpecForProviderScheduling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderScheduling(obj: JobSpecForProviderScheduling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxFailuresPerHour': obj.maxFailuresPerHour,
    'maxFailuresTotal': obj.maxFailuresTotal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparkConfig
 */
export interface JobSpecForProviderSparkConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecForProviderSparkConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver.
   *
   * @schema JobSpecForProviderSparkConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecForProviderSparkConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
   *
   * @schema JobSpecForProviderSparkConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderSparkConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderSparkConfigLoggingConfig[];

  /**
   * The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
   *
   * @schema JobSpecForProviderSparkConfig#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of jar file containing the driver jar. Conflicts with main_class
   *
   * @schema JobSpecForProviderSparkConfig#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @schema JobSpecForProviderSparkConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparkConfig(obj: JobSpecForProviderSparkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderSparkConfigLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparksqlConfig
 */
export interface JobSpecForProviderSparksqlConfig {
  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema JobSpecForProviderSparksqlConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderSparksqlConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderSparksqlConfigLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
   *
   * @schema JobSpecForProviderSparksqlConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
   *
   * @schema JobSpecForProviderSparksqlConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderSparksqlConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema JobSpecForProviderSparksqlConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparksqlConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparksqlConfig(obj: JobSpecForProviderSparksqlConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderSparksqlConfigLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderHadoopConfig
 */
export interface JobSpecInitProviderHadoopConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecInitProviderHadoopConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema JobSpecInitProviderHadoopConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecInitProviderHadoopConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
   *
   * @schema JobSpecInitProviderHadoopConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecInitProviderHadoopConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecInitProviderHadoopConfigLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
   *
   * @schema JobSpecInitProviderHadoopConfig#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
   *
   * @schema JobSpecInitProviderHadoopConfig#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
   *
   * @schema JobSpecInitProviderHadoopConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderHadoopConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderHadoopConfig(obj: JobSpecInitProviderHadoopConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecInitProviderHadoopConfigLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderHiveConfig
 */
export interface JobSpecInitProviderHiveConfig {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecInitProviderHiveConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
   *
   * @schema JobSpecInitProviderHiveConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
   *
   * @schema JobSpecInitProviderHiveConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
   *
   * @schema JobSpecInitProviderHiveConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecInitProviderHiveConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
   *
   * @schema JobSpecInitProviderHiveConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderHiveConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderHiveConfig(obj: JobSpecInitProviderHiveConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderPigConfig
 */
export interface JobSpecInitProviderPigConfig {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecInitProviderPigConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
   *
   * @schema JobSpecInitProviderPigConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecInitProviderPigConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecInitProviderPigConfigLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
   *
   * @schema JobSpecInitProviderPigConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
   *
   * @schema JobSpecInitProviderPigConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecInitProviderPigConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
   *
   * @schema JobSpecInitProviderPigConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderPigConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderPigConfig(obj: JobSpecInitProviderPigConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecInitProviderPigConfigLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderPrestoConfig
 */
export interface JobSpecInitProviderPrestoConfig {
  /**
   * Presto client tags to attach to this query.
   *
   * @schema JobSpecInitProviderPrestoConfig#clientTags
   */
  readonly clientTags?: string[];

  /**
   * Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecInitProviderPrestoConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * @schema JobSpecInitProviderPrestoConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecInitProviderPrestoConfigLoggingConfig[];

  /**
   * The format in which query output will be displayed. See the Presto documentation for supported output formats.
   *
   * @schema JobSpecInitProviderPrestoConfig#outputFormat
   */
  readonly outputFormat?: string;

  /**
   * A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
   *
   * @schema JobSpecInitProviderPrestoConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
   *
   * @schema JobSpecInitProviderPrestoConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecInitProviderPrestoConfig#queryList
   */
  readonly queryList?: string[];

}

/**
 * Converts an object of type 'JobSpecInitProviderPrestoConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderPrestoConfig(obj: JobSpecInitProviderPrestoConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clientTags': obj.clientTags?.map(y => y),
    'continueOnFailure': obj.continueOnFailure,
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecInitProviderPrestoConfigLoggingConfig(y)),
    'outputFormat': obj.outputFormat,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderPysparkConfig
 */
export interface JobSpecInitProviderPysparkConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecInitProviderPysparkConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver.
   *
   * @schema JobSpecInitProviderPysparkConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecInitProviderPysparkConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
   *
   * @schema JobSpecInitProviderPysparkConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecInitProviderPysparkConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecInitProviderPysparkConfigLoggingConfig[];

  /**
   * The HCFS URI of the main Python file to use as the driver. Must be a .py file.
   *
   * @schema JobSpecInitProviderPysparkConfig#mainPythonFileUri
   */
  readonly mainPythonFileUri?: string;

  /**
   * A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @schema JobSpecInitProviderPysparkConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
   *
   * @schema JobSpecInitProviderPysparkConfig#pythonFileUris
   */
  readonly pythonFileUris?: string[];

}

/**
 * Converts an object of type 'JobSpecInitProviderPysparkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderPysparkConfig(obj: JobSpecInitProviderPysparkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecInitProviderPysparkConfigLoggingConfig(y)),
    'mainPythonFileUri': obj.mainPythonFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pythonFileUris': obj.pythonFileUris?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderReference
 */
export interface JobSpecInitProviderReference {
  /**
   * @schema JobSpecInitProviderReference#jobId
   */
  readonly jobId?: string;

}

/**
 * Converts an object of type 'JobSpecInitProviderReference' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderReference(obj: JobSpecInitProviderReference | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jobId': obj.jobId,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderScheduling
 */
export interface JobSpecInitProviderScheduling {
  /**
   * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
   *
   * @schema JobSpecInitProviderScheduling#maxFailuresPerHour
   */
  readonly maxFailuresPerHour?: number;

  /**
   * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
   *
   * @schema JobSpecInitProviderScheduling#maxFailuresTotal
   */
  readonly maxFailuresTotal?: number;

}

/**
 * Converts an object of type 'JobSpecInitProviderScheduling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderScheduling(obj: JobSpecInitProviderScheduling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxFailuresPerHour': obj.maxFailuresPerHour,
    'maxFailuresTotal': obj.maxFailuresTotal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderSparkConfig
 */
export interface JobSpecInitProviderSparkConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecInitProviderSparkConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver.
   *
   * @schema JobSpecInitProviderSparkConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecInitProviderSparkConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
   *
   * @schema JobSpecInitProviderSparkConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecInitProviderSparkConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecInitProviderSparkConfigLoggingConfig[];

  /**
   * The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
   *
   * @schema JobSpecInitProviderSparkConfig#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of jar file containing the driver jar. Conflicts with main_class
   *
   * @schema JobSpecInitProviderSparkConfig#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @schema JobSpecInitProviderSparkConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderSparkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderSparkConfig(obj: JobSpecInitProviderSparkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecInitProviderSparkConfigLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderSparksqlConfig
 */
export interface JobSpecInitProviderSparksqlConfig {
  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema JobSpecInitProviderSparksqlConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecInitProviderSparksqlConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecInitProviderSparksqlConfigLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
   *
   * @schema JobSpecInitProviderSparksqlConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
   *
   * @schema JobSpecInitProviderSparksqlConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecInitProviderSparksqlConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema JobSpecInitProviderSparksqlConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderSparksqlConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderSparksqlConfig(obj: JobSpecInitProviderSparksqlConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecInitProviderSparksqlConfigLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecProviderConfigRefPolicy
 */
export interface JobSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: JobSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: JobSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecProviderConfigRefPolicy(obj: JobSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRef
 */
export interface JobSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: JobSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsToConfigRef(obj: JobSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema JobSpecPublishConnectionDetailsToMetadata
 */
export interface JobSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema JobSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema JobSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema JobSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsToMetadata(obj: JobSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderHadoopConfigLoggingConfig
 */
export interface JobSpecForProviderHadoopConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderHadoopConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderHadoopConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderHadoopConfigLoggingConfig(obj: JobSpecForProviderHadoopConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPigConfigLoggingConfig
 */
export interface JobSpecForProviderPigConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderPigConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPigConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPigConfigLoggingConfig(obj: JobSpecForProviderPigConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a Cluster in dataproc to populate clusterName.
 *
 * @schema JobSpecForProviderPlacementClusterNameRef
 */
export interface JobSpecForProviderPlacementClusterNameRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecForProviderPlacementClusterNameRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecForProviderPlacementClusterNameRef#policy
   */
  readonly policy?: JobSpecForProviderPlacementClusterNameRefPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameRef(obj: JobSpecForProviderPlacementClusterNameRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecForProviderPlacementClusterNameRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a Cluster in dataproc to populate clusterName.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelector
 */
export interface JobSpecForProviderPlacementClusterNameSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelector#policy
   */
  readonly policy?: JobSpecForProviderPlacementClusterNameSelectorPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameSelector(obj: JobSpecForProviderPlacementClusterNameSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_JobSpecForProviderPlacementClusterNameSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPrestoConfigLoggingConfig
 */
export interface JobSpecForProviderPrestoConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderPrestoConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPrestoConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPrestoConfigLoggingConfig(obj: JobSpecForProviderPrestoConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPysparkConfigLoggingConfig
 */
export interface JobSpecForProviderPysparkConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderPysparkConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPysparkConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPysparkConfigLoggingConfig(obj: JobSpecForProviderPysparkConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecForProviderRegionRefPolicy
 */
export interface JobSpecForProviderRegionRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderRegionRefPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderRegionRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderRegionRefPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderRegionRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionRefPolicy(obj: JobSpecForProviderRegionRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema JobSpecForProviderRegionSelectorPolicy
 */
export interface JobSpecForProviderRegionSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderRegionSelectorPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderRegionSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderRegionSelectorPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderRegionSelectorPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionSelectorPolicy(obj: JobSpecForProviderRegionSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparkConfigLoggingConfig
 */
export interface JobSpecForProviderSparkConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderSparkConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparkConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparkConfigLoggingConfig(obj: JobSpecForProviderSparkConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparksqlConfigLoggingConfig
 */
export interface JobSpecForProviderSparksqlConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderSparksqlConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparksqlConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparksqlConfigLoggingConfig(obj: JobSpecForProviderSparksqlConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderHadoopConfigLoggingConfig
 */
export interface JobSpecInitProviderHadoopConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecInitProviderHadoopConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderHadoopConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderHadoopConfigLoggingConfig(obj: JobSpecInitProviderHadoopConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderPigConfigLoggingConfig
 */
export interface JobSpecInitProviderPigConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecInitProviderPigConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderPigConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderPigConfigLoggingConfig(obj: JobSpecInitProviderPigConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderPrestoConfigLoggingConfig
 */
export interface JobSpecInitProviderPrestoConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecInitProviderPrestoConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderPrestoConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderPrestoConfigLoggingConfig(obj: JobSpecInitProviderPrestoConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderPysparkConfigLoggingConfig
 */
export interface JobSpecInitProviderPysparkConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecInitProviderPysparkConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderPysparkConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderPysparkConfigLoggingConfig(obj: JobSpecInitProviderPysparkConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderSparkConfigLoggingConfig
 */
export interface JobSpecInitProviderSparkConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecInitProviderSparkConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderSparkConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderSparkConfigLoggingConfig(obj: JobSpecInitProviderSparkConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecInitProviderSparksqlConfigLoggingConfig
 */
export interface JobSpecInitProviderSparksqlConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecInitProviderSparksqlConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecInitProviderSparksqlConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecInitProviderSparksqlConfigLoggingConfig(obj: JobSpecInitProviderSparksqlConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecProviderConfigRefPolicyResolution
 */
export enum JobSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecProviderConfigRefPolicyResolve
 */
export enum JobSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface JobSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: JobSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: JobSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsToConfigRefPolicy(obj: JobSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecForProviderPlacementClusterNameRefPolicy
 */
export interface JobSpecForProviderPlacementClusterNameRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderPlacementClusterNameRefPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderPlacementClusterNameRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderPlacementClusterNameRefPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderPlacementClusterNameRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameRefPolicy(obj: JobSpecForProviderPlacementClusterNameRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelectorPolicy
 */
export interface JobSpecForProviderPlacementClusterNameSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelectorPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderPlacementClusterNameSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelectorPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderPlacementClusterNameSelectorPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameSelectorPolicy(obj: JobSpecForProviderPlacementClusterNameSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderRegionRefPolicyResolution
 */
export enum JobSpecForProviderRegionRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderRegionRefPolicyResolve
 */
export enum JobSpecForProviderRegionRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderRegionSelectorPolicyResolution
 */
export enum JobSpecForProviderRegionSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderRegionSelectorPolicyResolve
 */
export enum JobSpecForProviderRegionSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum JobSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum JobSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderPlacementClusterNameRefPolicyResolution
 */
export enum JobSpecForProviderPlacementClusterNameRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderPlacementClusterNameRefPolicyResolve
 */
export enum JobSpecForProviderPlacementClusterNameRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelectorPolicyResolution
 */
export enum JobSpecForProviderPlacementClusterNameSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelectorPolicyResolve
 */
export enum JobSpecForProviderPlacementClusterNameSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}


/**
 * MetastoreService is the Schema for the MetastoreServices API. A managed metastore service that serves metadata queries.
 *
 * @schema MetastoreService
 */
export class MetastoreService extends ApiObject {
  /**
   * Returns the apiVersion and kind for "MetastoreService"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'MetastoreService',
  }

  /**
   * Renders a Kubernetes manifest for "MetastoreService".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: MetastoreServiceProps): any {
    return {
      ...MetastoreService.GVK,
      ...toJson_MetastoreServiceProps(props),
    };
  }

  /**
   * Defines a "MetastoreService" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: MetastoreServiceProps) {
    super(scope, id, {
      ...MetastoreService.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...MetastoreService.GVK,
      ...toJson_MetastoreServiceProps(resolved),
    };
  }
}

/**
 * MetastoreService is the Schema for the MetastoreServices API. A managed metastore service that serves metadata queries.
 *
 * @schema MetastoreService
 */
export interface MetastoreServiceProps {
  /**
   * @schema MetastoreService#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * MetastoreServiceSpec defines the desired state of MetastoreService
   *
   * @schema MetastoreService#spec
   */
  readonly spec: MetastoreServiceSpec;

}

/**
 * Converts an object of type 'MetastoreServiceProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceProps(obj: MetastoreServiceProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_MetastoreServiceSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * MetastoreServiceSpec defines the desired state of MetastoreService
 *
 * @schema MetastoreServiceSpec
 */
export interface MetastoreServiceSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
   *
   * @schema MetastoreServiceSpec#deletionPolicy
   */
  readonly deletionPolicy?: MetastoreServiceSpecDeletionPolicy;

  /**
   * @schema MetastoreServiceSpec#forProvider
   */
  readonly forProvider: MetastoreServiceSpecForProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
   *
   * @schema MetastoreServiceSpec#initProvider
   */
  readonly initProvider?: MetastoreServiceSpecInitProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. ManagementPolicies specify the array of actions Crossplane is allowed to take on the managed and external resources. This field is planned to replace the DeletionPolicy field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. If both are custom, the DeletionPolicy field will be ignored. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223 and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
   *
   * @schema MetastoreServiceSpec#managementPolicies
   */
  readonly managementPolicies?: MetastoreServiceSpecManagementPolicies[];

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema MetastoreServiceSpec#providerConfigRef
   */
  readonly providerConfigRef?: MetastoreServiceSpecProviderConfigRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema MetastoreServiceSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: MetastoreServiceSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema MetastoreServiceSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: MetastoreServiceSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'MetastoreServiceSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpec(obj: MetastoreServiceSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_MetastoreServiceSpecForProvider(obj.forProvider),
    'initProvider': toJson_MetastoreServiceSpecInitProvider(obj.initProvider),
    'managementPolicies': obj.managementPolicies?.map(y => y),
    'providerConfigRef': toJson_MetastoreServiceSpecProviderConfigRef(obj.providerConfigRef),
    'publishConnectionDetailsTo': toJson_MetastoreServiceSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_MetastoreServiceSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
 *
 * @schema MetastoreServiceSpecDeletionPolicy
 */
export enum MetastoreServiceSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema MetastoreServiceSpecForProvider
 */
export interface MetastoreServiceSpecForProvider {
  /**
   * The database type that the Metastore service stores its data. Default value is MYSQL. Possible values are: MYSQL, SPANNER.
   *
   * @schema MetastoreServiceSpecForProvider#databaseType
   */
  readonly databaseType?: string;

  /**
   * Information used to configure the Dataproc Metastore service to encrypt customer data at rest. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#encryptionConfig
   */
  readonly encryptionConfig?: MetastoreServiceSpecForProviderEncryptionConfig[];

  /**
   * Configuration information specific to running Hive metastore software as the metastore service. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#hiveMetastoreConfig
   */
  readonly hiveMetastoreConfig?: MetastoreServiceSpecForProviderHiveMetastoreConfig[];

  /**
   * User-defined labels for the metastore service.
   *
   * @schema MetastoreServiceSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The location where the metastore service should reside. The default value is global.
   *
   * @schema MetastoreServiceSpecForProvider#location
   */
  readonly location?: string;

  /**
   * The one hour maintenance window of the metastore service. This specifies when the service can be restarted for maintenance purposes in UTC time. Maintenance window is not needed for services with the SPANNER database type. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#maintenanceWindow
   */
  readonly maintenanceWindow?: MetastoreServiceSpecForProviderMaintenanceWindow[];

  /**
   * The relative resource name of the VPC network on which the instance can be accessed. It is specified in the following form: "projects/{projectNumber}/global/networks/{network_id}".
   *
   * @schema MetastoreServiceSpecForProvider#network
   */
  readonly network?: string;

  /**
   * The configuration specifying the network settings for the Dataproc Metastore service. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#networkConfig
   */
  readonly networkConfig?: MetastoreServiceSpecForProviderNetworkConfig[];

  /**
   * The TCP port at which the metastore service is reached. Default: 9083.
   *
   * @schema MetastoreServiceSpecForProvider#port
   */
  readonly port?: number;

  /**
   * The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
   *
   * @schema MetastoreServiceSpecForProvider#project
   */
  readonly project?: string;

  /**
   * The release channel of the service. If unspecified, defaults to STABLE. Default value is STABLE. Possible values are: CANARY, STABLE.
   *
   * @schema MetastoreServiceSpecForProvider#releaseChannel
   */
  readonly releaseChannel?: string;

  /**
   * The configuration specifying telemetry settings for the Dataproc Metastore service. If unspecified defaults to JSON. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#telemetryConfig
   */
  readonly telemetryConfig?: MetastoreServiceSpecForProviderTelemetryConfig[];

  /**
   * The tier of the service. Possible values are: DEVELOPER, ENTERPRISE.
   *
   * @schema MetastoreServiceSpecForProvider#tier
   */
  readonly tier?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProvider(obj: MetastoreServiceSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'databaseType': obj.databaseType,
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_MetastoreServiceSpecForProviderEncryptionConfig(y)),
    'hiveMetastoreConfig': obj.hiveMetastoreConfig?.map(y => toJson_MetastoreServiceSpecForProviderHiveMetastoreConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'location': obj.location,
    'maintenanceWindow': obj.maintenanceWindow?.map(y => toJson_MetastoreServiceSpecForProviderMaintenanceWindow(y)),
    'network': obj.network,
    'networkConfig': obj.networkConfig?.map(y => toJson_MetastoreServiceSpecForProviderNetworkConfig(y)),
    'port': obj.port,
    'project': obj.project,
    'releaseChannel': obj.releaseChannel,
    'telemetryConfig': obj.telemetryConfig?.map(y => toJson_MetastoreServiceSpecForProviderTelemetryConfig(y)),
    'tier': obj.tier,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
 *
 * @schema MetastoreServiceSpecInitProvider
 */
export interface MetastoreServiceSpecInitProvider {
  /**
   * The database type that the Metastore service stores its data. Default value is MYSQL. Possible values are: MYSQL, SPANNER.
   *
   * @schema MetastoreServiceSpecInitProvider#databaseType
   */
  readonly databaseType?: string;

  /**
   * Information used to configure the Dataproc Metastore service to encrypt customer data at rest. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProvider#encryptionConfig
   */
  readonly encryptionConfig?: any[];

  /**
   * Configuration information specific to running Hive metastore software as the metastore service. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProvider#hiveMetastoreConfig
   */
  readonly hiveMetastoreConfig?: MetastoreServiceSpecInitProviderHiveMetastoreConfig[];

  /**
   * User-defined labels for the metastore service.
   *
   * @schema MetastoreServiceSpecInitProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The one hour maintenance window of the metastore service. This specifies when the service can be restarted for maintenance purposes in UTC time. Maintenance window is not needed for services with the SPANNER database type. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProvider#maintenanceWindow
   */
  readonly maintenanceWindow?: MetastoreServiceSpecInitProviderMaintenanceWindow[];

  /**
   * The relative resource name of the VPC network on which the instance can be accessed. It is specified in the following form: "projects/{projectNumber}/global/networks/{network_id}".
   *
   * @schema MetastoreServiceSpecInitProvider#network
   */
  readonly network?: string;

  /**
   * The configuration specifying the network settings for the Dataproc Metastore service. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProvider#networkConfig
   */
  readonly networkConfig?: MetastoreServiceSpecInitProviderNetworkConfig[];

  /**
   * The TCP port at which the metastore service is reached. Default: 9083.
   *
   * @schema MetastoreServiceSpecInitProvider#port
   */
  readonly port?: number;

  /**
   * The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
   *
   * @schema MetastoreServiceSpecInitProvider#project
   */
  readonly project?: string;

  /**
   * The release channel of the service. If unspecified, defaults to STABLE. Default value is STABLE. Possible values are: CANARY, STABLE.
   *
   * @schema MetastoreServiceSpecInitProvider#releaseChannel
   */
  readonly releaseChannel?: string;

  /**
   * The configuration specifying telemetry settings for the Dataproc Metastore service. If unspecified defaults to JSON. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProvider#telemetryConfig
   */
  readonly telemetryConfig?: MetastoreServiceSpecInitProviderTelemetryConfig[];

  /**
   * The tier of the service. Possible values are: DEVELOPER, ENTERPRISE.
   *
   * @schema MetastoreServiceSpecInitProvider#tier
   */
  readonly tier?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProvider(obj: MetastoreServiceSpecInitProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'databaseType': obj.databaseType,
    'encryptionConfig': obj.encryptionConfig?.map(y => y),
    'hiveMetastoreConfig': obj.hiveMetastoreConfig?.map(y => toJson_MetastoreServiceSpecInitProviderHiveMetastoreConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'maintenanceWindow': obj.maintenanceWindow?.map(y => toJson_MetastoreServiceSpecInitProviderMaintenanceWindow(y)),
    'network': obj.network,
    'networkConfig': obj.networkConfig?.map(y => toJson_MetastoreServiceSpecInitProviderNetworkConfig(y)),
    'port': obj.port,
    'project': obj.project,
    'releaseChannel': obj.releaseChannel,
    'telemetryConfig': obj.telemetryConfig?.map(y => toJson_MetastoreServiceSpecInitProviderTelemetryConfig(y)),
    'tier': obj.tier,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * A ManagementAction represents an action that the Crossplane controllers can take on an external resource.
 *
 * @schema MetastoreServiceSpecManagementPolicies
 */
export enum MetastoreServiceSpecManagementPolicies {
  /** Observe */
  OBSERVE = "Observe",
  /** Create */
  CREATE = "Create",
  /** Update */
  UPDATE = "Update",
  /** Delete */
  DELETE = "Delete",
  /** LateInitialize */
  LATE_INITIALIZE = "LateInitialize",
  /** * */
  VALUE_ = "*",
}

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema MetastoreServiceSpecProviderConfigRef
 */
export interface MetastoreServiceSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecProviderConfigRef#policy
   */
  readonly policy?: MetastoreServiceSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecProviderConfigRef(obj: MetastoreServiceSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsTo
 */
export interface MetastoreServiceSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: MetastoreServiceSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: MetastoreServiceSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsTo(obj: MetastoreServiceSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_MetastoreServiceSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema MetastoreServiceSpecWriteConnectionSecretToRef
 */
export interface MetastoreServiceSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema MetastoreServiceSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema MetastoreServiceSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecWriteConnectionSecretToRef(obj: MetastoreServiceSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderEncryptionConfig
 */
export interface MetastoreServiceSpecForProviderEncryptionConfig {
  /**
   * The fully qualified customer provided Cloud KMS key name to use for customer data encryption. Use the following format: projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfig#kmsKey
   */
  readonly kmsKey?: string;

  /**
   * Reference to a CryptoKey in kms to populate kmsKey.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfig#kmsKeyRef
   */
  readonly kmsKeyRef?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef;

  /**
   * Selector for a CryptoKey in kms to populate kmsKey.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfig#kmsKeySelector
   */
  readonly kmsKeySelector?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfig(obj: MetastoreServiceSpecForProviderEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kmsKey': obj.kmsKey,
    'kmsKeyRef': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef(obj.kmsKeyRef),
    'kmsKeySelector': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector(obj.kmsKeySelector),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig
 */
export interface MetastoreServiceSpecForProviderHiveMetastoreConfig {
  /**
   * A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml). The mappings override system defaults (some keys cannot be overridden)
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig#configOverrides
   */
  readonly configOverrides?: { [key: string]: string };

  /**
   * Information used to configure the Hive metastore service as a service principal in a Kerberos realm. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig#kerberosConfig
   */
  readonly kerberosConfig?: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig[];

  /**
   * The Hive metastore schema version.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig#version
   */
  readonly version?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderHiveMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderHiveMetastoreConfig(obj: MetastoreServiceSpecForProviderHiveMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configOverrides': ((obj.configOverrides) === undefined) ? undefined : (Object.entries(obj.configOverrides).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig(y)),
    'version': obj.version,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderMaintenanceWindow
 */
export interface MetastoreServiceSpecForProviderMaintenanceWindow {
  /**
   * The day of week, when the window starts. Possible values are: MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY.
   *
   * @schema MetastoreServiceSpecForProviderMaintenanceWindow#dayOfWeek
   */
  readonly dayOfWeek?: string;

  /**
   * The hour of day (0-23) when the window starts.
   *
   * @schema MetastoreServiceSpecForProviderMaintenanceWindow#hourOfDay
   */
  readonly hourOfDay?: number;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderMaintenanceWindow' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderMaintenanceWindow(obj: MetastoreServiceSpecForProviderMaintenanceWindow | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dayOfWeek': obj.dayOfWeek,
    'hourOfDay': obj.hourOfDay,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderNetworkConfig
 */
export interface MetastoreServiceSpecForProviderNetworkConfig {
  /**
   * The consumer-side network configuration for the Dataproc Metastore instance. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfig#consumers
   */
  readonly consumers?: MetastoreServiceSpecForProviderNetworkConfigConsumers[];

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfig(obj: MetastoreServiceSpecForProviderNetworkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumers': obj.consumers?.map(y => toJson_MetastoreServiceSpecForProviderNetworkConfigConsumers(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderTelemetryConfig
 */
export interface MetastoreServiceSpecForProviderTelemetryConfig {
  /**
   * The output format of the Dataproc Metastore service's logs. Default value is JSON. Possible values are: LEGACY, JSON.
   *
   * @schema MetastoreServiceSpecForProviderTelemetryConfig#logFormat
   */
  readonly logFormat?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderTelemetryConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderTelemetryConfig(obj: MetastoreServiceSpecForProviderTelemetryConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'logFormat': obj.logFormat,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfig
 */
export interface MetastoreServiceSpecInitProviderHiveMetastoreConfig {
  /**
   * A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml). The mappings override system defaults (some keys cannot be overridden)
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfig#configOverrides
   */
  readonly configOverrides?: { [key: string]: string };

  /**
   * Information used to configure the Hive metastore service as a service principal in a Kerberos realm. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfig#kerberosConfig
   */
  readonly kerberosConfig?: MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig[];

  /**
   * The Hive metastore schema version.
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfig#version
   */
  readonly version?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProviderHiveMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProviderHiveMetastoreConfig(obj: MetastoreServiceSpecInitProviderHiveMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configOverrides': ((obj.configOverrides) === undefined) ? undefined : (Object.entries(obj.configOverrides).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig(y)),
    'version': obj.version,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecInitProviderMaintenanceWindow
 */
export interface MetastoreServiceSpecInitProviderMaintenanceWindow {
  /**
   * The day of week, when the window starts. Possible values are: MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY.
   *
   * @schema MetastoreServiceSpecInitProviderMaintenanceWindow#dayOfWeek
   */
  readonly dayOfWeek?: string;

  /**
   * The hour of day (0-23) when the window starts.
   *
   * @schema MetastoreServiceSpecInitProviderMaintenanceWindow#hourOfDay
   */
  readonly hourOfDay?: number;

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProviderMaintenanceWindow' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProviderMaintenanceWindow(obj: MetastoreServiceSpecInitProviderMaintenanceWindow | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dayOfWeek': obj.dayOfWeek,
    'hourOfDay': obj.hourOfDay,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecInitProviderNetworkConfig
 */
export interface MetastoreServiceSpecInitProviderNetworkConfig {
  /**
   * The consumer-side network configuration for the Dataproc Metastore instance. Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProviderNetworkConfig#consumers
   */
  readonly consumers?: any[];

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProviderNetworkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProviderNetworkConfig(obj: MetastoreServiceSpecInitProviderNetworkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumers': obj.consumers?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecInitProviderTelemetryConfig
 */
export interface MetastoreServiceSpecInitProviderTelemetryConfig {
  /**
   * The output format of the Dataproc Metastore service's logs. Default value is JSON. Possible values are: LEGACY, JSON.
   *
   * @schema MetastoreServiceSpecInitProviderTelemetryConfig#logFormat
   */
  readonly logFormat?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProviderTelemetryConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProviderTelemetryConfig(obj: MetastoreServiceSpecInitProviderTelemetryConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'logFormat': obj.logFormat,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecProviderConfigRefPolicy
 */
export interface MetastoreServiceSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecProviderConfigRefPolicy(obj: MetastoreServiceSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRef
 */
export interface MetastoreServiceSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRef(obj: MetastoreServiceSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata
 */
export interface MetastoreServiceSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsToMetadata(obj: MetastoreServiceSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a CryptoKey in kms to populate kmsKey.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a CryptoKey in kms to populate kmsKey.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig
 */
export interface MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig {
  /**
   * A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC). Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig#keytab
   */
  readonly keytab?: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab[];

  /**
   * A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig#krb5ConfigGcsUri
   */
  readonly krb5ConfigGcsUri?: string;

  /**
   * A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig#principal
   */
  readonly principal?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig(obj: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'keytab': obj.keytab?.map(y => toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab(y)),
    'krb5ConfigGcsUri': obj.krb5ConfigGcsUri,
    'principal': obj.principal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumers {
  /**
   * The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint. It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network. There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form: `projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * Reference to a Subnetwork in compute to populate subnetwork.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers#subnetworkRef
   */
  readonly subnetworkRef?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef;

  /**
   * Selector for a Subnetwork in compute to populate subnetwork.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers#subnetworkSelector
   */
  readonly subnetworkSelector?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumers' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumers(obj: MetastoreServiceSpecForProviderNetworkConfigConsumers | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'subnetwork': obj.subnetwork,
    'subnetworkRef': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef(obj.subnetworkRef),
    'subnetworkSelector': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector(obj.subnetworkSelector),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig
 */
export interface MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig {
  /**
   * A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC). Structure is documented below.
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig#keytab
   */
  readonly keytab?: MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab[];

  /**
   * A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig#krb5ConfigGcsUri
   */
  readonly krb5ConfigGcsUri?: string;

  /**
   * A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig#principal
   */
  readonly principal?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig(obj: MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'keytab': obj.keytab?.map(y => toJson_MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab(y)),
    'krb5ConfigGcsUri': obj.krb5ConfigGcsUri,
    'principal': obj.principal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecProviderConfigRefPolicyResolution
 */
export enum MetastoreServiceSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecProviderConfigRefPolicyResolve
 */
export enum MetastoreServiceSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy(obj: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab
 */
export interface MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab {
  /**
   * The relative resource name of a Secret Manager secret version, in the following form: "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab#cloudSecret
   */
  readonly cloudSecret?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab(obj: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'cloudSecret': obj.cloudSecret,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a Subnetwork in compute to populate subnetwork.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a Subnetwork in compute to populate subnetwork.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab
 */
export interface MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab {
  /**
   * The relative resource name of a Secret Manager secret version, in the following form: "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
   *
   * @schema MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab#cloudSecret
   */
  readonly cloudSecret?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab(obj: MetastoreServiceSpecInitProviderHiveMetastoreConfigKerberosConfigKeytab | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'cloudSecret': obj.cloudSecret,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolution
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolve
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolution
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolve
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolution
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolve
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolution
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolve
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}


/**
 * WorkflowTemplate is the Schema for the WorkflowTemplates API. A Workflow Template is a reusable workflow configuration.
 *
 * @schema WorkflowTemplate
 */
export class WorkflowTemplate extends ApiObject {
  /**
   * Returns the apiVersion and kind for "WorkflowTemplate"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'WorkflowTemplate',
  }

  /**
   * Renders a Kubernetes manifest for "WorkflowTemplate".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: WorkflowTemplateProps): any {
    return {
      ...WorkflowTemplate.GVK,
      ...toJson_WorkflowTemplateProps(props),
    };
  }

  /**
   * Defines a "WorkflowTemplate" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: WorkflowTemplateProps) {
    super(scope, id, {
      ...WorkflowTemplate.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...WorkflowTemplate.GVK,
      ...toJson_WorkflowTemplateProps(resolved),
    };
  }
}

/**
 * WorkflowTemplate is the Schema for the WorkflowTemplates API. A Workflow Template is a reusable workflow configuration.
 *
 * @schema WorkflowTemplate
 */
export interface WorkflowTemplateProps {
  /**
   * @schema WorkflowTemplate#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * WorkflowTemplateSpec defines the desired state of WorkflowTemplate
   *
   * @schema WorkflowTemplate#spec
   */
  readonly spec: WorkflowTemplateSpec;

}

/**
 * Converts an object of type 'WorkflowTemplateProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateProps(obj: WorkflowTemplateProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_WorkflowTemplateSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WorkflowTemplateSpec defines the desired state of WorkflowTemplate
 *
 * @schema WorkflowTemplateSpec
 */
export interface WorkflowTemplateSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
   *
   * @schema WorkflowTemplateSpec#deletionPolicy
   */
  readonly deletionPolicy?: WorkflowTemplateSpecDeletionPolicy;

  /**
   * @schema WorkflowTemplateSpec#forProvider
   */
  readonly forProvider: WorkflowTemplateSpecForProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
   *
   * @schema WorkflowTemplateSpec#initProvider
   */
  readonly initProvider?: WorkflowTemplateSpecInitProvider;

  /**
   * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. ManagementPolicies specify the array of actions Crossplane is allowed to take on the managed and external resources. This field is planned to replace the DeletionPolicy field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. If both are custom, the DeletionPolicy field will be ignored. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223 and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
   *
   * @schema WorkflowTemplateSpec#managementPolicies
   */
  readonly managementPolicies?: WorkflowTemplateSpecManagementPolicies[];

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema WorkflowTemplateSpec#providerConfigRef
   */
  readonly providerConfigRef?: WorkflowTemplateSpecProviderConfigRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema WorkflowTemplateSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: WorkflowTemplateSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema WorkflowTemplateSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: WorkflowTemplateSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'WorkflowTemplateSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpec(obj: WorkflowTemplateSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_WorkflowTemplateSpecForProvider(obj.forProvider),
    'initProvider': toJson_WorkflowTemplateSpecInitProvider(obj.initProvider),
    'managementPolicies': obj.managementPolicies?.map(y => y),
    'providerConfigRef': toJson_WorkflowTemplateSpecProviderConfigRef(obj.providerConfigRef),
    'publishConnectionDetailsTo': toJson_WorkflowTemplateSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_WorkflowTemplateSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
 *
 * @schema WorkflowTemplateSpecDeletionPolicy
 */
export enum WorkflowTemplateSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema WorkflowTemplateSpecForProvider
 */
export interface WorkflowTemplateSpecForProvider {
  /**
   * (Beta only) Optional. Timeout duration for the DAG of jobs. You can use "s", "m", "h", and "d" suffixes for second, minute, hour, and day duration values, respectively. The timeout duration must be from 10 minutes ("10m") to 24 hours ("24h" or "1d"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a (/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.
   *
   * @schema WorkflowTemplateSpecForProvider#dagTimeout
   */
  readonly dagTimeout?: string;

  /**
   * Required. The Directed Acyclic Graph of Jobs to submit.
   *
   * @schema WorkflowTemplateSpecForProvider#jobs
   */
  readonly jobs?: WorkflowTemplateSpecForProviderJobs[];

  /**
   * The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
   *
   * @schema WorkflowTemplateSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The location for the resource
   *
   * @schema WorkflowTemplateSpecForProvider#location
   */
  readonly location: string;

  /**
   * Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.
   *
   * @schema WorkflowTemplateSpecForProvider#parameters
   */
  readonly parameters?: WorkflowTemplateSpecForProviderParameters[];

  /**
   * Required. WorkflowTemplate scheduling information.
   *
   * @schema WorkflowTemplateSpecForProvider#placement
   */
  readonly placement?: WorkflowTemplateSpecForProviderPlacement[];

  /**
   * The project for the resource
   *
   * @schema WorkflowTemplateSpecForProvider#project
   */
  readonly project?: string;

  /**
   * Used to perform a consistent read-modify-write. This field should be left blank for a CreateWorkflowTemplate request. It is required for an UpdateWorkflowTemplate request, and must match the current server version. A typical update template flow would fetch the current template with a GetWorkflowTemplate request, which will return the current template with the version field filled in with the current server version. The user updates other fields in the template, then returns it as part of the UpdateWorkflowTemplate request.
   *
   * @schema WorkflowTemplateSpecForProvider#version
   */
  readonly version?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProvider(obj: WorkflowTemplateSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dagTimeout': obj.dagTimeout,
    'jobs': obj.jobs?.map(y => toJson_WorkflowTemplateSpecForProviderJobs(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'location': obj.location,
    'parameters': obj.parameters?.map(y => toJson_WorkflowTemplateSpecForProviderParameters(y)),
    'placement': obj.placement?.map(y => toJson_WorkflowTemplateSpecForProviderPlacement(y)),
    'project': obj.project,
    'version': obj.version,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.
 *
 * @schema WorkflowTemplateSpecInitProvider
 */
export interface WorkflowTemplateSpecInitProvider {
  /**
   * (Beta only) Optional. Timeout duration for the DAG of jobs. You can use "s", "m", "h", and "d" suffixes for second, minute, hour, and day duration values, respectively. The timeout duration must be from 10 minutes ("10m") to 24 hours ("24h" or "1d"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a (/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.
   *
   * @schema WorkflowTemplateSpecInitProvider#dagTimeout
   */
  readonly dagTimeout?: string;

  /**
   * Required. The Directed Acyclic Graph of Jobs to submit.
   *
   * @schema WorkflowTemplateSpecInitProvider#jobs
   */
  readonly jobs?: WorkflowTemplateSpecInitProviderJobs[];

  /**
   * The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
   *
   * @schema WorkflowTemplateSpecInitProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.
   *
   * @schema WorkflowTemplateSpecInitProvider#parameters
   */
  readonly parameters?: WorkflowTemplateSpecInitProviderParameters[];

  /**
   * Required. WorkflowTemplate scheduling information.
   *
   * @schema WorkflowTemplateSpecInitProvider#placement
   */
  readonly placement?: WorkflowTemplateSpecInitProviderPlacement[];

  /**
   * The project for the resource
   *
   * @schema WorkflowTemplateSpecInitProvider#project
   */
  readonly project?: string;

  /**
   * Used to perform a consistent read-modify-write. This field should be left blank for a CreateWorkflowTemplate request. It is required for an UpdateWorkflowTemplate request, and must match the current server version. A typical update template flow would fetch the current template with a GetWorkflowTemplate request, which will return the current template with the version field filled in with the current server version. The user updates other fields in the template, then returns it as part of the UpdateWorkflowTemplate request.
   *
   * @schema WorkflowTemplateSpecInitProvider#version
   */
  readonly version?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProvider(obj: WorkflowTemplateSpecInitProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dagTimeout': obj.dagTimeout,
    'jobs': obj.jobs?.map(y => toJson_WorkflowTemplateSpecInitProviderJobs(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'parameters': obj.parameters?.map(y => toJson_WorkflowTemplateSpecInitProviderParameters(y)),
    'placement': obj.placement?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacement(y)),
    'project': obj.project,
    'version': obj.version,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * A ManagementAction represents an action that the Crossplane controllers can take on an external resource.
 *
 * @schema WorkflowTemplateSpecManagementPolicies
 */
export enum WorkflowTemplateSpecManagementPolicies {
  /** Observe */
  OBSERVE = "Observe",
  /** Create */
  CREATE = "Create",
  /** Update */
  UPDATE = "Update",
  /** Delete */
  DELETE = "Delete",
  /** LateInitialize */
  LATE_INITIALIZE = "LateInitialize",
  /** * */
  VALUE_ = "*",
}

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema WorkflowTemplateSpecProviderConfigRef
 */
export interface WorkflowTemplateSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema WorkflowTemplateSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema WorkflowTemplateSpecProviderConfigRef#policy
   */
  readonly policy?: WorkflowTemplateSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecProviderConfigRef(obj: WorkflowTemplateSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_WorkflowTemplateSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsTo
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: WorkflowTemplateSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsTo(obj: WorkflowTemplateSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_WorkflowTemplateSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema WorkflowTemplateSpecWriteConnectionSecretToRef
 */
export interface WorkflowTemplateSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema WorkflowTemplateSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema WorkflowTemplateSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecWriteConnectionSecretToRef(obj: WorkflowTemplateSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobs
 */
export interface WorkflowTemplateSpecForProviderJobs {
  /**
   * Job is a Hadoop job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#hadoopJob
   */
  readonly hadoopJob?: WorkflowTemplateSpecForProviderJobsHadoopJob[];

  /**
   * Job is a Hive job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#hiveJob
   */
  readonly hiveJob?: WorkflowTemplateSpecForProviderJobsHiveJob[];

  /**
   * The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Job is a Pig job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#pigJob
   */
  readonly pigJob?: WorkflowTemplateSpecForProviderJobsPigJob[];

  /**
   * The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#prerequisiteStepIds
   */
  readonly prerequisiteStepIds?: string[];

  /**
   * Job is a Presto job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#prestoJob
   */
  readonly prestoJob?: WorkflowTemplateSpecForProviderJobsPrestoJob[];

  /**
   * Job is a PySpark job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#pysparkJob
   */
  readonly pysparkJob?: WorkflowTemplateSpecForProviderJobsPysparkJob[];

  /**
   * Job scheduling configuration.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#scheduling
   */
  readonly scheduling?: WorkflowTemplateSpecForProviderJobsScheduling[];

  /**
   * Job is a Spark job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#sparkJob
   */
  readonly sparkJob?: WorkflowTemplateSpecForProviderJobsSparkJob[];

  /**
   * Job is a SparkR job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#sparkRJob
   */
  readonly sparkRJob?: WorkflowTemplateSpecForProviderJobsSparkRJob[];

  /**
   * Job is a SparkSql job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#sparkSqlJob
   */
  readonly sparkSqlJob?: WorkflowTemplateSpecForProviderJobsSparkSqlJob[];

  /**
   * Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#stepId
   */
  readonly stepId?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobs' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobs(obj: WorkflowTemplateSpecForProviderJobs | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'hadoopJob': obj.hadoopJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHadoopJob(y)),
    'hiveJob': obj.hiveJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHiveJob(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pigJob': obj.pigJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPigJob(y)),
    'prerequisiteStepIds': obj.prerequisiteStepIds?.map(y => y),
    'prestoJob': obj.prestoJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPrestoJob(y)),
    'pysparkJob': obj.pysparkJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPysparkJob(y)),
    'scheduling': obj.scheduling?.map(y => toJson_WorkflowTemplateSpecForProviderJobsScheduling(y)),
    'sparkJob': obj.sparkJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkJob(y)),
    'sparkRJob': obj.sparkRJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkRJob(y)),
    'sparkSqlJob': obj.sparkSqlJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJob(y)),
    'stepId': obj.stepId,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParameters
 */
export interface WorkflowTemplateSpecForProviderParameters {
  /**
   * Brief description of the parameter. Must not exceed 1024 characters.
   *
   * @schema WorkflowTemplateSpecForProviderParameters#description
   */
  readonly description?: string;

  /**
   * Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
   *
   * @schema WorkflowTemplateSpecForProviderParameters#fields
   */
  readonly fields?: string[];

  /**
   * Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
   *
   * @schema WorkflowTemplateSpecForProviderParameters#name
   */
  readonly name?: string;

  /**
   * Validation rules to be applied to this parameter's value.
   *
   * @schema WorkflowTemplateSpecForProviderParameters#validation
   */
  readonly validation?: WorkflowTemplateSpecForProviderParametersValidation[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParameters' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParameters(obj: WorkflowTemplateSpecForProviderParameters | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'description': obj.description,
    'fields': obj.fields?.map(y => y),
    'name': obj.name,
    'validation': obj.validation?.map(y => toJson_WorkflowTemplateSpecForProviderParametersValidation(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacement
 */
export interface WorkflowTemplateSpecForProviderPlacement {
  /**
   * A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
   *
   * @schema WorkflowTemplateSpecForProviderPlacement#clusterSelector
   */
  readonly clusterSelector?: WorkflowTemplateSpecForProviderPlacementClusterSelector[];

  /**
   * A cluster that is managed by the workflow.
   *
   * @schema WorkflowTemplateSpecForProviderPlacement#managedCluster
   */
  readonly managedCluster?: WorkflowTemplateSpecForProviderPlacementManagedCluster[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacement' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacement(obj: WorkflowTemplateSpecForProviderPlacement | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterSelector': obj.clusterSelector?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementClusterSelector(y)),
    'managedCluster': obj.managedCluster?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedCluster(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobs
 */
export interface WorkflowTemplateSpecInitProviderJobs {
  /**
   * Job is a Hadoop job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#hadoopJob
   */
  readonly hadoopJob?: WorkflowTemplateSpecInitProviderJobsHadoopJob[];

  /**
   * Job is a Hive job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#hiveJob
   */
  readonly hiveJob?: WorkflowTemplateSpecInitProviderJobsHiveJob[];

  /**
   * The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Job is a Pig job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#pigJob
   */
  readonly pigJob?: WorkflowTemplateSpecInitProviderJobsPigJob[];

  /**
   * The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#prerequisiteStepIds
   */
  readonly prerequisiteStepIds?: string[];

  /**
   * Job is a Presto job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#prestoJob
   */
  readonly prestoJob?: WorkflowTemplateSpecInitProviderJobsPrestoJob[];

  /**
   * Job is a PySpark job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#pysparkJob
   */
  readonly pysparkJob?: WorkflowTemplateSpecInitProviderJobsPysparkJob[];

  /**
   * Job scheduling configuration.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#scheduling
   */
  readonly scheduling?: WorkflowTemplateSpecInitProviderJobsScheduling[];

  /**
   * Job is a Spark job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#sparkJob
   */
  readonly sparkJob?: WorkflowTemplateSpecInitProviderJobsSparkJob[];

  /**
   * Job is a SparkR job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#sparkRJob
   */
  readonly sparkRJob?: WorkflowTemplateSpecInitProviderJobsSparkRJob[];

  /**
   * Job is a SparkSql job.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#sparkSqlJob
   */
  readonly sparkSqlJob?: WorkflowTemplateSpecInitProviderJobsSparkSqlJob[];

  /**
   * Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
   *
   * @schema WorkflowTemplateSpecInitProviderJobs#stepId
   */
  readonly stepId?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobs' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobs(obj: WorkflowTemplateSpecInitProviderJobs | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'hadoopJob': obj.hadoopJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsHadoopJob(y)),
    'hiveJob': obj.hiveJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsHiveJob(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pigJob': obj.pigJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPigJob(y)),
    'prerequisiteStepIds': obj.prerequisiteStepIds?.map(y => y),
    'prestoJob': obj.prestoJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPrestoJob(y)),
    'pysparkJob': obj.pysparkJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPysparkJob(y)),
    'scheduling': obj.scheduling?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsScheduling(y)),
    'sparkJob': obj.sparkJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkJob(y)),
    'sparkRJob': obj.sparkRJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkRJob(y)),
    'sparkSqlJob': obj.sparkSqlJob?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkSqlJob(y)),
    'stepId': obj.stepId,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderParameters
 */
export interface WorkflowTemplateSpecInitProviderParameters {
  /**
   * Brief description of the parameter. Must not exceed 1024 characters.
   *
   * @schema WorkflowTemplateSpecInitProviderParameters#description
   */
  readonly description?: string;

  /**
   * Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
   *
   * @schema WorkflowTemplateSpecInitProviderParameters#fields
   */
  readonly fields?: string[];

  /**
   * Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
   *
   * @schema WorkflowTemplateSpecInitProviderParameters#name
   */
  readonly name?: string;

  /**
   * Validation rules to be applied to this parameter's value.
   *
   * @schema WorkflowTemplateSpecInitProviderParameters#validation
   */
  readonly validation?: WorkflowTemplateSpecInitProviderParametersValidation[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderParameters' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderParameters(obj: WorkflowTemplateSpecInitProviderParameters | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'description': obj.description,
    'fields': obj.fields?.map(y => y),
    'name': obj.name,
    'validation': obj.validation?.map(y => toJson_WorkflowTemplateSpecInitProviderParametersValidation(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacement
 */
export interface WorkflowTemplateSpecInitProviderPlacement {
  /**
   * A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacement#clusterSelector
   */
  readonly clusterSelector?: WorkflowTemplateSpecInitProviderPlacementClusterSelector[];

  /**
   * A cluster that is managed by the workflow.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacement#managedCluster
   */
  readonly managedCluster?: WorkflowTemplateSpecInitProviderPlacementManagedCluster[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacement' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacement(obj: WorkflowTemplateSpecInitProviderPlacement | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterSelector': obj.clusterSelector?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementClusterSelector(y)),
    'managedCluster': obj.managedCluster?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedCluster(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema WorkflowTemplateSpecProviderConfigRefPolicy
 */
export interface WorkflowTemplateSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema WorkflowTemplateSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: WorkflowTemplateSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema WorkflowTemplateSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: WorkflowTemplateSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecProviderConfigRefPolicy(obj: WorkflowTemplateSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRef
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRef(obj: WorkflowTemplateSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsToMetadata(obj: WorkflowTemplateSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHadoopJob
 */
export interface WorkflowTemplateSpecForProviderJobsHadoopJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file that contains the main class.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHadoopJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHadoopJob(obj: WorkflowTemplateSpecForProviderJobsHadoopJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHiveJob
 */
export interface WorkflowTemplateSpecForProviderJobsHiveJob {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsHiveJobQueryList[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHiveJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHiveJob(obj: WorkflowTemplateSpecForProviderJobsHiveJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHiveJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPigJob
 */
export interface WorkflowTemplateSpecForProviderJobsPigJob {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsPigJobQueryList[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPigJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPigJob(obj: WorkflowTemplateSpecForProviderJobsPigJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPigJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPrestoJob
 */
export interface WorkflowTemplateSpecForProviderJobsPrestoJob {
  /**
   * Presto client tags to attach to this query
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#clientTags
   */
  readonly clientTags?: string[];

  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig[];

  /**
   * The format in which query output will be displayed. See the Presto documentation for supported output formats
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#outputFormat
   */
  readonly outputFormat?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsPrestoJobQueryList[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPrestoJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPrestoJob(obj: WorkflowTemplateSpecForProviderJobsPrestoJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clientTags': obj.clientTags?.map(y => y),
    'continueOnFailure': obj.continueOnFailure,
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig(y)),
    'outputFormat': obj.outputFormat,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPrestoJobQueryList(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPysparkJob
 */
export interface WorkflowTemplateSpecForProviderJobsPysparkJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig[];

  /**
   * Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#mainPythonFileUri
   */
  readonly mainPythonFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#pythonFileUris
   */
  readonly pythonFileUris?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPysparkJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPysparkJob(obj: WorkflowTemplateSpecForProviderJobsPysparkJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig(y)),
    'mainPythonFileUri': obj.mainPythonFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pythonFileUris': obj.pythonFileUris?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsScheduling
 */
export interface WorkflowTemplateSpecForProviderJobsScheduling {
  /**
   * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
   *
   * @schema WorkflowTemplateSpecForProviderJobsScheduling#maxFailuresPerHour
   */
  readonly maxFailuresPerHour?: number;

  /**
   * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
   *
   * @schema WorkflowTemplateSpecForProviderJobsScheduling#maxFailuresTotal
   */
  readonly maxFailuresTotal?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsScheduling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsScheduling(obj: WorkflowTemplateSpecForProviderJobsScheduling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxFailuresPerHour': obj.maxFailuresPerHour,
    'maxFailuresTotal': obj.maxFailuresTotal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkJob
 */
export interface WorkflowTemplateSpecForProviderJobsSparkJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file that contains the main class.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkJob(obj: WorkflowTemplateSpecForProviderJobsSparkJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkRJob
 */
export interface WorkflowTemplateSpecForProviderJobsSparkRJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig[];

  /**
   * Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#mainRFileUri
   */
  readonly mainRFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkRJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkRJob(obj: WorkflowTemplateSpecForProviderJobsSparkRJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig(y)),
    'mainRFileUri': obj.mainRFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob
 */
export interface WorkflowTemplateSpecForProviderJobsSparkSqlJob {
  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkSqlJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJob(obj: WorkflowTemplateSpecForProviderJobsSparkSqlJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParametersValidation
 */
export interface WorkflowTemplateSpecForProviderParametersValidation {
  /**
   * Validation based on regular expressions.
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidation#regex
   */
  readonly regex?: WorkflowTemplateSpecForProviderParametersValidationRegex[];

  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidation#values
   */
  readonly values?: WorkflowTemplateSpecForProviderParametersValidationValues[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParametersValidation' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParametersValidation(obj: WorkflowTemplateSpecForProviderParametersValidation | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'regex': obj.regex?.map(y => toJson_WorkflowTemplateSpecForProviderParametersValidationRegex(y)),
    'values': obj.values?.map(y => toJson_WorkflowTemplateSpecForProviderParametersValidationValues(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementClusterSelector
 */
export interface WorkflowTemplateSpecForProviderPlacementClusterSelector {
  /**
   * Required. The cluster labels. Cluster must have all labels to match.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementClusterSelector#clusterLabels
   */
  readonly clusterLabels?: { [key: string]: string };

  /**
   * The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
   *
   * @schema WorkflowTemplateSpecForProviderPlacementClusterSelector#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementClusterSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementClusterSelector(obj: WorkflowTemplateSpecForProviderPlacementClusterSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterLabels': ((obj.clusterLabels) === undefined) ? undefined : (Object.entries(obj.clusterLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedCluster {
  /**
   * Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster#clusterName
   */
  readonly clusterName?: string;

  /**
   * Required. The cluster configuration.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster#config
   */
  readonly config?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfig[];

  /**
   * The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster#labels
   */
  readonly labels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedCluster' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedCluster(obj: WorkflowTemplateSpecForProviderPlacementManagedCluster | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterName': obj.clusterName,
    'config': obj.config?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob
 */
export interface WorkflowTemplateSpecInitProviderJobsHadoopJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file that contains the main class.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsHadoopJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsHadoopJob(obj: WorkflowTemplateSpecInitProviderJobsHadoopJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsHiveJob
 */
export interface WorkflowTemplateSpecInitProviderJobsHiveJob {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecInitProviderJobsHiveJobQueryList[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsHiveJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsHiveJob(obj: WorkflowTemplateSpecInitProviderJobsHiveJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsHiveJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPigJob
 */
export interface WorkflowTemplateSpecInitProviderJobsPigJob {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecInitProviderJobsPigJobQueryList[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPigJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPigJob(obj: WorkflowTemplateSpecInitProviderJobsPigJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPigJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob
 */
export interface WorkflowTemplateSpecInitProviderJobsPrestoJob {
  /**
   * Presto client tags to attach to this query
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#clientTags
   */
  readonly clientTags?: string[];

  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig[];

  /**
   * The format in which query output will be displayed. See the Presto documentation for supported output formats
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#outputFormat
   */
  readonly outputFormat?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPrestoJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPrestoJob(obj: WorkflowTemplateSpecInitProviderJobsPrestoJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clientTags': obj.clientTags?.map(y => y),
    'continueOnFailure': obj.continueOnFailure,
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig(y)),
    'outputFormat': obj.outputFormat,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob
 */
export interface WorkflowTemplateSpecInitProviderJobsPysparkJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig[];

  /**
   * Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#mainPythonFileUri
   */
  readonly mainPythonFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJob#pythonFileUris
   */
  readonly pythonFileUris?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPysparkJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPysparkJob(obj: WorkflowTemplateSpecInitProviderJobsPysparkJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig(y)),
    'mainPythonFileUri': obj.mainPythonFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pythonFileUris': obj.pythonFileUris?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsScheduling
 */
export interface WorkflowTemplateSpecInitProviderJobsScheduling {
  /**
   * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsScheduling#maxFailuresPerHour
   */
  readonly maxFailuresPerHour?: number;

  /**
   * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
   *
   * @schema WorkflowTemplateSpecInitProviderJobsScheduling#maxFailuresTotal
   */
  readonly maxFailuresTotal?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsScheduling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsScheduling(obj: WorkflowTemplateSpecInitProviderJobsScheduling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxFailuresPerHour': obj.maxFailuresPerHour,
    'maxFailuresTotal': obj.maxFailuresTotal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkJob
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file that contains the main class.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkJob(obj: WorkflowTemplateSpecInitProviderJobsSparkJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkRJob {
  /**
   * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig[];

  /**
   * Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob#mainRFileUri
   */
  readonly mainRFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkRJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkRJob(obj: WorkflowTemplateSpecInitProviderJobsSparkRJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig(y)),
    'mainRFileUri': obj.mainRFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkSqlJob {
  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkSqlJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkSqlJob(obj: WorkflowTemplateSpecInitProviderJobsSparkSqlJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderParametersValidation
 */
export interface WorkflowTemplateSpecInitProviderParametersValidation {
  /**
   * Validation based on regular expressions.
   *
   * @schema WorkflowTemplateSpecInitProviderParametersValidation#regex
   */
  readonly regex?: WorkflowTemplateSpecInitProviderParametersValidationRegex[];

  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecInitProviderParametersValidation#values
   */
  readonly values?: WorkflowTemplateSpecInitProviderParametersValidationValues[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderParametersValidation' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderParametersValidation(obj: WorkflowTemplateSpecInitProviderParametersValidation | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'regex': obj.regex?.map(y => toJson_WorkflowTemplateSpecInitProviderParametersValidationRegex(y)),
    'values': obj.values?.map(y => toJson_WorkflowTemplateSpecInitProviderParametersValidationValues(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementClusterSelector
 */
export interface WorkflowTemplateSpecInitProviderPlacementClusterSelector {
  /**
   * Required. The cluster labels. Cluster must have all labels to match.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementClusterSelector#clusterLabels
   */
  readonly clusterLabels?: { [key: string]: string };

  /**
   * The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementClusterSelector#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementClusterSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementClusterSelector(obj: WorkflowTemplateSpecInitProviderPlacementClusterSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterLabels': ((obj.clusterLabels) === undefined) ? undefined : (Object.entries(obj.clusterLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedCluster
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedCluster {
  /**
   * Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedCluster#clusterName
   */
  readonly clusterName?: string;

  /**
   * Required. The cluster configuration.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedCluster#config
   */
  readonly config?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig[];

  /**
   * The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedCluster#labels
   */
  readonly labels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedCluster' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedCluster(obj: WorkflowTemplateSpecInitProviderPlacementManagedCluster | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterName': obj.clusterName,
    'config': obj.config?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema WorkflowTemplateSpecProviderConfigRefPolicyResolution
 */
export enum WorkflowTemplateSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema WorkflowTemplateSpecProviderConfigRefPolicyResolve
 */
export enum WorkflowTemplateSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy(obj: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHiveJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsHiveJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHiveJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHiveJobQueryList(obj: WorkflowTemplateSpecForProviderJobsHiveJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPigJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsPigJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPigJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPigJobQueryList(obj: WorkflowTemplateSpecForProviderJobsPigJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPrestoJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsPrestoJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPrestoJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPrestoJobQueryList(obj: WorkflowTemplateSpecForProviderJobsPrestoJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList(obj: WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParametersValidationRegex
 */
export interface WorkflowTemplateSpecForProviderParametersValidationRegex {
  /**
   * Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidationRegex#regexes
   */
  readonly regexes?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParametersValidationRegex' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParametersValidationRegex(obj: WorkflowTemplateSpecForProviderParametersValidationRegex | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'regexes': obj.regexes?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParametersValidationValues
 */
export interface WorkflowTemplateSpecForProviderParametersValidationValues {
  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidationValues#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParametersValidationValues' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParametersValidationValues(obj: WorkflowTemplateSpecForProviderParametersValidationValues | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfig {
  /**
   * Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#autoscalingConfig
   */
  readonly autoscalingConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig[];

  /**
   * Encryption settings for the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#encryptionConfig
   */
  readonly encryptionConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig[];

  /**
   * Port/endpoint configuration for this cluster
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#endpointConfig
   */
  readonly endpointConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig[];

  /**
   * The shared Compute Engine config settings for all instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#gceClusterConfig
   */
  readonly gceClusterConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig[];

  /**
   * Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#initializationActions
   */
  readonly initializationActions?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions[];

  /**
   * Lifecycle setting for the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#lifecycleConfig
   */
  readonly lifecycleConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig[];

  /**
   * The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#masterConfig
   */
  readonly masterConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig[];

  /**
   * The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#secondaryWorkerConfig
   */
  readonly secondaryWorkerConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig[];

  /**
   * Security settings for the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#securityConfig
   */
  readonly securityConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig[];

  /**
   * The config settings for software inside the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#softwareConfig
   */
  readonly softwareConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig[];

  /**
   * A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

  /**
   * A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#tempBucket
   */
  readonly tempBucket?: string;

  /**
   * The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#workerConfig
   */
  readonly workerConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscalingConfig': obj.autoscalingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig(y)),
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig(y)),
    'endpointConfig': obj.endpointConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig(y)),
    'gceClusterConfig': obj.gceClusterConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig(y)),
    'initializationActions': obj.initializationActions?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions(y)),
    'lifecycleConfig': obj.lifecycleConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig(y)),
    'masterConfig': obj.masterConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig(y)),
    'secondaryWorkerConfig': obj.secondaryWorkerConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig(y)),
    'securityConfig': obj.securityConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig(y)),
    'softwareConfig': obj.softwareConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig(y)),
    'stagingBucket': obj.stagingBucket,
    'tempBucket': obj.tempBucket,
    'workerConfig': obj.workerConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsHadoopJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsHiveJobQueryList
 */
export interface WorkflowTemplateSpecInitProviderJobsHiveJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecInitProviderJobsHiveJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsHiveJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsHiveJobQueryList(obj: WorkflowTemplateSpecInitProviderJobsHiveJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsPigJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPigJobQueryList
 */
export interface WorkflowTemplateSpecInitProviderJobsPigJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPigJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPigJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPigJobQueryList(obj: WorkflowTemplateSpecInitProviderJobsPigJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsPrestoJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList
 */
export interface WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList(obj: WorkflowTemplateSpecInitProviderJobsPrestoJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsPysparkJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsSparkJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsSparkRJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig(obj: WorkflowTemplateSpecInitProviderJobsSparkSqlJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList
 */
export interface WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList#queries
   */
  readonly queries?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList(obj: WorkflowTemplateSpecInitProviderJobsSparkSqlJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderParametersValidationRegex
 */
export interface WorkflowTemplateSpecInitProviderParametersValidationRegex {
  /**
   * Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
   *
   * @schema WorkflowTemplateSpecInitProviderParametersValidationRegex#regexes
   */
  readonly regexes?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderParametersValidationRegex' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderParametersValidationRegex(obj: WorkflowTemplateSpecInitProviderParametersValidationRegex | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'regexes': obj.regexes?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderParametersValidationValues
 */
export interface WorkflowTemplateSpecInitProviderParametersValidationValues {
  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecInitProviderParametersValidationValues#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderParametersValidationValues' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderParametersValidationValues(obj: WorkflowTemplateSpecInitProviderParametersValidationValues | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig {
  /**
   * Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#autoscalingConfig
   */
  readonly autoscalingConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig[];

  /**
   * Encryption settings for the cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#encryptionConfig
   */
  readonly encryptionConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig[];

  /**
   * Port/endpoint configuration for this cluster
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#endpointConfig
   */
  readonly endpointConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig[];

  /**
   * The shared Compute Engine config settings for all instances in a cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#gceClusterConfig
   */
  readonly gceClusterConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig[];

  /**
   * Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#initializationActions
   */
  readonly initializationActions?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions[];

  /**
   * Lifecycle setting for the cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#lifecycleConfig
   */
  readonly lifecycleConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig[];

  /**
   * The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#masterConfig
   */
  readonly masterConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig[];

  /**
   * The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#secondaryWorkerConfig
   */
  readonly secondaryWorkerConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig[];

  /**
   * Security settings for the cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#securityConfig
   */
  readonly securityConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig[];

  /**
   * The config settings for software inside the cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#softwareConfig
   */
  readonly softwareConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig[];

  /**
   * A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

  /**
   * A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#tempBucket
   */
  readonly tempBucket?: string;

  /**
   * The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig#workerConfig
   */
  readonly workerConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscalingConfig': obj.autoscalingConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig(y)),
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig(y)),
    'endpointConfig': obj.endpointConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig(y)),
    'gceClusterConfig': obj.gceClusterConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig(y)),
    'initializationActions': obj.initializationActions?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions(y)),
    'lifecycleConfig': obj.lifecycleConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig(y)),
    'masterConfig': obj.masterConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig(y)),
    'secondaryWorkerConfig': obj.secondaryWorkerConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig(y)),
    'securityConfig': obj.securityConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig(y)),
    'softwareConfig': obj.softwareConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig(y)),
    'stagingBucket': obj.stagingBucket,
    'tempBucket': obj.tempBucket,
    'workerConfig': obj.workerConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig {
  /**
   * The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ Note that the policy must be in the same project and Dataproc region.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig#policy
   */
  readonly policy?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'policy': obj.policy,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig {
  /**
   * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig#gcePdKmsKeyName
   */
  readonly gcePdKmsKeyName?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gcePdKmsKeyName': obj.gcePdKmsKeyName,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig {
  /**
   * If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
   *
   * @default false.
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig#enableHttpPortAccess
   */
  readonly enableHttpPortAccess?: boolean;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableHttpPortAccess': obj.enableHttpPortAccess,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig {
  /**
   * If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#internalIpOnly
   */
  readonly internalIpOnly?: boolean;

  /**
   * The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#metadata
   */
  readonly metadata?: { [key: string]: string };

  /**
   * The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default*default`
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#network
   */
  readonly network?: string;

  /**
   * Node Group Affinity for sole-tenant clusters.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#nodeGroupAffinity
   */
  readonly nodeGroupAffinity?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity[];

  /**
   * The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#privateIpv6GoogleAccess
   */
  readonly privateIpv6GoogleAccess?: string;

  /**
   * Reservation Affinity for consuming Zonal reservation.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#reservationAffinity
   */
  readonly reservationAffinity?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity[];

  /**
   * The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#serviceAccount
   */
  readonly serviceAccount?: string;

  /**
   * The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#serviceAccountScopes
   */
  readonly serviceAccountScopes?: string[];

  /**
   * Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#shieldedInstanceConfig
   */
  readonly shieldedInstanceConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig[];

  /**
   * The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0 * sub0
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#tags
   */
  readonly tags?: string[];

  /**
   * The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'internalIpOnly': obj.internalIpOnly,
    'metadata': ((obj.metadata) === undefined) ? undefined : (Object.entries(obj.metadata).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'network': obj.network,
    'nodeGroupAffinity': obj.nodeGroupAffinity?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity(y)),
    'privateIpv6GoogleAccess': obj.privateIpv6GoogleAccess,
    'reservationAffinity': obj.reservationAffinity?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity(y)),
    'serviceAccount': obj.serviceAccount,
    'serviceAccountScopes': obj.serviceAccountScopes?.map(y => y),
    'shieldedInstanceConfig': obj.shieldedInstanceConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig(y)),
    'subnetwork': obj.subnetwork,
    'tags': obj.tags?.map(y => y),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions {
  /**
   * Required. Cloud Storage URI of executable file.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions#executableFile
   */
  readonly executableFile?: string;

  /**
   * Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
   *
   * @default 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions#executionTimeout
   */
  readonly executionTimeout?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'executableFile': obj.executableFile,
    'executionTimeout': obj.executionTimeout,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig {
  /**
   * The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig#autoDeleteTime
   */
  readonly autoDeleteTime?: string;

  /**
   * The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig#autoDeleteTtl
   */
  readonly autoDeleteTtl?: string;

  /**
   * The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig#idleDeleteTtl
   */
  readonly idleDeleteTtl?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoDeleteTime': obj.autoDeleteTime,
    'autoDeleteTtl': obj.autoDeleteTtl,
    'idleDeleteTtl': obj.idleDeleteTtl,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig {
  /**
   * The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators[];

  /**
   * Disk option config settings.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig[];

  /**
   * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#image
   */
  readonly image?: string;

  /**
   * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators[];

  /**
   * Disk option config settings.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig[];

  /**
   * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#image
   */
  readonly image?: string;

  /**
   * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig {
  /**
   * Kerberos related configuration.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig#kerberosConfig
   */
  readonly kerberosConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig {
  /**
   * The version of software inside the cluster. It must be one of the supported Dataproc Versions, such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version. If unspecified, it defaults to the latest Debian version.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig#imageVersion
   */
  readonly imageVersion?: string;

  /**
   * The set of components to activate on the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig#optionalComponents
   */
  readonly optionalComponents?: string[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'imageVersion': obj.imageVersion,
    'optionalComponents': obj.optionalComponents?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators[];

  /**
   * Disk option config settings.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig[];

  /**
   * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#image
   */
  readonly image?: string;

  /**
   * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig {
  /**
   * The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ Note that the policy must be in the same project and Dataproc region.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig#policy
   */
  readonly policy?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigAutoscalingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'policy': obj.policy,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig {
  /**
   * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig#gcePdKmsKeyName
   */
  readonly gcePdKmsKeyName?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gcePdKmsKeyName': obj.gcePdKmsKeyName,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig {
  /**
   * If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
   *
   * @default false.
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig#enableHttpPortAccess
   */
  readonly enableHttpPortAccess?: boolean;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigEndpointConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableHttpPortAccess': obj.enableHttpPortAccess,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig {
  /**
   * If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#internalIpOnly
   */
  readonly internalIpOnly?: boolean;

  /**
   * The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#metadata
   */
  readonly metadata?: { [key: string]: string };

  /**
   * The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default*default`
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#network
   */
  readonly network?: string;

  /**
   * Node Group Affinity for sole-tenant clusters.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#nodeGroupAffinity
   */
  readonly nodeGroupAffinity?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity[];

  /**
   * The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#privateIpv6GoogleAccess
   */
  readonly privateIpv6GoogleAccess?: string;

  /**
   * Reservation Affinity for consuming Zonal reservation.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#reservationAffinity
   */
  readonly reservationAffinity?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity[];

  /**
   * The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#serviceAccount
   */
  readonly serviceAccount?: string;

  /**
   * The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#serviceAccountScopes
   */
  readonly serviceAccountScopes?: string[];

  /**
   * Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#shieldedInstanceConfig
   */
  readonly shieldedInstanceConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig[];

  /**
   * The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0 * sub0
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#tags
   */
  readonly tags?: string[];

  /**
   * The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'internalIpOnly': obj.internalIpOnly,
    'metadata': ((obj.metadata) === undefined) ? undefined : (Object.entries(obj.metadata).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'network': obj.network,
    'nodeGroupAffinity': obj.nodeGroupAffinity?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity(y)),
    'privateIpv6GoogleAccess': obj.privateIpv6GoogleAccess,
    'reservationAffinity': obj.reservationAffinity?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity(y)),
    'serviceAccount': obj.serviceAccount,
    'serviceAccountScopes': obj.serviceAccountScopes?.map(y => y),
    'shieldedInstanceConfig': obj.shieldedInstanceConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig(y)),
    'subnetwork': obj.subnetwork,
    'tags': obj.tags?.map(y => y),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions {
  /**
   * Required. Cloud Storage URI of executable file.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions#executableFile
   */
  readonly executableFile?: string;

  /**
   * Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
   *
   * @default 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions#executionTimeout
   */
  readonly executionTimeout?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigInitializationActions | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'executableFile': obj.executableFile,
    'executionTimeout': obj.executionTimeout,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig {
  /**
   * The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig#autoDeleteTime
   */
  readonly autoDeleteTime?: string;

  /**
   * The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig#autoDeleteTtl
   */
  readonly autoDeleteTtl?: string;

  /**
   * The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig#idleDeleteTtl
   */
  readonly idleDeleteTtl?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigLifecycleConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoDeleteTime': obj.autoDeleteTime,
    'autoDeleteTtl': obj.autoDeleteTtl,
    'idleDeleteTtl': obj.idleDeleteTtl,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig {
  /**
   * The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators[];

  /**
   * Disk option config settings.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig[];

  /**
   * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#image
   */
  readonly image?: string;

  /**
   * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators[];

  /**
   * Disk option config settings.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig[];

  /**
   * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#image
   */
  readonly image?: string;

  /**
   * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig {
  /**
   * Kerberos related configuration.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig#kerberosConfig
   */
  readonly kerberosConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig {
  /**
   * The version of software inside the cluster. It must be one of the supported Dataproc Versions, such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version. If unspecified, it defaults to the latest Debian version.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig#imageVersion
   */
  readonly imageVersion?: string;

  /**
   * The set of components to activate on the cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig#optionalComponents
   */
  readonly optionalComponents?: string[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'imageVersion': obj.imageVersion,
    'optionalComponents': obj.optionalComponents?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators[];

  /**
   * Disk option config settings.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig[];

  /**
   * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#image
   */
  readonly image?: string;

  /**
   * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
  /**
   * Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1*node-group-1`
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity#nodeGroup
   */
  readonly nodeGroup?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodeGroup': obj.nodeGroup,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity {
  /**
   * Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#consumeReservationType
   */
  readonly consumeReservationType?: string;

  /**
   * Corresponds to the label key of reservation resource.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#key
   */
  readonly key?: string;

  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumeReservationType': obj.consumeReservationType,
    'key': obj.key,
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
  /**
   * Defines whether instances have Integrity Monitoring enabled.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableIntegrityMonitoring
   */
  readonly enableIntegrityMonitoring?: boolean;

  /**
   * Defines whether instances have Secure Boot enabled.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableSecureBoot
   */
  readonly enableSecureBoot?: boolean;

  /**
   * Defines whether instances have the vTPM enabled.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableVtpm
   */
  readonly enableVtpm?: boolean;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableIntegrityMonitoring': obj.enableIntegrityMonitoring,
    'enableSecureBoot': obj.enableSecureBoot,
    'enableVtpm': obj.enableVtpm,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig {
  /**
   * Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
  /**
   * Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig {
  /**
   * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustAdminServer
   */
  readonly crossRealmTrustAdminServer?: string;

  /**
   * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustKdc
   */
  readonly crossRealmTrustKdc?: string;

  /**
   * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustRealm
   */
  readonly crossRealmTrustRealm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustSharedPassword
   */
  readonly crossRealmTrustSharedPassword?: string;

  /**
   * Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#enableKerberos
   */
  readonly enableKerberos?: boolean;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#kdcDbKey
   */
  readonly kdcDbKey?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keyPassword
   */
  readonly keyPassword?: string;

  /**
   * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keystore
   */
  readonly keystore?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keystorePassword
   */
  readonly keystorePassword?: string;

  /**
   * The uri of the KMS key used to encrypt various sensitive files.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#kmsKey
   */
  readonly kmsKey?: string;

  /**
   * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#realm
   */
  readonly realm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the root principal password.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#rootPrincipalPassword
   */
  readonly rootPrincipalPassword?: string;

  /**
   * The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#tgtLifetimeHours
   */
  readonly tgtLifetimeHours?: number;

  /**
   * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#truststore
   */
  readonly truststore?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#truststorePassword
   */
  readonly truststorePassword?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'crossRealmTrustAdminServer': obj.crossRealmTrustAdminServer,
    'crossRealmTrustKdc': obj.crossRealmTrustKdc,
    'crossRealmTrustRealm': obj.crossRealmTrustRealm,
    'crossRealmTrustSharedPassword': obj.crossRealmTrustSharedPassword,
    'enableKerberos': obj.enableKerberos,
    'kdcDbKey': obj.kdcDbKey,
    'keyPassword': obj.keyPassword,
    'keystore': obj.keystore,
    'keystorePassword': obj.keystorePassword,
    'kmsKey': obj.kmsKey,
    'realm': obj.realm,
    'rootPrincipalPassword': obj.rootPrincipalPassword,
    'tgtLifetimeHours': obj.tgtLifetimeHours,
    'truststore': obj.truststore,
    'truststorePassword': obj.truststorePassword,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig {
  /**
   * Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
  /**
   * Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1*node-group-1`
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity#nodeGroup
   */
  readonly nodeGroup?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodeGroup': obj.nodeGroup,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity {
  /**
   * Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#consumeReservationType
   */
  readonly consumeReservationType?: string;

  /**
   * Corresponds to the label key of reservation resource.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#key
   */
  readonly key?: string;

  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumeReservationType': obj.consumeReservationType,
    'key': obj.key,
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
  /**
   * Defines whether instances have Integrity Monitoring enabled.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableIntegrityMonitoring
   */
  readonly enableIntegrityMonitoring?: boolean;

  /**
   * Defines whether instances have Secure Boot enabled.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableSecureBoot
   */
  readonly enableSecureBoot?: boolean;

  /**
   * Defines whether instances have the vTPM enabled.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableVtpm
   */
  readonly enableVtpm?: boolean;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableIntegrityMonitoring': obj.enableIntegrityMonitoring,
    'enableSecureBoot': obj.enableSecureBoot,
    'enableVtpm': obj.enableVtpm,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig {
  /**
   * Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
  /**
   * Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig {
  /**
   * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustAdminServer
   */
  readonly crossRealmTrustAdminServer?: string;

  /**
   * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustKdc
   */
  readonly crossRealmTrustKdc?: string;

  /**
   * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustRealm
   */
  readonly crossRealmTrustRealm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustSharedPassword
   */
  readonly crossRealmTrustSharedPassword?: string;

  /**
   * Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#enableKerberos
   */
  readonly enableKerberos?: boolean;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#kdcDbKey
   */
  readonly kdcDbKey?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keyPassword
   */
  readonly keyPassword?: string;

  /**
   * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keystore
   */
  readonly keystore?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keystorePassword
   */
  readonly keystorePassword?: string;

  /**
   * The uri of the KMS key used to encrypt various sensitive files.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#kmsKey
   */
  readonly kmsKey?: string;

  /**
   * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#realm
   */
  readonly realm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the root principal password.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#rootPrincipalPassword
   */
  readonly rootPrincipalPassword?: string;

  /**
   * The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#tgtLifetimeHours
   */
  readonly tgtLifetimeHours?: number;

  /**
   * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#truststore
   */
  readonly truststore?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#truststorePassword
   */
  readonly truststorePassword?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'crossRealmTrustAdminServer': obj.crossRealmTrustAdminServer,
    'crossRealmTrustKdc': obj.crossRealmTrustKdc,
    'crossRealmTrustRealm': obj.crossRealmTrustRealm,
    'crossRealmTrustSharedPassword': obj.crossRealmTrustSharedPassword,
    'enableKerberos': obj.enableKerberos,
    'kdcDbKey': obj.kdcDbKey,
    'keyPassword': obj.keyPassword,
    'keystore': obj.keystore,
    'keystorePassword': obj.keystorePassword,
    'kmsKey': obj.kmsKey,
    'realm': obj.realm,
    'rootPrincipalPassword': obj.rootPrincipalPassword,
    'tgtLifetimeHours': obj.tgtLifetimeHours,
    'truststore': obj.truststore,
    'truststorePassword': obj.truststorePassword,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig
 */
export interface WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig {
  /**
   * Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig(obj: WorkflowTemplateSpecInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

