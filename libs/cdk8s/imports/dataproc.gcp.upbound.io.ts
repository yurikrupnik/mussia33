// generated by cdk8s
import { ApiObject, ApiObjectMetadata, GroupVersionKind } from 'cdk8s';
import { Construct } from 'constructs';


/**
 * AutoscalingPolicy is the Schema for the AutoscalingPolicys API. Describes an autoscaling policy for Dataproc cluster autoscaler.
 *
 * @schema AutoscalingPolicy
 */
export class AutoscalingPolicy extends ApiObject {
  /**
   * Returns the apiVersion and kind for "AutoscalingPolicy"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'AutoscalingPolicy',
  }

  /**
   * Renders a Kubernetes manifest for "AutoscalingPolicy".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: AutoscalingPolicyProps): any {
    return {
      ...AutoscalingPolicy.GVK,
      ...toJson_AutoscalingPolicyProps(props),
    };
  }

  /**
   * Defines a "AutoscalingPolicy" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: AutoscalingPolicyProps) {
    super(scope, id, {
      ...AutoscalingPolicy.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...AutoscalingPolicy.GVK,
      ...toJson_AutoscalingPolicyProps(resolved),
    };
  }
}

/**
 * AutoscalingPolicy is the Schema for the AutoscalingPolicys API. Describes an autoscaling policy for Dataproc cluster autoscaler.
 *
 * @schema AutoscalingPolicy
 */
export interface AutoscalingPolicyProps {
  /**
   * @schema AutoscalingPolicy#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * AutoscalingPolicySpec defines the desired state of AutoscalingPolicy
   *
   * @schema AutoscalingPolicy#spec
   */
  readonly spec: AutoscalingPolicySpec;

}

/**
 * Converts an object of type 'AutoscalingPolicyProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicyProps(obj: AutoscalingPolicyProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_AutoscalingPolicySpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * AutoscalingPolicySpec defines the desired state of AutoscalingPolicy
 *
 * @schema AutoscalingPolicySpec
 */
export interface AutoscalingPolicySpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
   *
   * @schema AutoscalingPolicySpec#deletionPolicy
   */
  readonly deletionPolicy?: AutoscalingPolicySpecDeletionPolicy;

  /**
   * @schema AutoscalingPolicySpec#forProvider
   */
  readonly forProvider: AutoscalingPolicySpecForProvider;

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema AutoscalingPolicySpec#providerConfigRef
   */
  readonly providerConfigRef?: AutoscalingPolicySpecProviderConfigRef;

  /**
   * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
   *
   * @schema AutoscalingPolicySpec#providerRef
   */
  readonly providerRef?: AutoscalingPolicySpecProviderRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema AutoscalingPolicySpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: AutoscalingPolicySpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema AutoscalingPolicySpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: AutoscalingPolicySpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'AutoscalingPolicySpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpec(obj: AutoscalingPolicySpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_AutoscalingPolicySpecForProvider(obj.forProvider),
    'providerConfigRef': toJson_AutoscalingPolicySpecProviderConfigRef(obj.providerConfigRef),
    'providerRef': toJson_AutoscalingPolicySpecProviderRef(obj.providerRef),
    'publishConnectionDetailsTo': toJson_AutoscalingPolicySpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_AutoscalingPolicySpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
 *
 * @schema AutoscalingPolicySpecDeletionPolicy
 */
export enum AutoscalingPolicySpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema AutoscalingPolicySpecForProvider
 */
export interface AutoscalingPolicySpecForProvider {
  /**
   * Basic algorithm for autoscaling. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProvider#basicAlgorithm
   */
  readonly basicAlgorithm?: AutoscalingPolicySpecForProviderBasicAlgorithm[];

  /**
   * The  location where the autoscaling policy should reside. The default value is global.
   *
   * @schema AutoscalingPolicySpecForProvider#location
   */
  readonly location?: string;

  /**
   * The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
   *
   * @schema AutoscalingPolicySpecForProvider#project
   */
  readonly project?: string;

  /**
   * Describes how the autoscaler will operate for secondary workers. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProvider#secondaryWorkerConfig
   */
  readonly secondaryWorkerConfig?: AutoscalingPolicySpecForProviderSecondaryWorkerConfig[];

  /**
   * Describes how the autoscaler will operate for primary workers. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProvider#workerConfig
   */
  readonly workerConfig?: AutoscalingPolicySpecForProviderWorkerConfig[];

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProvider(obj: AutoscalingPolicySpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'basicAlgorithm': obj.basicAlgorithm?.map(y => toJson_AutoscalingPolicySpecForProviderBasicAlgorithm(y)),
    'location': obj.location,
    'project': obj.project,
    'secondaryWorkerConfig': obj.secondaryWorkerConfig?.map(y => toJson_AutoscalingPolicySpecForProviderSecondaryWorkerConfig(y)),
    'workerConfig': obj.workerConfig?.map(y => toJson_AutoscalingPolicySpecForProviderWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema AutoscalingPolicySpecProviderConfigRef
 */
export interface AutoscalingPolicySpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema AutoscalingPolicySpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema AutoscalingPolicySpecProviderConfigRef#policy
   */
  readonly policy?: AutoscalingPolicySpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecProviderConfigRef(obj: AutoscalingPolicySpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_AutoscalingPolicySpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
 *
 * @schema AutoscalingPolicySpecProviderRef
 */
export interface AutoscalingPolicySpecProviderRef {
  /**
   * Name of the referenced object.
   *
   * @schema AutoscalingPolicySpecProviderRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema AutoscalingPolicySpecProviderRef#policy
   */
  readonly policy?: AutoscalingPolicySpecProviderRefPolicy;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecProviderRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecProviderRef(obj: AutoscalingPolicySpecProviderRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_AutoscalingPolicySpecProviderRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsTo
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: AutoscalingPolicySpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsTo(obj: AutoscalingPolicySpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_AutoscalingPolicySpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema AutoscalingPolicySpecWriteConnectionSecretToRef
 */
export interface AutoscalingPolicySpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema AutoscalingPolicySpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema AutoscalingPolicySpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecWriteConnectionSecretToRef(obj: AutoscalingPolicySpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderBasicAlgorithm
 */
export interface AutoscalingPolicySpecForProviderBasicAlgorithm {
  /**
   * Duration between scaling events. A scaling period starts after the update operation from the previous event has completed. Bounds: [2m, 1d]. Default: 2m.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithm#cooldownPeriod
   */
  readonly cooldownPeriod?: string;

  /**
   * YARN autoscaling configuration. Structure is documented below.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithm#yarnConfig
   */
  readonly yarnConfig: AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig[];

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderBasicAlgorithm' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderBasicAlgorithm(obj: AutoscalingPolicySpecForProviderBasicAlgorithm | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'cooldownPeriod': obj.cooldownPeriod,
    'yarnConfig': obj.yarnConfig?.map(y => toJson_AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig
 */
export interface AutoscalingPolicySpecForProviderSecondaryWorkerConfig {
  /**
   * Maximum number of instances for this group. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set. Bounds: [minInstances, ). Defaults to 0.
   *
   * @default 0.
   * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig#maxInstances
   */
  readonly maxInstances?: number;

  /**
   * Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
   *
   * @default 0.
   * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig#minInstances
   */
  readonly minInstances?: number;

  /**
   * Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker. The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if maxInstances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created. If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
   *
   * @schema AutoscalingPolicySpecForProviderSecondaryWorkerConfig#weight
   */
  readonly weight?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderSecondaryWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderSecondaryWorkerConfig(obj: AutoscalingPolicySpecForProviderSecondaryWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxInstances': obj.maxInstances,
    'minInstances': obj.minInstances,
    'weight': obj.weight,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderWorkerConfig
 */
export interface AutoscalingPolicySpecForProviderWorkerConfig {
  /**
   * Maximum number of instances for this group.
   *
   * @schema AutoscalingPolicySpecForProviderWorkerConfig#maxInstances
   */
  readonly maxInstances: number;

  /**
   * Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
   *
   * @default 2.
   * @schema AutoscalingPolicySpecForProviderWorkerConfig#minInstances
   */
  readonly minInstances?: number;

  /**
   * Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker. The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if maxInstances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created. If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
   *
   * @schema AutoscalingPolicySpecForProviderWorkerConfig#weight
   */
  readonly weight?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderWorkerConfig(obj: AutoscalingPolicySpecForProviderWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxInstances': obj.maxInstances,
    'minInstances': obj.minInstances,
    'weight': obj.weight,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema AutoscalingPolicySpecProviderConfigRefPolicy
 */
export interface AutoscalingPolicySpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema AutoscalingPolicySpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: AutoscalingPolicySpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema AutoscalingPolicySpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: AutoscalingPolicySpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecProviderConfigRefPolicy(obj: AutoscalingPolicySpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema AutoscalingPolicySpecProviderRefPolicy
 */
export interface AutoscalingPolicySpecProviderRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema AutoscalingPolicySpecProviderRefPolicy#resolution
   */
  readonly resolution?: AutoscalingPolicySpecProviderRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema AutoscalingPolicySpecProviderRefPolicy#resolve
   */
  readonly resolve?: AutoscalingPolicySpecProviderRefPolicyResolve;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecProviderRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecProviderRefPolicy(obj: AutoscalingPolicySpecProviderRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRef
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRef(obj: AutoscalingPolicySpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsToMetadata(obj: AutoscalingPolicySpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig
 */
export interface AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig {
  /**
   * Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations. Bounds: [0s, 1d].
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#gracefulDecommissionTimeout
   */
  readonly gracefulDecommissionTimeout: string;

  /**
   * Fraction of average pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. Bounds: [0.0, 1.0].
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleDownFactor
   */
  readonly scaleDownFactor: number;

  /**
   * Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change. Bounds: [0.0, 1.0]. Default: 0.0.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleDownMinWorkerFraction
   */
  readonly scaleDownMinWorkerFraction?: number;

  /**
   * Fraction of average pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). Bounds: [0.0, 1.0].
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleUpFactor
   */
  readonly scaleUpFactor: number;

  /**
   * Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change. Bounds: [0.0, 1.0]. Default: 0.0.
   *
   * @schema AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig#scaleUpMinWorkerFraction
   */
  readonly scaleUpMinWorkerFraction?: number;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig(obj: AutoscalingPolicySpecForProviderBasicAlgorithmYarnConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gracefulDecommissionTimeout': obj.gracefulDecommissionTimeout,
    'scaleDownFactor': obj.scaleDownFactor,
    'scaleDownMinWorkerFraction': obj.scaleDownMinWorkerFraction,
    'scaleUpFactor': obj.scaleUpFactor,
    'scaleUpMinWorkerFraction': obj.scaleUpMinWorkerFraction,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema AutoscalingPolicySpecProviderConfigRefPolicyResolution
 */
export enum AutoscalingPolicySpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema AutoscalingPolicySpecProviderConfigRefPolicyResolve
 */
export enum AutoscalingPolicySpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema AutoscalingPolicySpecProviderRefPolicyResolution
 */
export enum AutoscalingPolicySpecProviderRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema AutoscalingPolicySpecProviderRefPolicyResolve
 */
export enum AutoscalingPolicySpecProviderRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy(obj: AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum AutoscalingPolicySpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}


/**
 * Cluster is the Schema for the Clusters API. Manages a Cloud Dataproc cluster resource.
 *
 * @schema Cluster
 */
export class Cluster extends ApiObject {
  /**
   * Returns the apiVersion and kind for "Cluster"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'Cluster',
  }

  /**
   * Renders a Kubernetes manifest for "Cluster".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: ClusterProps): any {
    return {
      ...Cluster.GVK,
      ...toJson_ClusterProps(props),
    };
  }

  /**
   * Defines a "Cluster" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: ClusterProps) {
    super(scope, id, {
      ...Cluster.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...Cluster.GVK,
      ...toJson_ClusterProps(resolved),
    };
  }
}

/**
 * Cluster is the Schema for the Clusters API. Manages a Cloud Dataproc cluster resource.
 *
 * @schema Cluster
 */
export interface ClusterProps {
  /**
   * @schema Cluster#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * ClusterSpec defines the desired state of Cluster
   *
   * @schema Cluster#spec
   */
  readonly spec: ClusterSpec;

}

/**
 * Converts an object of type 'ClusterProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterProps(obj: ClusterProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_ClusterSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ClusterSpec defines the desired state of Cluster
 *
 * @schema ClusterSpec
 */
export interface ClusterSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
   *
   * @schema ClusterSpec#deletionPolicy
   */
  readonly deletionPolicy?: ClusterSpecDeletionPolicy;

  /**
   * @schema ClusterSpec#forProvider
   */
  readonly forProvider: ClusterSpecForProvider;

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema ClusterSpec#providerConfigRef
   */
  readonly providerConfigRef?: ClusterSpecProviderConfigRef;

  /**
   * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
   *
   * @schema ClusterSpec#providerRef
   */
  readonly providerRef?: ClusterSpecProviderRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema ClusterSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: ClusterSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema ClusterSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: ClusterSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'ClusterSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpec(obj: ClusterSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_ClusterSpecForProvider(obj.forProvider),
    'providerConfigRef': toJson_ClusterSpecProviderConfigRef(obj.providerConfigRef),
    'providerRef': toJson_ClusterSpecProviderRef(obj.providerRef),
    'publishConnectionDetailsTo': toJson_ClusterSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_ClusterSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
 *
 * @schema ClusterSpecDeletionPolicy
 */
export enum ClusterSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema ClusterSpecForProvider
 */
export interface ClusterSpecForProvider {
  /**
   * Allows you to configure various aspects of the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProvider#clusterConfig
   */
  readonly clusterConfig?: ClusterSpecForProviderClusterConfig[];

  /**
   * Does not affect auto scaling decomissioning from an autoscaling policy. Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress. Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs). Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of Duration). Only supported on Dataproc image versions 1.2 and higher. For more context see the docs
   *
   * @schema ClusterSpecForProvider#gracefulDecommissionTimeout
   */
  readonly gracefulDecommissionTimeout?: string;

  /**
   * The list of labels (key/value pairs) to be applied to instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name which is the name of the cluster.
   *
   * @schema ClusterSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The name of the cluster, unique within the project and zone.
   *
   * @schema ClusterSpecForProvider#name
   */
  readonly name: string;

  /**
   * The ID of the project in which the cluster will exist. If it is not provided, the provider project is used.
   *
   * @schema ClusterSpecForProvider#project
   */
  readonly project?: string;

  /**
   * The region in which the cluster and associated nodes will be created in. Defaults to global.
   *
   * @default global.
   * @schema ClusterSpecForProvider#region
   */
  readonly region?: string;

  /**
   * Allows you to configure a virtual Dataproc on GKE cluster. Structure defined below.
   *
   * @schema ClusterSpecForProvider#virtualClusterConfig
   */
  readonly virtualClusterConfig?: ClusterSpecForProviderVirtualClusterConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProvider(obj: ClusterSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterConfig': obj.clusterConfig?.map(y => toJson_ClusterSpecForProviderClusterConfig(y)),
    'gracefulDecommissionTimeout': obj.gracefulDecommissionTimeout,
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'name': obj.name,
    'project': obj.project,
    'region': obj.region,
    'virtualClusterConfig': obj.virtualClusterConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema ClusterSpecProviderConfigRef
 */
export interface ClusterSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecProviderConfigRef#policy
   */
  readonly policy?: ClusterSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecProviderConfigRef(obj: ClusterSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
 *
 * @schema ClusterSpecProviderRef
 */
export interface ClusterSpecProviderRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecProviderRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecProviderRef#policy
   */
  readonly policy?: ClusterSpecProviderRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecProviderRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecProviderRef(obj: ClusterSpecProviderRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecProviderRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema ClusterSpecPublishConnectionDetailsTo
 */
export interface ClusterSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema ClusterSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: ClusterSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema ClusterSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: ClusterSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema ClusterSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsTo(obj: ClusterSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_ClusterSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_ClusterSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema ClusterSpecWriteConnectionSecretToRef
 */
export interface ClusterSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema ClusterSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema ClusterSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'ClusterSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecWriteConnectionSecretToRef(obj: ClusterSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfig
 */
export interface ClusterSpecForProviderClusterConfig {
  /**
   * The autoscaling policy config associated with the cluster. Note that once set, if autoscaling_config is the only field set in cluster_config, it can only be removed by setting policy_uri = "", rather than removing the whole block. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#autoscalingConfig
   */
  readonly autoscalingConfig?: ClusterSpecForProviderClusterConfigAutoscalingConfig[];

  /**
   * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#dataprocMetricConfig
   */
  readonly dataprocMetricConfig?: ClusterSpecForProviderClusterConfigDataprocMetricConfig[];

  /**
   * The Customer managed encryption keys settings for the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#encryptionConfig
   */
  readonly encryptionConfig?: ClusterSpecForProviderClusterConfigEncryptionConfig[];

  /**
   * The config settings for port access on the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#endpointConfig
   */
  readonly endpointConfig?: ClusterSpecForProviderClusterConfigEndpointConfig[];

  /**
   * Common config settings for resources of Google Compute Engine cluster instances, applicable to all instances in the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#gceClusterConfig
   */
  readonly gceClusterConfig?: ClusterSpecForProviderClusterConfigGceClusterConfig[];

  /**
   * Commands to execute on each node after config is completed. You can specify multiple versions of these. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#initializationAction
   */
  readonly initializationAction?: ClusterSpecForProviderClusterConfigInitializationAction[];

  /**
   * The settings for auto deletion cluster schedule. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#lifecycleConfig
   */
  readonly lifecycleConfig?: ClusterSpecForProviderClusterConfigLifecycleConfig[];

  /**
   * The Google Compute Engine config settings for the master instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#masterConfig
   */
  readonly masterConfig?: ClusterSpecForProviderClusterConfigMasterConfig[];

  /**
   * The config setting for metastore service with the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#metastoreConfig
   */
  readonly metastoreConfig?: ClusterSpecForProviderClusterConfigMetastoreConfig[];

  /**
   * The Google Compute Engine config settings for the additional instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#preemptibleWorkerConfig
   */
  readonly preemptibleWorkerConfig?: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig[];

  /**
   * Security related configuration. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#securityConfig
   */
  readonly securityConfig?: ClusterSpecForProviderClusterConfigSecurityConfig[];

  /**
   * The config settings for software inside the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#softwareConfig
   */
  readonly softwareConfig?: ClusterSpecForProviderClusterConfigSoftwareConfig[];

  /**
   * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
   *
   * @schema ClusterSpecForProviderClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

  /**
   * The Cloud Storage temp bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
   *
   * @schema ClusterSpecForProviderClusterConfig#tempBucket
   */
  readonly tempBucket?: string;

  /**
   * The Google Compute Engine config settings for the worker instances in a cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderClusterConfig#workerConfig
   */
  readonly workerConfig?: ClusterSpecForProviderClusterConfigWorkerConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfig(obj: ClusterSpecForProviderClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscalingConfig': obj.autoscalingConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigAutoscalingConfig(y)),
    'dataprocMetricConfig': obj.dataprocMetricConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfig(y)),
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigEncryptionConfig(y)),
    'endpointConfig': obj.endpointConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigEndpointConfig(y)),
    'gceClusterConfig': obj.gceClusterConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfig(y)),
    'initializationAction': obj.initializationAction?.map(y => toJson_ClusterSpecForProviderClusterConfigInitializationAction(y)),
    'lifecycleConfig': obj.lifecycleConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigLifecycleConfig(y)),
    'masterConfig': obj.masterConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigMasterConfig(y)),
    'metastoreConfig': obj.metastoreConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigMetastoreConfig(y)),
    'preemptibleWorkerConfig': obj.preemptibleWorkerConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig(y)),
    'securityConfig': obj.securityConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigSecurityConfig(y)),
    'softwareConfig': obj.softwareConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigSoftwareConfig(y)),
    'stagingBucket': obj.stagingBucket,
    'tempBucket': obj.tempBucket,
    'workerConfig': obj.workerConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfig {
  /**
   * Configuration of auxiliary services used by this cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfig#auxiliaryServicesConfig
   */
  readonly auxiliaryServicesConfig?: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig[];

  /**
   * The configuration for running the Dataproc cluster on Kubernetes. Structure defined below.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfig#kubernetesClusterConfig
   */
  readonly kubernetesClusterConfig?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig[];

  /**
   * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfig(obj: ClusterSpecForProviderVirtualClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'auxiliaryServicesConfig': obj.auxiliaryServicesConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig(y)),
    'kubernetesClusterConfig': obj.kubernetesClusterConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig(y)),
    'stagingBucket': obj.stagingBucket,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecProviderConfigRefPolicy
 */
export interface ClusterSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecProviderConfigRefPolicy(obj: ClusterSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecProviderRefPolicy
 */
export interface ClusterSpecProviderRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecProviderRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecProviderRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecProviderRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecProviderRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecProviderRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecProviderRefPolicy(obj: ClusterSpecProviderRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRef
 */
export interface ClusterSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: ClusterSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsToConfigRef(obj: ClusterSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema ClusterSpecPublishConnectionDetailsToMetadata
 */
export interface ClusterSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema ClusterSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema ClusterSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema ClusterSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsToMetadata(obj: ClusterSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigAutoscalingConfig
 */
export interface ClusterSpecForProviderClusterConfigAutoscalingConfig {
  /**
   * The autoscaling policy used by the cluster.
   *
   * @schema ClusterSpecForProviderClusterConfigAutoscalingConfig#policyUri
   */
  readonly policyUri: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigAutoscalingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigAutoscalingConfig(obj: ClusterSpecForProviderClusterConfigAutoscalingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'policyUri': obj.policyUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfig
 */
export interface ClusterSpecForProviderClusterConfigDataprocMetricConfig {
  /**
   * Metrics sources to enable.
   *
   * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfig#metrics
   */
  readonly metrics: ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigDataprocMetricConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfig(obj: ClusterSpecForProviderClusterConfigDataprocMetricConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metrics': obj.metrics?.map(y => toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigEncryptionConfig
 */
export interface ClusterSpecForProviderClusterConfigEncryptionConfig {
  /**
   * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
   *
   * @schema ClusterSpecForProviderClusterConfigEncryptionConfig#kmsKeyName
   */
  readonly kmsKeyName: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigEncryptionConfig(obj: ClusterSpecForProviderClusterConfigEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kmsKeyName': obj.kmsKeyName,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigEndpointConfig
 */
export interface ClusterSpecForProviderClusterConfigEndpointConfig {
  /**
   * The flag to enable http access to specific ports on the cluster from external sources (aka Component Gateway). Defaults to false.
   *
   * @default false.
   * @schema ClusterSpecForProviderClusterConfigEndpointConfig#enableHttpPortAccess
   */
  readonly enableHttpPortAccess: boolean;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigEndpointConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigEndpointConfig(obj: ClusterSpecForProviderClusterConfigEndpointConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableHttpPortAccess': obj.enableHttpPortAccess,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfig
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfig {
  /**
   * By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. If set to true, all instances in the cluster will only have internal IP addresses. Note: Private Google Access (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster will be launched in.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#internalIpOnly
   */
  readonly internalIpOnly?: boolean;

  /**
   * A map of the Compute Engine metadata entries to add to all instances (see Project and instance metadata).
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#metadata
   */
  readonly metadata?: { [key: string]: string };

  /**
   * The name or self_link of the Google Compute Engine network to the cluster will be part of. Conflicts with subnetwork. If neither is specified, this defaults to the "default" network.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#network
   */
  readonly network?: string;

  /**
   * Node Group Affinity for sole-tenant clusters.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#nodeGroupAffinity
   */
  readonly nodeGroupAffinity?: ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity[];

  /**
   * Reservation Affinity for consuming zonal reservation.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#reservationAffinity
   */
  readonly reservationAffinity?: ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity[];

  /**
   * The service account to be used by the Node VMs. If not specified, the "default" service account is used.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccount
   */
  readonly serviceAccount?: string;

  /**
   * Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccountRef
   */
  readonly serviceAccountRef?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef;

  /**
   * The set of Google API scopes to be made available on all of the node VMs under the service_account specified. Both OAuth2 URLs and gcloud short names are supported. To allow full access to all Cloud APIs, use the cloud-platform scope. See a complete list of scopes here.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccountScopes
   */
  readonly serviceAccountScopes?: string[];

  /**
   * Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#serviceAccountSelector
   */
  readonly serviceAccountSelector?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector;

  /**
   * Shielded Instance Config for clusters using Compute Engine Shielded VMs.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#shieldedInstanceConfig
   */
  readonly shieldedInstanceConfig?: ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig[];

  /**
   * The name or self_link of the Google Compute Engine subnetwork the cluster will be part of. Conflicts with network.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * The list of instance tags applied to instances in the cluster. Tags are used to identify valid sources or targets for network firewalls.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#tags
   */
  readonly tags?: string[];

  /**
   * The GCP zone where your data is stored and used (i.e. where the master and the worker nodes will be created in). If region is set to 'global' (default) then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement to determine this automatically for you. Note: This setting additionally determines and restricts which computing resources are available for use with other configs such as cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfig#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfig(obj: ClusterSpecForProviderClusterConfigGceClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'internalIpOnly': obj.internalIpOnly,
    'metadata': ((obj.metadata) === undefined) ? undefined : (Object.entries(obj.metadata).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'network': obj.network,
    'nodeGroupAffinity': obj.nodeGroupAffinity?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity(y)),
    'reservationAffinity': obj.reservationAffinity?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity(y)),
    'serviceAccount': obj.serviceAccount,
    'serviceAccountRef': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef(obj.serviceAccountRef),
    'serviceAccountScopes': obj.serviceAccountScopes?.map(y => y),
    'serviceAccountSelector': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector(obj.serviceAccountSelector),
    'shieldedInstanceConfig': obj.shieldedInstanceConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig(y)),
    'subnetwork': obj.subnetwork,
    'tags': obj.tags?.map(y => y),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigInitializationAction
 */
export interface ClusterSpecForProviderClusterConfigInitializationAction {
  /**
   * The script to be executed during initialization of the cluster. The script must be a GCS file with a gs:// prefix.
   *
   * @schema ClusterSpecForProviderClusterConfigInitializationAction#script
   */
  readonly script: string;

  /**
   * The maximum duration (in seconds) which script is allowed to take to execute its action. GCP will default to a predetermined computed value if not set (currently 300).
   *
   * @schema ClusterSpecForProviderClusterConfigInitializationAction#timeoutSec
   */
  readonly timeoutSec?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigInitializationAction' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigInitializationAction(obj: ClusterSpecForProviderClusterConfigInitializationAction | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'script': obj.script,
    'timeoutSec': obj.timeoutSec,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigLifecycleConfig
 */
export interface ClusterSpecForProviderClusterConfigLifecycleConfig {
  /**
   * The time when cluster will be auto-deleted. A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
   *
   * @schema ClusterSpecForProviderClusterConfigLifecycleConfig#autoDeleteTime
   */
  readonly autoDeleteTime?: string;

  /**
   * The duration to keep the cluster alive while idling (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
   *
   * @schema ClusterSpecForProviderClusterConfigLifecycleConfig#idleDeleteTtl
   */
  readonly idleDeleteTtl?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigLifecycleConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigLifecycleConfig(obj: ClusterSpecForProviderClusterConfigLifecycleConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoDeleteTime': obj.autoDeleteTime,
    'idleDeleteTtl': obj.idleDeleteTtl,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMasterConfig
 */
export interface ClusterSpecForProviderClusterConfigMasterConfig {
  /**
   * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#accelerators
   */
  readonly accelerators?: ClusterSpecForProviderClusterConfigMasterConfigAccelerators[];

  /**
   * Disk Config
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecForProviderClusterConfigMasterConfigDiskConfig[];

  /**
   * The URI for the image to use for this worker.  See the guide for more information.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#imageUri
   */
  readonly imageUri?: string;

  /**
   * The name of a Google Compute Engine machine type to create for the master. If not specified, GCP will default to a predetermined computed value (currently n1-standard-4).
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#machineType
   */
  readonly machineType?: string;

  /**
   * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone. See the guide for details about which CPU families are available (and defaulted) for each zone.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Specifies the number of master nodes to create. If not specified, GCP will default to a predetermined computed value (currently 1).
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfig#numInstances
   */
  readonly numInstances?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMasterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMasterConfig(obj: ClusterSpecForProviderClusterConfigMasterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_ClusterSpecForProviderClusterConfigMasterConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigMasterConfigDiskConfig(y)),
    'imageUri': obj.imageUri,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMetastoreConfig
 */
export interface ClusterSpecForProviderClusterConfigMetastoreConfig {
  /**
   * Resource name of an existing Dataproc Metastore service.
   *
   * @schema ClusterSpecForProviderClusterConfigMetastoreConfig#dataprocMetastoreService
   */
  readonly dataprocMetastoreService: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMetastoreConfig(obj: ClusterSpecForProviderClusterConfigMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocMetastoreService': obj.dataprocMetastoreService,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig
 */
export interface ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig {
  /**
   * Disk Config
   *
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig[];

  /**
   * Specifies the number of preemptible nodes to create. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE Accepted values are:
   *
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig(obj: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig(y)),
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigSecurityConfig
 */
export interface ClusterSpecForProviderClusterConfigSecurityConfig {
  /**
   * Kerberos Configuration
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfig#kerberosConfig
   */
  readonly kerberosConfig: ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigSecurityConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigSecurityConfig(obj: ClusterSpecForProviderClusterConfigSecurityConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigSoftwareConfig
 */
export interface ClusterSpecForProviderClusterConfigSoftwareConfig {
  /**
   * The Cloud Dataproc image version to use for the cluster - this controls the sets of software versions installed onto the nodes when you create clusters. If not specified, defaults to the latest version. For a list of valid versions see Cloud Dataproc versions
   *
   * @schema ClusterSpecForProviderClusterConfigSoftwareConfig#imageVersion
   */
  readonly imageVersion?: string;

  /**
   * The set of optional components to activate on the cluster. Accepted values are:
   *
   * @schema ClusterSpecForProviderClusterConfigSoftwareConfig#optionalComponents
   */
  readonly optionalComponents?: string[];

  /**
   * A list of override and additional properties (key/value pairs) used to modify various aspects of the common configuration files used when creating a cluster. For a list of valid properties please see Cluster properties
   *
   * @schema ClusterSpecForProviderClusterConfigSoftwareConfig#overrideProperties
   */
  readonly overrideProperties?: { [key: string]: string };

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigSoftwareConfig(obj: ClusterSpecForProviderClusterConfigSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'imageVersion': obj.imageVersion,
    'optionalComponents': obj.optionalComponents?.map(y => y),
    'overrideProperties': ((obj.overrideProperties) === undefined) ? undefined : (Object.entries(obj.overrideProperties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigWorkerConfig
 */
export interface ClusterSpecForProviderClusterConfigWorkerConfig {
  /**
   * The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#accelerators
   */
  readonly accelerators?: ClusterSpecForProviderClusterConfigWorkerConfigAccelerators[];

  /**
   * Disk Config
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#diskConfig
   */
  readonly diskConfig?: ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig[];

  /**
   * The URI for the image to use for this worker.  See the guide for more information.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#imageUri
   */
  readonly imageUri?: string;

  /**
   * The name of a Google Compute Engine machine type to create for the worker nodes. If not specified, GCP will default to a predetermined computed value (currently n1-standard-4).
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone. See the guide for details about which CPU families are available (and defaulted) for each zone.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Specifies the number of worker nodes to create. If not specified, GCP will default to a predetermined computed value (currently 2). There is currently a beta feature which allows you to run a Single Node Cluster. In order to take advantage of this you need to set "dataproc:dataproc.allow.zero.workers" = "true" in cluster_config.software_config.properties
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfig#numInstances
   */
  readonly numInstances?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigWorkerConfig(obj: ClusterSpecForProviderClusterConfigWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_ClusterSpecForProviderClusterConfigWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig(y)),
    'imageUri': obj.imageUri,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig {
  /**
   * The config setting for metastore service with the cluster. Structure defined below.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig#metastoreConfig
   */
  readonly metastoreConfig?: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig[];

  /**
   * The Spark History Server configuration for the workload.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig#sparkHistoryServerConfig
   */
  readonly sparkHistoryServerConfig?: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig(obj: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metastoreConfig': obj.metastoreConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig(y)),
    'sparkHistoryServerConfig': obj.sparkHistoryServerConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig {
  /**
   * The configuration for running the Dataproc cluster on GKE.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig#gkeClusterConfig
   */
  readonly gkeClusterConfig: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig[];

  /**
   * A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig#kubernetesNamespace
   */
  readonly kubernetesNamespace?: string;

  /**
   * The software configuration for this Dataproc cluster running on Kubernetes.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig#kubernetesSoftwareConfig
   */
  readonly kubernetesSoftwareConfig: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gkeClusterConfig': obj.gkeClusterConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig(y)),
    'kubernetesNamespace': obj.kubernetesNamespace,
    'kubernetesSoftwareConfig': obj.kubernetesSoftwareConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecProviderConfigRefPolicyResolution
 */
export enum ClusterSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecProviderConfigRefPolicyResolve
 */
export enum ClusterSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecProviderRefPolicyResolution
 */
export enum ClusterSpecProviderRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecProviderRefPolicyResolve
 */
export enum ClusterSpecProviderRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface ClusterSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecPublishConnectionDetailsToConfigRefPolicy(obj: ClusterSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics
 */
export interface ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics {
  /**
   * One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
   *
   * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics#metricOverrides
   */
  readonly metricOverrides?: string[];

  /**
   * A source for the collection of Dataproc OSS metrics (see available OSS metrics).
   *
   * @schema ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics#metricSource
   */
  readonly metricSource: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics(obj: ClusterSpecForProviderClusterConfigDataprocMetricConfigMetrics | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metricOverrides': obj.metricOverrides?.map(y => y),
    'metricSource': obj.metricSource,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity {
  /**
   * The URI of a sole-tenant node group resource that the cluster will be created on.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity#nodeGroupUri
   */
  readonly nodeGroupUri: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity(obj: ClusterSpecForProviderClusterConfigGceClusterConfigNodeGroupAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodeGroupUri': obj.nodeGroupUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity {
  /**
   * Corresponds to the type of reservation consumption.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity#consumeReservationType
   */
  readonly consumeReservationType?: string;

  /**
   * Corresponds to the label key of reservation resource.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity#key
   */
  readonly key?: string;

  /**
   * Corresponds to the label values of reservation resource.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity(obj: ClusterSpecForProviderClusterConfigGceClusterConfigReservationAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumeReservationType': obj.consumeReservationType,
    'key': obj.key,
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef {
  /**
   * Name of the referenced object.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef#policy
   */
  readonly policy?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector#policy
   */
  readonly policy?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig {
  /**
   * Defines whether instances have integrity monitoring enabled.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableIntegrityMonitoring
   */
  readonly enableIntegrityMonitoring?: boolean;

  /**
   * Defines whether instances have Secure Boot enabled.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableSecureBoot
   */
  readonly enableSecureBoot?: boolean;

  /**
   * Defines whether instances have the vTPM enabled.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig#enableVtpm
   */
  readonly enableVtpm?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig(obj: ClusterSpecForProviderClusterConfigGceClusterConfigShieldedInstanceConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableIntegrityMonitoring': obj.enableIntegrityMonitoring,
    'enableSecureBoot': obj.enableSecureBoot,
    'enableVtpm': obj.enableVtpm,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMasterConfigAccelerators
 */
export interface ClusterSpecForProviderClusterConfigMasterConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount: number;

  /**
   * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfigAccelerators#acceleratorType
   */
  readonly acceleratorType: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMasterConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMasterConfigAccelerators(obj: ClusterSpecForProviderClusterConfigMasterConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig
 */
export interface ClusterSpecForProviderClusterConfigMasterConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigMasterConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigMasterConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigMasterConfigDiskConfig(obj: ClusterSpecForProviderClusterConfigMasterConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig
 */
export interface ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig(obj: ClusterSpecForProviderClusterConfigPreemptibleWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig
 */
export interface ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig {
  /**
   * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustAdminServer
   */
  readonly crossRealmTrustAdminServer?: string;

  /**
   * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustKdc
   */
  readonly crossRealmTrustKdc?: string;

  /**
   * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustRealm
   */
  readonly crossRealmTrustRealm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#crossRealmTrustSharedPasswordUri
   */
  readonly crossRealmTrustSharedPasswordUri?: string;

  /**
   * Flag to indicate whether to Kerberize the cluster.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#enableKerberos
   */
  readonly enableKerberos?: boolean;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#kdcDbKeyUri
   */
  readonly kdcDbKeyUri?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#keyPasswordUri
   */
  readonly keyPasswordUri?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificated, the password is generated by Dataproc.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#keystorePasswordUri
   */
  readonly keystorePasswordUri?: string;

  /**
   * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#keystoreUri
   */
  readonly keystoreUri?: string;

  /**
   * The URI of the KMS key used to encrypt various sensitive files.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#kmsKeyUri
   */
  readonly kmsKeyUri: string;

  /**
   * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#realm
   */
  readonly realm?: string;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the root principal password.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#rootPrincipalPasswordUri
   */
  readonly rootPrincipalPasswordUri: string;

  /**
   * The lifetime of the ticket granting ticket, in hours.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#tgtLifetimeHours
   */
  readonly tgtLifetimeHours?: number;

  /**
   * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#truststorePasswordUri
   */
  readonly truststorePasswordUri?: string;

  /**
   * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig#truststoreUri
   */
  readonly truststoreUri?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig(obj: ClusterSpecForProviderClusterConfigSecurityConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'crossRealmTrustAdminServer': obj.crossRealmTrustAdminServer,
    'crossRealmTrustKdc': obj.crossRealmTrustKdc,
    'crossRealmTrustRealm': obj.crossRealmTrustRealm,
    'crossRealmTrustSharedPasswordUri': obj.crossRealmTrustSharedPasswordUri,
    'enableKerberos': obj.enableKerberos,
    'kdcDbKeyUri': obj.kdcDbKeyUri,
    'keyPasswordUri': obj.keyPasswordUri,
    'keystorePasswordUri': obj.keystorePasswordUri,
    'keystoreUri': obj.keystoreUri,
    'kmsKeyUri': obj.kmsKeyUri,
    'realm': obj.realm,
    'rootPrincipalPasswordUri': obj.rootPrincipalPasswordUri,
    'tgtLifetimeHours': obj.tgtLifetimeHours,
    'truststorePasswordUri': obj.truststorePasswordUri,
    'truststoreUri': obj.truststoreUri,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigWorkerConfigAccelerators
 */
export interface ClusterSpecForProviderClusterConfigWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount: number;

  /**
   * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigWorkerConfigAccelerators(obj: ClusterSpecForProviderClusterConfigWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig
 */
export interface ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig {
  /**
   * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
   *
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * The disk type of the primary disk attached to each node. One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
   *
   * @default pd-standard".
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
   *
   * @default 0.
   * @schema ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig(obj: ClusterSpecForProviderClusterConfigWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
  /**
   * Resource name of an existing Dataproc Metastore service.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig#dataprocMetastoreService
   */
  readonly dataprocMetastoreService?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig(obj: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocMetastoreService': obj.dataprocMetastoreService,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
  /**
   * Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig#dataprocCluster
   */
  readonly dataprocCluster?: string;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig(obj: ClusterSpecForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dataprocCluster': obj.dataprocCluster,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
  /**
   * A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional)
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig#gkeClusterTarget
   */
  readonly gkeClusterTarget?: string;

  /**
   * GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig#nodePoolTarget
   */
  readonly nodePoolTarget?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gkeClusterTarget': obj.gkeClusterTarget,
    'nodePoolTarget': obj.nodePoolTarget?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
  /**
   * The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig#componentVersion
   */
  readonly componentVersion: { [key: string]: string };

  /**
   * The properties to set on daemon config files. Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'componentVersion': ((obj.componentVersion) === undefined) ? undefined : (Object.entries(obj.componentVersion).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum ClusterSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy#resolution
   */
  readonly resolution?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy#resolve
   */
  readonly resolve?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy
 */
export interface ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy#resolution
   */
  readonly resolution?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy#resolve
   */
  readonly resolve?: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve;

}

/**
 * Converts an object of type 'ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy(obj: ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
  /**
   * The target GKE node pool.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#nodePool
   */
  readonly nodePool: string;

  /**
   * (Input only) The configuration for the GKE node pool. If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#nodePoolConfig
   */
  readonly nodePoolConfig?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig[];

  /**
   * The roles associated with the GKE node pool. One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget#roles
   */
  readonly roles: string[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodePool': obj.nodePool,
    'nodePoolConfig': obj.nodePoolConfig?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig(y)),
    'roles': obj.roles?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve
 */
export enum ClusterSpecForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
  /**
   * The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#autoscaling
   */
  readonly autoscaling?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling[];

  /**
   * The node pool configuration.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#config
   */
  readonly config?: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig[];

  /**
   * The list of Compute Engine zones where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig#locations
   */
  readonly locations: string[];

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscaling': obj.autoscaling?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling(y)),
    'config': obj.config?.map(y => toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig(y)),
    'locations': obj.locations?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
  /**
   * The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling#maxNodeCount
   */
  readonly maxNodeCount?: number;

  /**
   * The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling#minNodeCount
   */
  readonly minNodeCount?: number;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxNodeCount': obj.maxNodeCount,
    'minNodeCount': obj.minNodeCount,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig
 */
export interface ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
  /**
   * The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#localSsdCount
   */
  readonly localSsdCount?: number;

  /**
   * The name of a Compute Engine machine type.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Whether the nodes are created as preemptible VM instances. Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#preemptible
   */
  readonly preemptible?: boolean;

  /**
   * Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
   *
   * @schema ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig#spot
   */
  readonly spot?: boolean;

}

/**
 * Converts an object of type 'ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig(obj: ClusterSpecForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'localSsdCount': obj.localSsdCount,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'preemptible': obj.preemptible,
    'spot': obj.spot,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */


/**
 * Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.
 *
 * @schema Job
 */
export class Job extends ApiObject {
  /**
   * Returns the apiVersion and kind for "Job"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'Job',
  }

  /**
   * Renders a Kubernetes manifest for "Job".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: JobProps): any {
    return {
      ...Job.GVK,
      ...toJson_JobProps(props),
    };
  }

  /**
   * Defines a "Job" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: JobProps) {
    super(scope, id, {
      ...Job.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...Job.GVK,
      ...toJson_JobProps(resolved),
    };
  }
}

/**
 * Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.
 *
 * @schema Job
 */
export interface JobProps {
  /**
   * @schema Job#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * JobSpec defines the desired state of Job
   *
   * @schema Job#spec
   */
  readonly spec: JobSpec;

}

/**
 * Converts an object of type 'JobProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobProps(obj: JobProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_JobSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * JobSpec defines the desired state of Job
 *
 * @schema JobSpec
 */
export interface JobSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
   *
   * @schema JobSpec#deletionPolicy
   */
  readonly deletionPolicy?: JobSpecDeletionPolicy;

  /**
   * @schema JobSpec#forProvider
   */
  readonly forProvider: JobSpecForProvider;

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema JobSpec#providerConfigRef
   */
  readonly providerConfigRef?: JobSpecProviderConfigRef;

  /**
   * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
   *
   * @schema JobSpec#providerRef
   */
  readonly providerRef?: JobSpecProviderRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema JobSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: JobSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema JobSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: JobSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'JobSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpec(obj: JobSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_JobSpecForProvider(obj.forProvider),
    'providerConfigRef': toJson_JobSpecProviderConfigRef(obj.providerConfigRef),
    'providerRef': toJson_JobSpecProviderRef(obj.providerRef),
    'publishConnectionDetailsTo': toJson_JobSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_JobSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
 *
 * @schema JobSpecDeletionPolicy
 */
export enum JobSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema JobSpecForProvider
 */
export interface JobSpecForProvider {
  /**
   * By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure that the job is first cancelled before issuing the delete.
   *
   * @schema JobSpecForProvider#forceDelete
   */
  readonly forceDelete?: boolean;

  /**
   * @schema JobSpecForProvider#hadoopConfig
   */
  readonly hadoopConfig?: JobSpecForProviderHadoopConfig[];

  /**
   * @schema JobSpecForProvider#hiveConfig
   */
  readonly hiveConfig?: JobSpecForProviderHiveConfig[];

  /**
   * The list of labels (key/value pairs) to add to the job.
   *
   * @schema JobSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * @schema JobSpecForProvider#pigConfig
   */
  readonly pigConfig?: JobSpecForProviderPigConfig[];

  /**
   * @schema JobSpecForProvider#placement
   */
  readonly placement: JobSpecForProviderPlacement[];

  /**
   * @schema JobSpecForProvider#prestoConfig
   */
  readonly prestoConfig?: JobSpecForProviderPrestoConfig[];

  /**
   * The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider project is used.
   *
   * @schema JobSpecForProvider#project
   */
  readonly project?: string;

  /**
   * @schema JobSpecForProvider#pysparkConfig
   */
  readonly pysparkConfig?: JobSpecForProviderPysparkConfig[];

  /**
   * @schema JobSpecForProvider#reference
   */
  readonly reference?: JobSpecForProviderReference[];

  /**
   * The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If not specified, defaults to global.
   *
   * @schema JobSpecForProvider#region
   */
  readonly region?: string;

  /**
   * Reference to a Cluster in dataproc to populate region.
   *
   * @schema JobSpecForProvider#regionRef
   */
  readonly regionRef?: JobSpecForProviderRegionRef;

  /**
   * Selector for a Cluster in dataproc to populate region.
   *
   * @schema JobSpecForProvider#regionSelector
   */
  readonly regionSelector?: JobSpecForProviderRegionSelector;

  /**
   * @schema JobSpecForProvider#scheduling
   */
  readonly scheduling?: JobSpecForProviderScheduling[];

  /**
   * @schema JobSpecForProvider#sparkConfig
   */
  readonly sparkConfig?: JobSpecForProviderSparkConfig[];

  /**
   * @schema JobSpecForProvider#sparksqlConfig
   */
  readonly sparksqlConfig?: JobSpecForProviderSparksqlConfig[];

}

/**
 * Converts an object of type 'JobSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProvider(obj: JobSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'forceDelete': obj.forceDelete,
    'hadoopConfig': obj.hadoopConfig?.map(y => toJson_JobSpecForProviderHadoopConfig(y)),
    'hiveConfig': obj.hiveConfig?.map(y => toJson_JobSpecForProviderHiveConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pigConfig': obj.pigConfig?.map(y => toJson_JobSpecForProviderPigConfig(y)),
    'placement': obj.placement?.map(y => toJson_JobSpecForProviderPlacement(y)),
    'prestoConfig': obj.prestoConfig?.map(y => toJson_JobSpecForProviderPrestoConfig(y)),
    'project': obj.project,
    'pysparkConfig': obj.pysparkConfig?.map(y => toJson_JobSpecForProviderPysparkConfig(y)),
    'reference': obj.reference?.map(y => toJson_JobSpecForProviderReference(y)),
    'region': obj.region,
    'regionRef': toJson_JobSpecForProviderRegionRef(obj.regionRef),
    'regionSelector': toJson_JobSpecForProviderRegionSelector(obj.regionSelector),
    'scheduling': obj.scheduling?.map(y => toJson_JobSpecForProviderScheduling(y)),
    'sparkConfig': obj.sparkConfig?.map(y => toJson_JobSpecForProviderSparkConfig(y)),
    'sparksqlConfig': obj.sparksqlConfig?.map(y => toJson_JobSpecForProviderSparksqlConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema JobSpecProviderConfigRef
 */
export interface JobSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecProviderConfigRef#policy
   */
  readonly policy?: JobSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'JobSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecProviderConfigRef(obj: JobSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
 *
 * @schema JobSpecProviderRef
 */
export interface JobSpecProviderRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecProviderRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecProviderRef#policy
   */
  readonly policy?: JobSpecProviderRefPolicy;

}

/**
 * Converts an object of type 'JobSpecProviderRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecProviderRef(obj: JobSpecProviderRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecProviderRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema JobSpecPublishConnectionDetailsTo
 */
export interface JobSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema JobSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: JobSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema JobSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: JobSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema JobSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsTo(obj: JobSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_JobSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_JobSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema JobSpecWriteConnectionSecretToRef
 */
export interface JobSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema JobSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema JobSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'JobSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecWriteConnectionSecretToRef(obj: JobSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderHadoopConfig
 */
export interface JobSpecForProviderHadoopConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecForProviderHadoopConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema JobSpecForProviderHadoopConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecForProviderHadoopConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
   *
   * @schema JobSpecForProviderHadoopConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderHadoopConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderHadoopConfigLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
   *
   * @schema JobSpecForProviderHadoopConfig#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
   *
   * @schema JobSpecForProviderHadoopConfig#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
   *
   * @schema JobSpecForProviderHadoopConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderHadoopConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderHadoopConfig(obj: JobSpecForProviderHadoopConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderHadoopConfigLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderHiveConfig
 */
export interface JobSpecForProviderHiveConfig {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecForProviderHiveConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
   *
   * @schema JobSpecForProviderHiveConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
   *
   * @schema JobSpecForProviderHiveConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
   *
   * @schema JobSpecForProviderHiveConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderHiveConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
   *
   * @schema JobSpecForProviderHiveConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderHiveConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderHiveConfig(obj: JobSpecForProviderHiveConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPigConfig
 */
export interface JobSpecForProviderPigConfig {
  /**
   * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecForProviderPigConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
   *
   * @schema JobSpecForProviderPigConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderPigConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderPigConfigLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
   *
   * @schema JobSpecForProviderPigConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
   *
   * @schema JobSpecForProviderPigConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderPigConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
   *
   * @schema JobSpecForProviderPigConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPigConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPigConfig(obj: JobSpecForProviderPigConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderPigConfigLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPlacement
 */
export interface JobSpecForProviderPlacement {
  /**
   * The name of the cluster where the job will be submitted.
   *
   * @schema JobSpecForProviderPlacement#clusterName
   */
  readonly clusterName?: string;

  /**
   * Reference to a Cluster in dataproc to populate clusterName.
   *
   * @schema JobSpecForProviderPlacement#clusterNameRef
   */
  readonly clusterNameRef?: JobSpecForProviderPlacementClusterNameRef;

  /**
   * Selector for a Cluster in dataproc to populate clusterName.
   *
   * @schema JobSpecForProviderPlacement#clusterNameSelector
   */
  readonly clusterNameSelector?: JobSpecForProviderPlacementClusterNameSelector;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacement' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacement(obj: JobSpecForProviderPlacement | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterName': obj.clusterName,
    'clusterNameRef': toJson_JobSpecForProviderPlacementClusterNameRef(obj.clusterNameRef),
    'clusterNameSelector': toJson_JobSpecForProviderPlacementClusterNameSelector(obj.clusterNameSelector),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPrestoConfig
 */
export interface JobSpecForProviderPrestoConfig {
  /**
   * Presto client tags to attach to this query.
   *
   * @schema JobSpecForProviderPrestoConfig#clientTags
   */
  readonly clientTags?: string[];

  /**
   * Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
   *
   * @default false.
   * @schema JobSpecForProviderPrestoConfig#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * @schema JobSpecForProviderPrestoConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderPrestoConfigLoggingConfig[];

  /**
   * The format in which query output will be displayed. See the Presto documentation for supported output formats.
   *
   * @schema JobSpecForProviderPrestoConfig#outputFormat
   */
  readonly outputFormat?: string;

  /**
   * A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
   *
   * @schema JobSpecForProviderPrestoConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
   *
   * @schema JobSpecForProviderPrestoConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderPrestoConfig#queryList
   */
  readonly queryList?: string[];

}

/**
 * Converts an object of type 'JobSpecForProviderPrestoConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPrestoConfig(obj: JobSpecForProviderPrestoConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clientTags': obj.clientTags?.map(y => y),
    'continueOnFailure': obj.continueOnFailure,
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderPrestoConfigLoggingConfig(y)),
    'outputFormat': obj.outputFormat,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPysparkConfig
 */
export interface JobSpecForProviderPysparkConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecForProviderPysparkConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver.
   *
   * @schema JobSpecForProviderPysparkConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecForProviderPysparkConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
   *
   * @schema JobSpecForProviderPysparkConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderPysparkConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderPysparkConfigLoggingConfig[];

  /**
   * The HCFS URI of the main Python file to use as the driver. Must be a .py file.
   *
   * @schema JobSpecForProviderPysparkConfig#mainPythonFileUri
   */
  readonly mainPythonFileUri: string;

  /**
   * A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @schema JobSpecForProviderPysparkConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
   *
   * @schema JobSpecForProviderPysparkConfig#pythonFileUris
   */
  readonly pythonFileUris?: string[];

}

/**
 * Converts an object of type 'JobSpecForProviderPysparkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPysparkConfig(obj: JobSpecForProviderPysparkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderPysparkConfigLoggingConfig(y)),
    'mainPythonFileUri': obj.mainPythonFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pythonFileUris': obj.pythonFileUris?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderReference
 */
export interface JobSpecForProviderReference {
  /**
   * @schema JobSpecForProviderReference#jobId
   */
  readonly jobId?: string;

}

/**
 * Converts an object of type 'JobSpecForProviderReference' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderReference(obj: JobSpecForProviderReference | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jobId': obj.jobId,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a Cluster in dataproc to populate region.
 *
 * @schema JobSpecForProviderRegionRef
 */
export interface JobSpecForProviderRegionRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecForProviderRegionRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecForProviderRegionRef#policy
   */
  readonly policy?: JobSpecForProviderRegionRefPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionRef(obj: JobSpecForProviderRegionRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecForProviderRegionRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a Cluster in dataproc to populate region.
 *
 * @schema JobSpecForProviderRegionSelector
 */
export interface JobSpecForProviderRegionSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema JobSpecForProviderRegionSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema JobSpecForProviderRegionSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema JobSpecForProviderRegionSelector#policy
   */
  readonly policy?: JobSpecForProviderRegionSelectorPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionSelector(obj: JobSpecForProviderRegionSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_JobSpecForProviderRegionSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderScheduling
 */
export interface JobSpecForProviderScheduling {
  /**
   * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
   *
   * @schema JobSpecForProviderScheduling#maxFailuresPerHour
   */
  readonly maxFailuresPerHour: number;

  /**
   * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
   *
   * @schema JobSpecForProviderScheduling#maxFailuresTotal
   */
  readonly maxFailuresTotal: number;

}

/**
 * Converts an object of type 'JobSpecForProviderScheduling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderScheduling(obj: JobSpecForProviderScheduling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxFailuresPerHour': obj.maxFailuresPerHour,
    'maxFailuresTotal': obj.maxFailuresTotal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparkConfig
 */
export interface JobSpecForProviderSparkConfig {
  /**
   * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema JobSpecForProviderSparkConfig#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * The arguments to pass to the driver.
   *
   * @schema JobSpecForProviderSparkConfig#args
   */
  readonly args?: string[];

  /**
   * HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @schema JobSpecForProviderSparkConfig#fileUris
   */
  readonly fileUris?: string[];

  /**
   * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
   *
   * @schema JobSpecForProviderSparkConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderSparkConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderSparkConfigLoggingConfig[];

  /**
   * The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
   *
   * @schema JobSpecForProviderSparkConfig#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of jar file containing the driver jar. Conflicts with main_class
   *
   * @schema JobSpecForProviderSparkConfig#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @schema JobSpecForProviderSparkConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparkConfig(obj: JobSpecForProviderSparkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderSparkConfigLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparksqlConfig
 */
export interface JobSpecForProviderSparksqlConfig {
  /**
   * HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema JobSpecForProviderSparksqlConfig#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * @schema JobSpecForProviderSparksqlConfig#loggingConfig
   */
  readonly loggingConfig?: JobSpecForProviderSparksqlConfigLoggingConfig[];

  /**
   * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
   *
   * @schema JobSpecForProviderSparksqlConfig#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
   *
   * @schema JobSpecForProviderSparksqlConfig#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
   *
   * @schema JobSpecForProviderSparksqlConfig#queryList
   */
  readonly queryList?: string[];

  /**
   * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema JobSpecForProviderSparksqlConfig#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparksqlConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparksqlConfig(obj: JobSpecForProviderSparksqlConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_JobSpecForProviderSparksqlConfigLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => y),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecProviderConfigRefPolicy
 */
export interface JobSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: JobSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: JobSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecProviderConfigRefPolicy(obj: JobSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecProviderRefPolicy
 */
export interface JobSpecProviderRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecProviderRefPolicy#resolution
   */
  readonly resolution?: JobSpecProviderRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecProviderRefPolicy#resolve
   */
  readonly resolve?: JobSpecProviderRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecProviderRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecProviderRefPolicy(obj: JobSpecProviderRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRef
 */
export interface JobSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: JobSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsToConfigRef(obj: JobSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema JobSpecPublishConnectionDetailsToMetadata
 */
export interface JobSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema JobSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema JobSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema JobSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsToMetadata(obj: JobSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderHadoopConfigLoggingConfig
 */
export interface JobSpecForProviderHadoopConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderHadoopConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderHadoopConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderHadoopConfigLoggingConfig(obj: JobSpecForProviderHadoopConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPigConfigLoggingConfig
 */
export interface JobSpecForProviderPigConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderPigConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPigConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPigConfigLoggingConfig(obj: JobSpecForProviderPigConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a Cluster in dataproc to populate clusterName.
 *
 * @schema JobSpecForProviderPlacementClusterNameRef
 */
export interface JobSpecForProviderPlacementClusterNameRef {
  /**
   * Name of the referenced object.
   *
   * @schema JobSpecForProviderPlacementClusterNameRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema JobSpecForProviderPlacementClusterNameRef#policy
   */
  readonly policy?: JobSpecForProviderPlacementClusterNameRefPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameRef(obj: JobSpecForProviderPlacementClusterNameRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_JobSpecForProviderPlacementClusterNameRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a Cluster in dataproc to populate clusterName.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelector
 */
export interface JobSpecForProviderPlacementClusterNameSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelector#policy
   */
  readonly policy?: JobSpecForProviderPlacementClusterNameSelectorPolicy;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameSelector(obj: JobSpecForProviderPlacementClusterNameSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_JobSpecForProviderPlacementClusterNameSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPrestoConfigLoggingConfig
 */
export interface JobSpecForProviderPrestoConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderPrestoConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPrestoConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPrestoConfigLoggingConfig(obj: JobSpecForProviderPrestoConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderPysparkConfigLoggingConfig
 */
export interface JobSpecForProviderPysparkConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderPysparkConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderPysparkConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPysparkConfigLoggingConfig(obj: JobSpecForProviderPysparkConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecForProviderRegionRefPolicy
 */
export interface JobSpecForProviderRegionRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderRegionRefPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderRegionRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderRegionRefPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderRegionRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionRefPolicy(obj: JobSpecForProviderRegionRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema JobSpecForProviderRegionSelectorPolicy
 */
export interface JobSpecForProviderRegionSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderRegionSelectorPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderRegionSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderRegionSelectorPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderRegionSelectorPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderRegionSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderRegionSelectorPolicy(obj: JobSpecForProviderRegionSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparkConfigLoggingConfig
 */
export interface JobSpecForProviderSparkConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderSparkConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparkConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparkConfigLoggingConfig(obj: JobSpecForProviderSparkConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema JobSpecForProviderSparksqlConfigLoggingConfig
 */
export interface JobSpecForProviderSparksqlConfigLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema JobSpecForProviderSparksqlConfigLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels: { [key: string]: string };

}

/**
 * Converts an object of type 'JobSpecForProviderSparksqlConfigLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderSparksqlConfigLoggingConfig(obj: JobSpecForProviderSparksqlConfigLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecProviderConfigRefPolicyResolution
 */
export enum JobSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecProviderConfigRefPolicyResolve
 */
export enum JobSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecProviderRefPolicyResolution
 */
export enum JobSpecProviderRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecProviderRefPolicyResolve
 */
export enum JobSpecProviderRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface JobSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: JobSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: JobSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecPublishConnectionDetailsToConfigRefPolicy(obj: JobSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema JobSpecForProviderPlacementClusterNameRefPolicy
 */
export interface JobSpecForProviderPlacementClusterNameRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderPlacementClusterNameRefPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderPlacementClusterNameRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderPlacementClusterNameRefPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderPlacementClusterNameRefPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameRefPolicy(obj: JobSpecForProviderPlacementClusterNameRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelectorPolicy
 */
export interface JobSpecForProviderPlacementClusterNameSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelectorPolicy#resolution
   */
  readonly resolution?: JobSpecForProviderPlacementClusterNameSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema JobSpecForProviderPlacementClusterNameSelectorPolicy#resolve
   */
  readonly resolve?: JobSpecForProviderPlacementClusterNameSelectorPolicyResolve;

}

/**
 * Converts an object of type 'JobSpecForProviderPlacementClusterNameSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_JobSpecForProviderPlacementClusterNameSelectorPolicy(obj: JobSpecForProviderPlacementClusterNameSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderRegionRefPolicyResolution
 */
export enum JobSpecForProviderRegionRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderRegionRefPolicyResolve
 */
export enum JobSpecForProviderRegionRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderRegionSelectorPolicyResolution
 */
export enum JobSpecForProviderRegionSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderRegionSelectorPolicyResolve
 */
export enum JobSpecForProviderRegionSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum JobSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum JobSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderPlacementClusterNameRefPolicyResolution
 */
export enum JobSpecForProviderPlacementClusterNameRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderPlacementClusterNameRefPolicyResolve
 */
export enum JobSpecForProviderPlacementClusterNameRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelectorPolicyResolution
 */
export enum JobSpecForProviderPlacementClusterNameSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema JobSpecForProviderPlacementClusterNameSelectorPolicyResolve
 */
export enum JobSpecForProviderPlacementClusterNameSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}


/**
 * MetastoreService is the Schema for the MetastoreServices API. A managed metastore service that serves metadata queries.
 *
 * @schema MetastoreService
 */
export class MetastoreService extends ApiObject {
  /**
   * Returns the apiVersion and kind for "MetastoreService"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'MetastoreService',
  }

  /**
   * Renders a Kubernetes manifest for "MetastoreService".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: MetastoreServiceProps): any {
    return {
      ...MetastoreService.GVK,
      ...toJson_MetastoreServiceProps(props),
    };
  }

  /**
   * Defines a "MetastoreService" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: MetastoreServiceProps) {
    super(scope, id, {
      ...MetastoreService.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...MetastoreService.GVK,
      ...toJson_MetastoreServiceProps(resolved),
    };
  }
}

/**
 * MetastoreService is the Schema for the MetastoreServices API. A managed metastore service that serves metadata queries.
 *
 * @schema MetastoreService
 */
export interface MetastoreServiceProps {
  /**
   * @schema MetastoreService#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * MetastoreServiceSpec defines the desired state of MetastoreService
   *
   * @schema MetastoreService#spec
   */
  readonly spec: MetastoreServiceSpec;

}

/**
 * Converts an object of type 'MetastoreServiceProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceProps(obj: MetastoreServiceProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_MetastoreServiceSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * MetastoreServiceSpec defines the desired state of MetastoreService
 *
 * @schema MetastoreServiceSpec
 */
export interface MetastoreServiceSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
   *
   * @schema MetastoreServiceSpec#deletionPolicy
   */
  readonly deletionPolicy?: MetastoreServiceSpecDeletionPolicy;

  /**
   * @schema MetastoreServiceSpec#forProvider
   */
  readonly forProvider: MetastoreServiceSpecForProvider;

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema MetastoreServiceSpec#providerConfigRef
   */
  readonly providerConfigRef?: MetastoreServiceSpecProviderConfigRef;

  /**
   * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
   *
   * @schema MetastoreServiceSpec#providerRef
   */
  readonly providerRef?: MetastoreServiceSpecProviderRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema MetastoreServiceSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: MetastoreServiceSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema MetastoreServiceSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: MetastoreServiceSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'MetastoreServiceSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpec(obj: MetastoreServiceSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_MetastoreServiceSpecForProvider(obj.forProvider),
    'providerConfigRef': toJson_MetastoreServiceSpecProviderConfigRef(obj.providerConfigRef),
    'providerRef': toJson_MetastoreServiceSpecProviderRef(obj.providerRef),
    'publishConnectionDetailsTo': toJson_MetastoreServiceSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_MetastoreServiceSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
 *
 * @schema MetastoreServiceSpecDeletionPolicy
 */
export enum MetastoreServiceSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema MetastoreServiceSpecForProvider
 */
export interface MetastoreServiceSpecForProvider {
  /**
   * The database type that the Metastore service stores its data. Default value is MYSQL. Possible values are MYSQL and SPANNER.
   *
   * @schema MetastoreServiceSpecForProvider#databaseType
   */
  readonly databaseType?: string;

  /**
   * Information used to configure the Dataproc Metastore service to encrypt customer data at rest. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#encryptionConfig
   */
  readonly encryptionConfig?: MetastoreServiceSpecForProviderEncryptionConfig[];

  /**
   * Configuration information specific to running Hive metastore software as the metastore service. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#hiveMetastoreConfig
   */
  readonly hiveMetastoreConfig?: MetastoreServiceSpecForProviderHiveMetastoreConfig[];

  /**
   * User-defined labels for the metastore service.
   *
   * @schema MetastoreServiceSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The location where the metastore service should reside. The default value is global.
   *
   * @schema MetastoreServiceSpecForProvider#location
   */
  readonly location?: string;

  /**
   * The one hour maintenance window of the metastore service. This specifies when the service can be restarted for maintenance purposes in UTC time. Maintenance window is not needed for services with the SPANNER database type. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#maintenanceWindow
   */
  readonly maintenanceWindow?: MetastoreServiceSpecForProviderMaintenanceWindow[];

  /**
   * The relative resource name of the VPC network on which the instance can be accessed. It is specified in the following form: "projects/{projectNumber}/global/networks/{network_id}".
   *
   * @schema MetastoreServiceSpecForProvider#network
   */
  readonly network?: string;

  /**
   * The configuration specifying the network settings for the Dataproc Metastore service. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#networkConfig
   */
  readonly networkConfig?: MetastoreServiceSpecForProviderNetworkConfig[];

  /**
   * The TCP port at which the metastore service is reached. Default: 9083.
   *
   * @schema MetastoreServiceSpecForProvider#port
   */
  readonly port?: number;

  /**
   * The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
   *
   * @schema MetastoreServiceSpecForProvider#project
   */
  readonly project?: string;

  /**
   * The release channel of the service. If unspecified, defaults to STABLE. Default value is STABLE. Possible values are CANARY and STABLE.
   *
   * @schema MetastoreServiceSpecForProvider#releaseChannel
   */
  readonly releaseChannel?: string;

  /**
   * The configuration specifying telemetry settings for the Dataproc Metastore service. If unspecified defaults to JSON. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProvider#telemetryConfig
   */
  readonly telemetryConfig?: MetastoreServiceSpecForProviderTelemetryConfig[];

  /**
   * The tier of the service. Possible values are DEVELOPER and ENTERPRISE.
   *
   * @schema MetastoreServiceSpecForProvider#tier
   */
  readonly tier?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProvider(obj: MetastoreServiceSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'databaseType': obj.databaseType,
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_MetastoreServiceSpecForProviderEncryptionConfig(y)),
    'hiveMetastoreConfig': obj.hiveMetastoreConfig?.map(y => toJson_MetastoreServiceSpecForProviderHiveMetastoreConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'location': obj.location,
    'maintenanceWindow': obj.maintenanceWindow?.map(y => toJson_MetastoreServiceSpecForProviderMaintenanceWindow(y)),
    'network': obj.network,
    'networkConfig': obj.networkConfig?.map(y => toJson_MetastoreServiceSpecForProviderNetworkConfig(y)),
    'port': obj.port,
    'project': obj.project,
    'releaseChannel': obj.releaseChannel,
    'telemetryConfig': obj.telemetryConfig?.map(y => toJson_MetastoreServiceSpecForProviderTelemetryConfig(y)),
    'tier': obj.tier,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema MetastoreServiceSpecProviderConfigRef
 */
export interface MetastoreServiceSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecProviderConfigRef#policy
   */
  readonly policy?: MetastoreServiceSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecProviderConfigRef(obj: MetastoreServiceSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
 *
 * @schema MetastoreServiceSpecProviderRef
 */
export interface MetastoreServiceSpecProviderRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecProviderRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecProviderRef#policy
   */
  readonly policy?: MetastoreServiceSpecProviderRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecProviderRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecProviderRef(obj: MetastoreServiceSpecProviderRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecProviderRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsTo
 */
export interface MetastoreServiceSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: MetastoreServiceSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: MetastoreServiceSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsTo(obj: MetastoreServiceSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_MetastoreServiceSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema MetastoreServiceSpecWriteConnectionSecretToRef
 */
export interface MetastoreServiceSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema MetastoreServiceSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema MetastoreServiceSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecWriteConnectionSecretToRef(obj: MetastoreServiceSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderEncryptionConfig
 */
export interface MetastoreServiceSpecForProviderEncryptionConfig {
  /**
   * The fully qualified customer provided Cloud KMS key name to use for customer data encryption. Use the following format: projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfig#kmsKey
   */
  readonly kmsKey?: string;

  /**
   * Reference to a CryptoKey in kms to populate kmsKey.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfig#kmsKeyRef
   */
  readonly kmsKeyRef?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef;

  /**
   * Selector for a CryptoKey in kms to populate kmsKey.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfig#kmsKeySelector
   */
  readonly kmsKeySelector?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfig(obj: MetastoreServiceSpecForProviderEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kmsKey': obj.kmsKey,
    'kmsKeyRef': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef(obj.kmsKeyRef),
    'kmsKeySelector': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector(obj.kmsKeySelector),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig
 */
export interface MetastoreServiceSpecForProviderHiveMetastoreConfig {
  /**
   * A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml). The mappings override system defaults (some keys cannot be overridden)
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig#configOverrides
   */
  readonly configOverrides?: { [key: string]: string };

  /**
   * Information used to configure the Hive metastore service as a service principal in a Kerberos realm. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig#kerberosConfig
   */
  readonly kerberosConfig?: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig[];

  /**
   * The Hive metastore schema version.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfig#version
   */
  readonly version: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderHiveMetastoreConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderHiveMetastoreConfig(obj: MetastoreServiceSpecForProviderHiveMetastoreConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configOverrides': ((obj.configOverrides) === undefined) ? undefined : (Object.entries(obj.configOverrides).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig(y)),
    'version': obj.version,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderMaintenanceWindow
 */
export interface MetastoreServiceSpecForProviderMaintenanceWindow {
  /**
   * The day of week, when the window starts. Possible values are MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, and SUNDAY.
   *
   * @schema MetastoreServiceSpecForProviderMaintenanceWindow#dayOfWeek
   */
  readonly dayOfWeek: string;

  /**
   * The hour of day (0-23) when the window starts.
   *
   * @schema MetastoreServiceSpecForProviderMaintenanceWindow#hourOfDay
   */
  readonly hourOfDay: number;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderMaintenanceWindow' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderMaintenanceWindow(obj: MetastoreServiceSpecForProviderMaintenanceWindow | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dayOfWeek': obj.dayOfWeek,
    'hourOfDay': obj.hourOfDay,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderNetworkConfig
 */
export interface MetastoreServiceSpecForProviderNetworkConfig {
  /**
   * The consumer-side network configuration for the Dataproc Metastore instance. Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfig#consumers
   */
  readonly consumers: MetastoreServiceSpecForProviderNetworkConfigConsumers[];

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfig(obj: MetastoreServiceSpecForProviderNetworkConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumers': obj.consumers?.map(y => toJson_MetastoreServiceSpecForProviderNetworkConfigConsumers(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderTelemetryConfig
 */
export interface MetastoreServiceSpecForProviderTelemetryConfig {
  /**
   * The output format of the Dataproc Metastore service's logs. Default value is JSON. Possible values are LEGACY and JSON.
   *
   * @schema MetastoreServiceSpecForProviderTelemetryConfig#logFormat
   */
  readonly logFormat?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderTelemetryConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderTelemetryConfig(obj: MetastoreServiceSpecForProviderTelemetryConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'logFormat': obj.logFormat,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecProviderConfigRefPolicy
 */
export interface MetastoreServiceSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecProviderConfigRefPolicy(obj: MetastoreServiceSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecProviderRefPolicy
 */
export interface MetastoreServiceSpecProviderRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecProviderRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecProviderRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecProviderRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecProviderRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecProviderRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecProviderRefPolicy(obj: MetastoreServiceSpecProviderRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRef
 */
export interface MetastoreServiceSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRef(obj: MetastoreServiceSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata
 */
export interface MetastoreServiceSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsToMetadata(obj: MetastoreServiceSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a CryptoKey in kms to populate kmsKey.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a CryptoKey in kms to populate kmsKey.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig
 */
export interface MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig {
  /**
   * A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC). Structure is documented below.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig#keytab
   */
  readonly keytab: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab[];

  /**
   * A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig#krb5ConfigGcsUri
   */
  readonly krb5ConfigGcsUri: string;

  /**
   * A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig#principal
   */
  readonly principal: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig(obj: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'keytab': obj.keytab?.map(y => toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab(y)),
    'krb5ConfigGcsUri': obj.krb5ConfigGcsUri,
    'principal': obj.principal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumers {
  /**
   * The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint. It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network. There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form: `projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * Reference to a Subnetwork in compute to populate subnetwork.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers#subnetworkRef
   */
  readonly subnetworkRef?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef;

  /**
   * Selector for a Subnetwork in compute to populate subnetwork.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumers#subnetworkSelector
   */
  readonly subnetworkSelector?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumers' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumers(obj: MetastoreServiceSpecForProviderNetworkConfigConsumers | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'subnetwork': obj.subnetwork,
    'subnetworkRef': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef(obj.subnetworkRef),
    'subnetworkSelector': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector(obj.subnetworkSelector),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecProviderConfigRefPolicyResolution
 */
export enum MetastoreServiceSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecProviderConfigRefPolicyResolve
 */
export enum MetastoreServiceSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecProviderRefPolicyResolution
 */
export enum MetastoreServiceSpecProviderRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecProviderRefPolicyResolve
 */
export enum MetastoreServiceSpecProviderRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy(obj: MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy
 */
export interface MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy(obj: MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab
 */
export interface MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab {
  /**
   * The relative resource name of a Secret Manager secret version, in the following form: "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
   *
   * @schema MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab#cloudSecret
   */
  readonly cloudSecret: string;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab(obj: MetastoreServiceSpecForProviderHiveMetastoreConfigKerberosConfigKeytab | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'cloudSecret': obj.cloudSecret,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Reference to a Subnetwork in compute to populate subnetwork.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef {
  /**
   * Name of the referenced object.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Selector for a Subnetwork in compute to populate subnetwork.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector {
  /**
   * MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector#matchControllerRef
   */
  readonly matchControllerRef?: boolean;

  /**
   * MatchLabels ensures an object with matching labels is selected.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector#matchLabels
   */
  readonly matchLabels?: { [key: string]: string };

  /**
   * Policies for selection.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector#policy
   */
  readonly policy?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'matchControllerRef': obj.matchControllerRef,
    'matchLabels': ((obj.matchLabels) === undefined) ? undefined : (Object.entries(obj.matchLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'policy': toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum MetastoreServiceSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolution
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolve
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeyRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolution
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolve
 */
export enum MetastoreServiceSpecForProviderEncryptionConfigKmsKeySelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for selection.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy
 */
export interface MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy#resolution
   */
  readonly resolution?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy#resolve
   */
  readonly resolve?: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolve;

}

/**
 * Converts an object of type 'MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy(obj: MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolution
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolve
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolution
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolve
 */
export enum MetastoreServiceSpecForProviderNetworkConfigConsumersSubnetworkSelectorPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}


/**
 * WorkflowTemplate is the Schema for the WorkflowTemplates API. A Workflow Template is a reusable workflow configuration.
 *
 * @schema WorkflowTemplate
 */
export class WorkflowTemplate extends ApiObject {
  /**
   * Returns the apiVersion and kind for "WorkflowTemplate"
   */
  public static readonly GVK: GroupVersionKind = {
    apiVersion: 'dataproc.gcp.upbound.io/v1beta1',
    kind: 'WorkflowTemplate',
  }

  /**
   * Renders a Kubernetes manifest for "WorkflowTemplate".
   *
   * This can be used to inline resource manifests inside other objects (e.g. as templates).
   *
   * @param props initialization props
   */
  public static manifest(props: WorkflowTemplateProps): any {
    return {
      ...WorkflowTemplate.GVK,
      ...toJson_WorkflowTemplateProps(props),
    };
  }

  /**
   * Defines a "WorkflowTemplate" API object
   * @param scope the scope in which to define this object
   * @param id a scope-local name for the object
   * @param props initialization props
   */
  public constructor(scope: Construct, id: string, props: WorkflowTemplateProps) {
    super(scope, id, {
      ...WorkflowTemplate.GVK,
      ...props,
    });
  }

  /**
   * Renders the object to Kubernetes JSON.
   */
  public toJson(): any {
    const resolved = super.toJson();

    return {
      ...WorkflowTemplate.GVK,
      ...toJson_WorkflowTemplateProps(resolved),
    };
  }
}

/**
 * WorkflowTemplate is the Schema for the WorkflowTemplates API. A Workflow Template is a reusable workflow configuration.
 *
 * @schema WorkflowTemplate
 */
export interface WorkflowTemplateProps {
  /**
   * @schema WorkflowTemplate#metadata
   */
  readonly metadata?: ApiObjectMetadata;

  /**
   * WorkflowTemplateSpec defines the desired state of WorkflowTemplate
   *
   * @schema WorkflowTemplate#spec
   */
  readonly spec: WorkflowTemplateSpec;

}

/**
 * Converts an object of type 'WorkflowTemplateProps' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateProps(obj: WorkflowTemplateProps | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'metadata': obj.metadata,
    'spec': toJson_WorkflowTemplateSpec(obj.spec),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WorkflowTemplateSpec defines the desired state of WorkflowTemplate
 *
 * @schema WorkflowTemplateSpec
 */
export interface WorkflowTemplateSpec {
  /**
   * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
   *
   * @schema WorkflowTemplateSpec#deletionPolicy
   */
  readonly deletionPolicy?: WorkflowTemplateSpecDeletionPolicy;

  /**
   * @schema WorkflowTemplateSpec#forProvider
   */
  readonly forProvider: WorkflowTemplateSpecForProvider;

  /**
   * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
   *
   * @schema WorkflowTemplateSpec#providerConfigRef
   */
  readonly providerConfigRef?: WorkflowTemplateSpecProviderConfigRef;

  /**
   * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
   *
   * @schema WorkflowTemplateSpec#providerRef
   */
  readonly providerRef?: WorkflowTemplateSpecProviderRef;

  /**
   * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
   *
   * @schema WorkflowTemplateSpec#publishConnectionDetailsTo
   */
  readonly publishConnectionDetailsTo?: WorkflowTemplateSpecPublishConnectionDetailsTo;

  /**
   * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
   *
   * @schema WorkflowTemplateSpec#writeConnectionSecretToRef
   */
  readonly writeConnectionSecretToRef?: WorkflowTemplateSpecWriteConnectionSecretToRef;

}

/**
 * Converts an object of type 'WorkflowTemplateSpec' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpec(obj: WorkflowTemplateSpec | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'deletionPolicy': obj.deletionPolicy,
    'forProvider': toJson_WorkflowTemplateSpecForProvider(obj.forProvider),
    'providerConfigRef': toJson_WorkflowTemplateSpecProviderConfigRef(obj.providerConfigRef),
    'providerRef': toJson_WorkflowTemplateSpecProviderRef(obj.providerRef),
    'publishConnectionDetailsTo': toJson_WorkflowTemplateSpecPublishConnectionDetailsTo(obj.publishConnectionDetailsTo),
    'writeConnectionSecretToRef': toJson_WorkflowTemplateSpecWriteConnectionSecretToRef(obj.writeConnectionSecretToRef),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either "Delete" or "Orphan" the external resource.
 *
 * @schema WorkflowTemplateSpecDeletionPolicy
 */
export enum WorkflowTemplateSpecDeletionPolicy {
  /** Orphan */
  ORPHAN = "Orphan",
  /** Delete */
  DELETE = "Delete",
}

/**
 * @schema WorkflowTemplateSpecForProvider
 */
export interface WorkflowTemplateSpecForProvider {
  /**
   * (Beta only) Optional. Timeout duration for the DAG of jobs. You can use "s", "m", "h", and "d" suffixes for second, minute, hour, and day duration values, respectively. The timeout duration must be from 10 minutes ("10m") to 24 hours ("24h" or "1d"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a (/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.
   *
   * @schema WorkflowTemplateSpecForProvider#dagTimeout
   */
  readonly dagTimeout?: string;

  /**
   * Required. The Directed Acyclic Graph of Jobs to submit.
   *
   * @schema WorkflowTemplateSpecForProvider#jobs
   */
  readonly jobs: WorkflowTemplateSpecForProviderJobs[];

  /**
   * Optional. The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
   *
   * @schema WorkflowTemplateSpecForProvider#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * The location for the resource
   *
   * @schema WorkflowTemplateSpecForProvider#location
   */
  readonly location: string;

  /**
   * Optional. Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.
   *
   * @schema WorkflowTemplateSpecForProvider#parameters
   */
  readonly parameters?: WorkflowTemplateSpecForProviderParameters[];

  /**
   * Required. WorkflowTemplate scheduling information.
   *
   * @schema WorkflowTemplateSpecForProvider#placement
   */
  readonly placement: WorkflowTemplateSpecForProviderPlacement[];

  /**
   * The project for the resource
   *
   * @schema WorkflowTemplateSpecForProvider#project
   */
  readonly project?: string;

  /**
   * Optional. Used to perform a consistent read-modify-write. This field should be left blank for a CreateWorkflowTemplate request. It is required for an UpdateWorkflowTemplate request, and must match the current server version. A typical update template flow would fetch the current template with a GetWorkflowTemplate request, which will return the current template with the version field filled in with the current server version. The user updates other fields in the template, then returns it as part of the UpdateWorkflowTemplate request.
   *
   * @schema WorkflowTemplateSpecForProvider#version
   */
  readonly version?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProvider' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProvider(obj: WorkflowTemplateSpecForProvider | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'dagTimeout': obj.dagTimeout,
    'jobs': obj.jobs?.map(y => toJson_WorkflowTemplateSpecForProviderJobs(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'location': obj.location,
    'parameters': obj.parameters?.map(y => toJson_WorkflowTemplateSpecForProviderParameters(y)),
    'placement': obj.placement?.map(y => toJson_WorkflowTemplateSpecForProviderPlacement(y)),
    'project': obj.project,
    'version': obj.version,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.
 *
 * @schema WorkflowTemplateSpecProviderConfigRef
 */
export interface WorkflowTemplateSpecProviderConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema WorkflowTemplateSpecProviderConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema WorkflowTemplateSpecProviderConfigRef#policy
   */
  readonly policy?: WorkflowTemplateSpecProviderConfigRefPolicy;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecProviderConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecProviderConfigRef(obj: WorkflowTemplateSpecProviderConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_WorkflowTemplateSpecProviderConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`
 *
 * @schema WorkflowTemplateSpecProviderRef
 */
export interface WorkflowTemplateSpecProviderRef {
  /**
   * Name of the referenced object.
   *
   * @schema WorkflowTemplateSpecProviderRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema WorkflowTemplateSpecProviderRef#policy
   */
  readonly policy?: WorkflowTemplateSpecProviderRefPolicy;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecProviderRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecProviderRef(obj: WorkflowTemplateSpecProviderRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_WorkflowTemplateSpecProviderRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsTo
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsTo {
  /**
   * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsTo#configRef
   */
  readonly configRef?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRef;

  /**
   * Metadata is the metadata for connection secret.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsTo#metadata
   */
  readonly metadata?: WorkflowTemplateSpecPublishConnectionDetailsToMetadata;

  /**
   * Name is the name of the connection secret.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsTo#name
   */
  readonly name: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsTo' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsTo(obj: WorkflowTemplateSpecPublishConnectionDetailsTo | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'configRef': toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRef(obj.configRef),
    'metadata': toJson_WorkflowTemplateSpecPublishConnectionDetailsToMetadata(obj.metadata),
    'name': obj.name,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.
 *
 * @schema WorkflowTemplateSpecWriteConnectionSecretToRef
 */
export interface WorkflowTemplateSpecWriteConnectionSecretToRef {
  /**
   * Name of the secret.
   *
   * @schema WorkflowTemplateSpecWriteConnectionSecretToRef#name
   */
  readonly name: string;

  /**
   * Namespace of the secret.
   *
   * @schema WorkflowTemplateSpecWriteConnectionSecretToRef#namespace
   */
  readonly namespace: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecWriteConnectionSecretToRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecWriteConnectionSecretToRef(obj: WorkflowTemplateSpecWriteConnectionSecretToRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'namespace': obj.namespace,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobs
 */
export interface WorkflowTemplateSpecForProviderJobs {
  /**
   * Optional. Job is a Hadoop job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#hadoopJob
   */
  readonly hadoopJob?: WorkflowTemplateSpecForProviderJobsHadoopJob[];

  /**
   * Optional. Job is a Hive job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#hiveJob
   */
  readonly hiveJob?: WorkflowTemplateSpecForProviderJobsHiveJob[];

  /**
   * Optional. The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Optional. Job is a Pig job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#pigJob
   */
  readonly pigJob?: WorkflowTemplateSpecForProviderJobsPigJob[];

  /**
   * Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#prerequisiteStepIds
   */
  readonly prerequisiteStepIds?: string[];

  /**
   * Optional. Job is a Presto job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#prestoJob
   */
  readonly prestoJob?: WorkflowTemplateSpecForProviderJobsPrestoJob[];

  /**
   * Optional. Job is a PySpark job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#pysparkJob
   */
  readonly pysparkJob?: WorkflowTemplateSpecForProviderJobsPysparkJob[];

  /**
   * Optional. Job scheduling configuration.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#scheduling
   */
  readonly scheduling?: WorkflowTemplateSpecForProviderJobsScheduling[];

  /**
   * Optional. Job is a Spark job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#sparkJob
   */
  readonly sparkJob?: WorkflowTemplateSpecForProviderJobsSparkJob[];

  /**
   * Optional. Job is a SparkR job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#sparkRJob
   */
  readonly sparkRJob?: WorkflowTemplateSpecForProviderJobsSparkRJob[];

  /**
   * Optional. Job is a SparkSql job.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#sparkSqlJob
   */
  readonly sparkSqlJob?: WorkflowTemplateSpecForProviderJobsSparkSqlJob[];

  /**
   * Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
   *
   * @schema WorkflowTemplateSpecForProviderJobs#stepId
   */
  readonly stepId: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobs' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobs(obj: WorkflowTemplateSpecForProviderJobs | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'hadoopJob': obj.hadoopJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHadoopJob(y)),
    'hiveJob': obj.hiveJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHiveJob(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pigJob': obj.pigJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPigJob(y)),
    'prerequisiteStepIds': obj.prerequisiteStepIds?.map(y => y),
    'prestoJob': obj.prestoJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPrestoJob(y)),
    'pysparkJob': obj.pysparkJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPysparkJob(y)),
    'scheduling': obj.scheduling?.map(y => toJson_WorkflowTemplateSpecForProviderJobsScheduling(y)),
    'sparkJob': obj.sparkJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkJob(y)),
    'sparkRJob': obj.sparkRJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkRJob(y)),
    'sparkSqlJob': obj.sparkSqlJob?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJob(y)),
    'stepId': obj.stepId,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParameters
 */
export interface WorkflowTemplateSpecForProviderParameters {
  /**
   * Optional. Brief description of the parameter. Must not exceed 1024 characters.
   *
   * @schema WorkflowTemplateSpecForProviderParameters#description
   */
  readonly description?: string;

  /**
   * Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
   *
   * @schema WorkflowTemplateSpecForProviderParameters#fields
   */
  readonly fields: string[];

  /**
   * Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
   *
   * @schema WorkflowTemplateSpecForProviderParameters#name
   */
  readonly name: string;

  /**
   * Optional. Validation rules to be applied to this parameter's value.
   *
   * @schema WorkflowTemplateSpecForProviderParameters#validation
   */
  readonly validation?: WorkflowTemplateSpecForProviderParametersValidation[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParameters' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParameters(obj: WorkflowTemplateSpecForProviderParameters | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'description': obj.description,
    'fields': obj.fields?.map(y => y),
    'name': obj.name,
    'validation': obj.validation?.map(y => toJson_WorkflowTemplateSpecForProviderParametersValidation(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacement
 */
export interface WorkflowTemplateSpecForProviderPlacement {
  /**
   * Optional. A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
   *
   * @schema WorkflowTemplateSpecForProviderPlacement#clusterSelector
   */
  readonly clusterSelector?: WorkflowTemplateSpecForProviderPlacementClusterSelector[];

  /**
   * A cluster that is managed by the workflow.
   *
   * @schema WorkflowTemplateSpecForProviderPlacement#managedCluster
   */
  readonly managedCluster?: WorkflowTemplateSpecForProviderPlacementManagedCluster[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacement' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacement(obj: WorkflowTemplateSpecForProviderPlacement | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterSelector': obj.clusterSelector?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementClusterSelector(y)),
    'managedCluster': obj.managedCluster?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedCluster(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema WorkflowTemplateSpecProviderConfigRefPolicy
 */
export interface WorkflowTemplateSpecProviderConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema WorkflowTemplateSpecProviderConfigRefPolicy#resolution
   */
  readonly resolution?: WorkflowTemplateSpecProviderConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema WorkflowTemplateSpecProviderConfigRefPolicy#resolve
   */
  readonly resolve?: WorkflowTemplateSpecProviderConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecProviderConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecProviderConfigRefPolicy(obj: WorkflowTemplateSpecProviderConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Policies for referencing.
 *
 * @schema WorkflowTemplateSpecProviderRefPolicy
 */
export interface WorkflowTemplateSpecProviderRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema WorkflowTemplateSpecProviderRefPolicy#resolution
   */
  readonly resolution?: WorkflowTemplateSpecProviderRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema WorkflowTemplateSpecProviderRefPolicy#resolve
   */
  readonly resolve?: WorkflowTemplateSpecProviderRefPolicyResolve;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecProviderRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecProviderRefPolicy(obj: WorkflowTemplateSpecProviderRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRef
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsToConfigRef {
  /**
   * Name of the referenced object.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRef#name
   */
  readonly name: string;

  /**
   * Policies for referencing.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRef#policy
   */
  readonly policy?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsToConfigRef' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRef(obj: WorkflowTemplateSpecPublishConnectionDetailsToConfigRef | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'name': obj.name,
    'policy': toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy(obj.policy),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Metadata is the metadata for connection secret.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsToMetadata {
  /**
   * Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.annotations". - It is up to Secret Store implementation for others store types.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata#annotations
   */
  readonly annotations?: { [key: string]: string };

  /**
   * Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as "metadata.labels". - It is up to Secret Store implementation for others store types.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata#labels
   */
  readonly labels?: { [key: string]: string };

  /**
   * Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToMetadata#type
   */
  readonly type?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsToMetadata' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsToMetadata(obj: WorkflowTemplateSpecPublishConnectionDetailsToMetadata | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'annotations': ((obj.annotations) === undefined) ? undefined : (Object.entries(obj.annotations).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'type': obj.type,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHadoopJob
 */
export interface WorkflowTemplateSpecForProviderJobsHadoopJob {
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#args
   */
  readonly args?: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file that contains the main class.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHadoopJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHadoopJob(obj: WorkflowTemplateSpecForProviderJobsHadoopJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHiveJob
 */
export interface WorkflowTemplateSpecForProviderJobsHiveJob {
  /**
   * Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsHiveJobQueryList[];

  /**
   * Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHiveJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHiveJob(obj: WorkflowTemplateSpecForProviderJobsHiveJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsHiveJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPigJob
 */
export interface WorkflowTemplateSpecForProviderJobsPigJob {
  /**
   * Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig[];

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsPigJobQueryList[];

  /**
   * Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPigJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPigJob(obj: WorkflowTemplateSpecForProviderJobsPigJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'continueOnFailure': obj.continueOnFailure,
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPigJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPrestoJob
 */
export interface WorkflowTemplateSpecForProviderJobsPrestoJob {
  /**
   * Optional. Presto client tags to attach to this query
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#clientTags
   */
  readonly clientTags?: string[];

  /**
   * Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#continueOnFailure
   */
  readonly continueOnFailure?: boolean;

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig[];

  /**
   * Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#outputFormat
   */
  readonly outputFormat?: string;

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsPrestoJobQueryList[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPrestoJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPrestoJob(obj: WorkflowTemplateSpecForProviderJobsPrestoJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clientTags': obj.clientTags?.map(y => y),
    'continueOnFailure': obj.continueOnFailure,
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig(y)),
    'outputFormat': obj.outputFormat,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPrestoJobQueryList(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPysparkJob
 */
export interface WorkflowTemplateSpecForProviderJobsPysparkJob {
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#args
   */
  readonly args?: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig[];

  /**
   * Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#mainPythonFileUri
   */
  readonly mainPythonFileUri: string;

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJob#pythonFileUris
   */
  readonly pythonFileUris?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPysparkJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPysparkJob(obj: WorkflowTemplateSpecForProviderJobsPysparkJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig(y)),
    'mainPythonFileUri': obj.mainPythonFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'pythonFileUris': obj.pythonFileUris?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsScheduling
 */
export interface WorkflowTemplateSpecForProviderJobsScheduling {
  /**
   * Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
   *
   * @schema WorkflowTemplateSpecForProviderJobsScheduling#maxFailuresPerHour
   */
  readonly maxFailuresPerHour?: number;

  /**
   * Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
   *
   * @schema WorkflowTemplateSpecForProviderJobsScheduling#maxFailuresTotal
   */
  readonly maxFailuresTotal?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsScheduling' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsScheduling(obj: WorkflowTemplateSpecForProviderJobsScheduling | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'maxFailuresPerHour': obj.maxFailuresPerHour,
    'maxFailuresTotal': obj.maxFailuresTotal,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkJob
 */
export interface WorkflowTemplateSpecForProviderJobsSparkJob {
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#args
   */
  readonly args?: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig[];

  /**
   * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#mainClass
   */
  readonly mainClass?: string;

  /**
   * The HCFS URI of the jar file that contains the main class.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#mainJarFileUri
   */
  readonly mainJarFileUri?: string;

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkJob(obj: WorkflowTemplateSpecForProviderJobsSparkJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig(y)),
    'mainClass': obj.mainClass,
    'mainJarFileUri': obj.mainJarFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkRJob
 */
export interface WorkflowTemplateSpecForProviderJobsSparkRJob {
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#archiveUris
   */
  readonly archiveUris?: string[];

  /**
   * Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#args
   */
  readonly args?: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#fileUris
   */
  readonly fileUris?: string[];

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig[];

  /**
   * Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#mainRFileUri
   */
  readonly mainRFileUri: string;

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJob#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkRJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkRJob(obj: WorkflowTemplateSpecForProviderJobsSparkRJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'archiveUris': obj.archiveUris?.map(y => y),
    'args': obj.args?.map(y => y),
    'fileUris': obj.fileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig(y)),
    'mainRFileUri': obj.mainRFileUri,
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob
 */
export interface WorkflowTemplateSpecForProviderJobsSparkSqlJob {
  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#jarFileUris
   */
  readonly jarFileUris?: string[];

  /**
   * Optional. The runtime log config for job execution.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#loggingConfig
   */
  readonly loggingConfig?: WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig[];

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#properties
   */
  readonly properties?: { [key: string]: string };

  /**
   * The HCFS URI of the script that contains SQL queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#queryFileUri
   */
  readonly queryFileUri?: string;

  /**
   * A list of queries.
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#queryList
   */
  readonly queryList?: WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList[];

  /**
   * Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJob#scriptVariables
   */
  readonly scriptVariables?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkSqlJob' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJob(obj: WorkflowTemplateSpecForProviderJobsSparkSqlJob | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'jarFileUris': obj.jarFileUris?.map(y => y),
    'loggingConfig': obj.loggingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig(y)),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'queryFileUri': obj.queryFileUri,
    'queryList': obj.queryList?.map(y => toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList(y)),
    'scriptVariables': ((obj.scriptVariables) === undefined) ? undefined : (Object.entries(obj.scriptVariables).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParametersValidation
 */
export interface WorkflowTemplateSpecForProviderParametersValidation {
  /**
   * Validation based on regular expressions.
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidation#regex
   */
  readonly regex?: WorkflowTemplateSpecForProviderParametersValidationRegex[];

  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidation#values
   */
  readonly values?: WorkflowTemplateSpecForProviderParametersValidationValues[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParametersValidation' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParametersValidation(obj: WorkflowTemplateSpecForProviderParametersValidation | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'regex': obj.regex?.map(y => toJson_WorkflowTemplateSpecForProviderParametersValidationRegex(y)),
    'values': obj.values?.map(y => toJson_WorkflowTemplateSpecForProviderParametersValidationValues(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementClusterSelector
 */
export interface WorkflowTemplateSpecForProviderPlacementClusterSelector {
  /**
   * Required. The cluster labels. Cluster must have all labels to match.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementClusterSelector#clusterLabels
   */
  readonly clusterLabels: { [key: string]: string };

  /**
   * Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
   *
   * @schema WorkflowTemplateSpecForProviderPlacementClusterSelector#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementClusterSelector' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementClusterSelector(obj: WorkflowTemplateSpecForProviderPlacementClusterSelector | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterLabels': ((obj.clusterLabels) === undefined) ? undefined : (Object.entries(obj.clusterLabels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedCluster {
  /**
   * Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster#clusterName
   */
  readonly clusterName: string;

  /**
   * Required. The cluster configuration.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster#config
   */
  readonly config: WorkflowTemplateSpecForProviderPlacementManagedClusterConfig[];

  /**
   * Optional. The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedCluster#labels
   */
  readonly labels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedCluster' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedCluster(obj: WorkflowTemplateSpecForProviderPlacementManagedCluster | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'clusterName': obj.clusterName,
    'config': obj.config?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfig(y)),
    'labels': ((obj.labels) === undefined) ? undefined : (Object.entries(obj.labels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema WorkflowTemplateSpecProviderConfigRefPolicyResolution
 */
export enum WorkflowTemplateSpecProviderConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema WorkflowTemplateSpecProviderConfigRefPolicyResolve
 */
export enum WorkflowTemplateSpecProviderConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema WorkflowTemplateSpecProviderRefPolicyResolution
 */
export enum WorkflowTemplateSpecProviderRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema WorkflowTemplateSpecProviderRefPolicyResolve
 */
export enum WorkflowTemplateSpecProviderRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * Policies for referencing.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy
 */
export interface WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy {
  /**
   * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy#resolution
   */
  readonly resolution?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolution;

  /**
   * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
   *
   * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy#resolve
   */
  readonly resolve?: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolve;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy(obj: WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicy | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'resolution': obj.resolution,
    'resolve': obj.resolve,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsHadoopJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsHiveJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsHiveJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsHiveJobQueryList#queries
   */
  readonly queries: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsHiveJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsHiveJobQueryList(obj: WorkflowTemplateSpecForProviderJobsHiveJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsPigJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPigJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsPigJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsPigJobQueryList#queries
   */
  readonly queries: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPigJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPigJobQueryList(obj: WorkflowTemplateSpecForProviderJobsPigJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsPrestoJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPrestoJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsPrestoJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsPrestoJobQueryList#queries
   */
  readonly queries: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPrestoJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPrestoJobQueryList(obj: WorkflowTemplateSpecForProviderJobsPrestoJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsPysparkJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsSparkJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsSparkRJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig
 */
export interface WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig {
  /**
   * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig#driverLogLevels
   */
  readonly driverLogLevels?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig(obj: WorkflowTemplateSpecForProviderJobsSparkSqlJobLoggingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'driverLogLevels': ((obj.driverLogLevels) === undefined) ? undefined : (Object.entries(obj.driverLogLevels).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList
 */
export interface WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
   *
   * @schema WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList#queries
   */
  readonly queries: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList(obj: WorkflowTemplateSpecForProviderJobsSparkSqlJobQueryList | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'queries': obj.queries?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParametersValidationRegex
 */
export interface WorkflowTemplateSpecForProviderParametersValidationRegex {
  /**
   * Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidationRegex#regexes
   */
  readonly regexes: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParametersValidationRegex' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParametersValidationRegex(obj: WorkflowTemplateSpecForProviderParametersValidationRegex | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'regexes': obj.regexes?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderParametersValidationValues
 */
export interface WorkflowTemplateSpecForProviderParametersValidationValues {
  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecForProviderParametersValidationValues#values
   */
  readonly values: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderParametersValidationValues' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderParametersValidationValues(obj: WorkflowTemplateSpecForProviderParametersValidationValues | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfig {
  /**
   * Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#autoscalingConfig
   */
  readonly autoscalingConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig[];

  /**
   * Optional. Encryption settings for the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#encryptionConfig
   */
  readonly encryptionConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig[];

  /**
   * Optional. Port/endpoint configuration for this cluster
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#endpointConfig
   */
  readonly endpointConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig[];

  /**
   * Optional. The shared Compute Engine config settings for all instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#gceClusterConfig
   */
  readonly gceClusterConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig[];

  /**
   * Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#initializationActions
   */
  readonly initializationActions?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions[];

  /**
   * Optional. Lifecycle setting for the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#lifecycleConfig
   */
  readonly lifecycleConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig[];

  /**
   * Optional. The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#masterConfig
   */
  readonly masterConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig[];

  /**
   * Optional. The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#secondaryWorkerConfig
   */
  readonly secondaryWorkerConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig[];

  /**
   * Optional. Security settings for the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#securityConfig
   */
  readonly securityConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig[];

  /**
   * Optional. The config settings for software inside the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#softwareConfig
   */
  readonly softwareConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig[];

  /**
   * Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#stagingBucket
   */
  readonly stagingBucket?: string;

  /**
   * Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#tempBucket
   */
  readonly tempBucket?: string;

  /**
   * Optional. The Compute Engine config settings for additional worker instances in a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfig#workerConfig
   */
  readonly workerConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoscalingConfig': obj.autoscalingConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig(y)),
    'encryptionConfig': obj.encryptionConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig(y)),
    'endpointConfig': obj.endpointConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig(y)),
    'gceClusterConfig': obj.gceClusterConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig(y)),
    'initializationActions': obj.initializationActions?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions(y)),
    'lifecycleConfig': obj.lifecycleConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig(y)),
    'masterConfig': obj.masterConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig(y)),
    'secondaryWorkerConfig': obj.secondaryWorkerConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig(y)),
    'securityConfig': obj.securityConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig(y)),
    'softwareConfig': obj.softwareConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig(y)),
    'stagingBucket': obj.stagingBucket,
    'tempBucket': obj.tempBucket,
    'workerConfig': obj.workerConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolution
 */
export enum WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolution {
  /** Required */
  REQUIRED = "Required",
  /** Optional */
  OPTIONAL = "Optional",
}

/**
 * Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.
 *
 * @schema WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolve
 */
export enum WorkflowTemplateSpecPublishConnectionDetailsToConfigRefPolicyResolve {
  /** Always */
  ALWAYS = "Always",
  /** IfNotPresent */
  IF_NOT_PRESENT = "IfNotPresent",
}

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig {
  /**
   * Optional. The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ Note that the policy must be in the same project and Dataproc region.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig#policy
   */
  readonly policy?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigAutoscalingConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'policy': obj.policy,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig {
  /**
   * Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig#gcePdKmsKeyName
   */
  readonly gcePdKmsKeyName?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEncryptionConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'gcePdKmsKeyName': obj.gcePdKmsKeyName,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig {
  /**
   * Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
   *
   * @default false.
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig#enableHttpPortAccess
   */
  readonly enableHttpPortAccess?: boolean;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigEndpointConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableHttpPortAccess': obj.enableHttpPortAccess,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig {
  /**
   * Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#internalIpOnly
   */
  readonly internalIpOnly?: boolean;

  /**
   * The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#metadata
   */
  readonly metadata?: { [key: string]: string };

  /**
   * Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default*default`
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#network
   */
  readonly network?: string;

  /**
   * Optional. Node Group Affinity for sole-tenant clusters.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#nodeGroupAffinity
   */
  readonly nodeGroupAffinity?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity[];

  /**
   * Optional. The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#privateIpv6GoogleAccess
   */
  readonly privateIpv6GoogleAccess?: string;

  /**
   * Optional. Reservation Affinity for consuming Zonal reservation.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#reservationAffinity
   */
  readonly reservationAffinity?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity[];

  /**
   * Optional. The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#serviceAccount
   */
  readonly serviceAccount?: string;

  /**
   * Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#serviceAccountScopes
   */
  readonly serviceAccountScopes?: string[];

  /**
   * Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#shieldedInstanceConfig
   */
  readonly shieldedInstanceConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig[];

  /**
   * Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0 * sub0
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#subnetwork
   */
  readonly subnetwork?: string;

  /**
   * The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#tags
   */
  readonly tags?: string[];

  /**
   * Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig#zone
   */
  readonly zone?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'internalIpOnly': obj.internalIpOnly,
    'metadata': ((obj.metadata) === undefined) ? undefined : (Object.entries(obj.metadata).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
    'network': obj.network,
    'nodeGroupAffinity': obj.nodeGroupAffinity?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity(y)),
    'privateIpv6GoogleAccess': obj.privateIpv6GoogleAccess,
    'reservationAffinity': obj.reservationAffinity?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity(y)),
    'serviceAccount': obj.serviceAccount,
    'serviceAccountScopes': obj.serviceAccountScopes?.map(y => y),
    'shieldedInstanceConfig': obj.shieldedInstanceConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig(y)),
    'subnetwork': obj.subnetwork,
    'tags': obj.tags?.map(y => y),
    'zone': obj.zone,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions {
  /**
   * Required. Cloud Storage URI of executable file.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions#executableFile
   */
  readonly executableFile?: string;

  /**
   * Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
   *
   * @default 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions#executionTimeout
   */
  readonly executionTimeout?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigInitializationActions | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'executableFile': obj.executableFile,
    'executionTimeout': obj.executionTimeout,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig {
  /**
   * Optional. The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig#autoDeleteTime
   */
  readonly autoDeleteTime?: string;

  /**
   * Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig#autoDeleteTtl
   */
  readonly autoDeleteTtl?: string;

  /**
   * Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig#idleDeleteTtl
   */
  readonly idleDeleteTtl?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigLifecycleConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'autoDeleteTime': obj.autoDeleteTime,
    'autoDeleteTtl': obj.autoDeleteTtl,
    'idleDeleteTtl': obj.idleDeleteTtl,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig {
  /**
   * Optional. The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators[];

  /**
   * Optional. Disk option config settings.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig[];

  /**
   * Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#image
   */
  readonly image?: string;

  /**
   * Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Optional. Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Optional. The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig {
  /**
   * Optional. The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators[];

  /**
   * Optional. Disk option config settings.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig[];

  /**
   * Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#image
   */
  readonly image?: string;

  /**
   * Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Optional. Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Optional. The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig {
  /**
   * Kerberos related configuration.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig#kerberosConfig
   */
  readonly kerberosConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'kerberosConfig': obj.kerberosConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig(y)),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig {
  /**
   * Optional. The version of software inside the cluster. It must be one of the supported (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig#imageVersion
   */
  readonly imageVersion?: string;

  /**
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig#optionalComponents
   */
  readonly optionalComponents?: string[];

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig#properties
   */
  readonly properties?: { [key: string]: string };

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSoftwareConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'imageVersion': obj.imageVersion,
    'optionalComponents': obj.optionalComponents?.map(y => y),
    'properties': ((obj.properties) === undefined) ? undefined : (Object.entries(obj.properties).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {})),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig {
  /**
   * Optional. The Compute Engine accelerator configuration for these instances.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#accelerators
   */
  readonly accelerators?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators[];

  /**
   * Optional. Disk option config settings.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#diskConfig
   */
  readonly diskConfig?: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig[];

  /**
   * Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#image
   */
  readonly image?: string;

  /**
   * Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#machineType
   */
  readonly machineType?: string;

  /**
   * Optional. Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#minCpuPlatform
   */
  readonly minCpuPlatform?: string;

  /**
   * Optional. The number of VM instances in the instance group. For master instance groups, must be set to 1.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#numInstances
   */
  readonly numInstances?: number;

  /**
   * Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig#preemptibility
   */
  readonly preemptibility?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'accelerators': obj.accelerators?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators(y)),
    'diskConfig': obj.diskConfig?.map(y => toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig(y)),
    'image': obj.image,
    'machineType': obj.machineType,
    'minCpuPlatform': obj.minCpuPlatform,
    'numInstances': obj.numInstances,
    'preemptibility': obj.preemptibility,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
  /**
   * Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1*node-group-1`
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity#nodeGroup
   */
  readonly nodeGroup: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'nodeGroup': obj.nodeGroup,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity {
  /**
   * Optional. Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#consumeReservationType
   */
  readonly consumeReservationType?: string;

  /**
   * Optional. Corresponds to the label key of reservation resource.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#key
   */
  readonly key?: string;

  /**
   * Required. List of allowed values for the parameter.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity#values
   */
  readonly values?: string[];

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'consumeReservationType': obj.consumeReservationType,
    'key': obj.key,
    'values': obj.values?.map(y => y),
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
  /**
   * Optional. Defines whether instances have Integrity Monitoring enabled.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableIntegrityMonitoring
   */
  readonly enableIntegrityMonitoring?: boolean;

  /**
   * Optional. Defines whether instances have Secure Boot enabled.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableSecureBoot
   */
  readonly enableSecureBoot?: boolean;

  /**
   * Optional. Defines whether instances have the vTPM enabled.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig#enableVtpm
   */
  readonly enableVtpm?: boolean;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'enableIntegrityMonitoring': obj.enableIntegrityMonitoring,
    'enableSecureBoot': obj.enableSecureBoot,
    'enableVtpm': obj.enableVtpm,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig {
  /**
   * Optional. Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigMasterConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
  /**
   * Optional. Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig {
  /**
   * Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustAdminServer
   */
  readonly crossRealmTrustAdminServer?: string;

  /**
   * Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustKdc
   */
  readonly crossRealmTrustKdc?: string;

  /**
   * Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustRealm
   */
  readonly crossRealmTrustRealm?: string;

  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#crossRealmTrustSharedPassword
   */
  readonly crossRealmTrustSharedPassword?: string;

  /**
   * Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#enableKerberos
   */
  readonly enableKerberos?: boolean;

  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#kdcDbKey
   */
  readonly kdcDbKey?: string;

  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keyPassword
   */
  readonly keyPassword?: string;

  /**
   * Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keystore
   */
  readonly keystore?: string;

  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#keystorePassword
   */
  readonly keystorePassword?: string;

  /**
   * Optional. The uri of the KMS key used to encrypt various sensitive files.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#kmsKey
   */
  readonly kmsKey?: string;

  /**
   * Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#realm
   */
  readonly realm?: string;

  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#rootPrincipalPassword
   */
  readonly rootPrincipalPassword?: string;

  /**
   * Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#tgtLifetimeHours
   */
  readonly tgtLifetimeHours?: number;

  /**
   * Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#truststore
   */
  readonly truststore?: string;

  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig#truststorePassword
   */
  readonly truststorePassword?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'crossRealmTrustAdminServer': obj.crossRealmTrustAdminServer,
    'crossRealmTrustKdc': obj.crossRealmTrustKdc,
    'crossRealmTrustRealm': obj.crossRealmTrustRealm,
    'crossRealmTrustSharedPassword': obj.crossRealmTrustSharedPassword,
    'enableKerberos': obj.enableKerberos,
    'kdcDbKey': obj.kdcDbKey,
    'keyPassword': obj.keyPassword,
    'keystore': obj.keystore,
    'keystorePassword': obj.keystorePassword,
    'kmsKey': obj.kmsKey,
    'realm': obj.realm,
    'rootPrincipalPassword': obj.rootPrincipalPassword,
    'tgtLifetimeHours': obj.tgtLifetimeHours,
    'truststore': obj.truststore,
    'truststorePassword': obj.truststorePassword,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators {
  /**
   * The number of the accelerator cards of this type exposed to this instance.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators#acceleratorCount
   */
  readonly acceleratorCount?: number;

  /**
   * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators#acceleratorType
   */
  readonly acceleratorType?: string;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigAccelerators | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'acceleratorCount': obj.acceleratorCount,
    'acceleratorType': obj.acceleratorType,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

/**
 * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig
 */
export interface WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig {
  /**
   * Optional. Size in GB of the boot disk (default is 500GB).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#bootDiskSizeGb
   */
  readonly bootDiskSizeGb?: number;

  /**
   * Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#bootDiskType
   */
  readonly bootDiskType?: string;

  /**
   * Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
   *
   * @schema WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig#numLocalSsds
   */
  readonly numLocalSsds?: number;

}

/**
 * Converts an object of type 'WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig' to JSON representation.
 */
/* eslint-disable max-len, quote-props */
export function toJson_WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig(obj: WorkflowTemplateSpecForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig | undefined): Record<string, any> | undefined {
  if (obj === undefined) { return undefined; }
  const result = {
    'bootDiskSizeGb': obj.bootDiskSizeGb,
    'bootDiskType': obj.bootDiskType,
    'numLocalSsds': obj.numLocalSsds,
  };
  // filter undefined values
  return Object.entries(result).reduce((r, i) => (i[1] === undefined) ? r : ({ ...r, [i[0]]: i[1] }), {});
}
/* eslint-enable max-len, quote-props */

